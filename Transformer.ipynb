{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-only Transformer in JAX\n",
    "Used for NLP and goofing around. The biggest model we trained was 124M Parameters.\n",
    "\n",
    "Made with brain power of:\n",
    "- Aksel Joonas Reedi\n",
    "- Elisa Klunder\n",
    "- Mihkel Mariusz Jezierski\n",
    "- Mika Ernesto Umana Lemus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import orbax.checkpoint as ocp\n",
    "from flax.training import orbax_utils\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_length = 1024\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 768\n",
    "n_heads = 12\n",
    "mlp_dim_mul = 4  # between 2 and 8 according to UvA\n",
    "n_blocks = 12\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "\n",
    "\n",
    "# Generation\n",
    "temperature = 1\n",
    "\n",
    "# Checkpoints\n",
    "delete_checkpoints = True\n",
    "CHECKPOINT_PATH = \"/Users/akseljoonas/Documents/Kool/NN/Final Project/checkpoints\"\n",
    "\n",
    "# Parallelising\n",
    "devices = jax.local_devices()\n",
    "print(devices)\n",
    "\n",
    "# Check if hyperparams make sense\n",
    "assert embed_dim % n_heads == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396780"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def open_data(folder_path=\"./dataset\"):\n",
    "    def remove_char(pre_nietzsche_data):\n",
    "        n_nietzsche_data = pre_nietzsche_data.replace(\"_\", \"\")\n",
    "        nietzsche_data = n_nietzsche_data.replace(\"$\", \"\")\n",
    "        return nietzsche_data\n",
    "\n",
    "    full_txt = \"\"\n",
    "    for path in os.listdir(folder_path):\n",
    "        if \".txt\" in path:\n",
    "            txt = open(os.path.join(folder_path, path), \"r\", encoding=\"utf-8\").read()\n",
    "            txt = remove_char(txt)\n",
    "            full_txt += txt\n",
    "    return full_txt\n",
    "\n",
    "\n",
    "text = open_data()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, tokenizer_type: str = \"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "        elif self.tokenizer_type == \"gpt-2\":\n",
    "            self.enc = tiktoken.encoding_for_model(\"gpt-2\")\n",
    "            self.vocab_size = self.enc.n_vocab\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return int(jnp.copy(self.vocab_size))\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "\n",
    "        return vocab_size, all_characters\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        elif self.tokenizer_type == \"gpt-2\":\n",
    "            encoded_text = self.enc.encode(text)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        elif self.tokenizer_type == \"gpt-2\":\n",
    "            text = self.enc.decode(encoded_text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"gpt-2\") \n",
    "all_data = tokenizer.encode(text)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked.\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size, key) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "        self.key = key\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5)\n",
      "(8, 5)\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size, key=key)\n",
    "train, targets = batch_loader.get_batch(batch_size, context_length, training=True)\n",
    "print(train.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_dim to head_size (B,T,C)\n",
    "        q = self.query(data) # from embed_size to head_size (B,T,C)\n",
    "        v = self.value(data) # from embed_size to head_size (B,T,C)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf \n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle lol\n",
    "        self.layer1 = nn.Dense(features=(self.dim_mul*self.embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequential(nn.Module):\n",
    "    layers: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    n_blocks: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(self.context_length, self.embed_dim)\n",
    "        #########################\n",
    "        self.blocks = CustomSequential(\n",
    "            [\n",
    "                Block(\n",
    "                    head_num=self.head_num,\n",
    "                    embed_dim=self.embed_dim,\n",
    "                    dim_mul=self.dim_mul,\n",
    "                )\n",
    "                for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        _, context_length = data.shape\n",
    "\n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(context_length))\n",
    "\n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.blocks(\n",
    "            embedded_data, training\n",
    "        )  # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optax.adamw(learning_rate)  # scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")\n",
    "\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=n_heads,\n",
    "    dim_mul=mlp_dim_mul,\n",
    "    n_blocks=n_blocks,\n",
    ")\n",
    "\n",
    "# specify what the key is used\n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training State\n",
    "params = variables[\"params\"]\n",
    "\n",
    "orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "\n",
    "dir_path = CHECKPOINT_PATH\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "if delete_checkpoints:\n",
    "    path = ocp.test_utils.erase_and_create_empty(dir_path)\n",
    "\n",
    "\n",
    "if len(os.listdir(dir_path)) > 0:  # If we have saved checkpoints\n",
    "    print('Getting a saved checkpoint')\n",
    "    subdirs = sorted((int(d) for d in os.listdir(dir_path)), reverse=True)\n",
    "    best_model_dir = os.path.join(dir_path, str(subdirs[0]) + \"/default/\")\n",
    "    print(f\"Loaded state {best_model_dir}\")\n",
    "    empty_state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=jax.tree_util.tree_map(np.zeros_like, params),\n",
    "        key=dropout_key,\n",
    "        tx=optimizer,\n",
    "    )\n",
    "    target = {\"model\": empty_state}\n",
    "    state = orbax_checkpointer.restore(best_model_dir, item=target)[\"model\"]\n",
    "    path = dir_path\n",
    "else:\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply, params=params, key=dropout_key, tx=optimizer\n",
    "    )\n",
    "    path = dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "# Checkpoints\n",
    "options = ocp.CheckpointManagerOptions(max_to_keep=3)\n",
    "checkpoint_manager = ocp.CheckpointManager(path, orbax_checkpointer, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        data, labels = batch\n",
    "\n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn(\n",
    "            {\"params\": params}, data, training=True, rngs={\"dropout\": dropout_train_key}\n",
    "        )\n",
    "\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, data, training=False)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(state, num_epochs, dropout_key):\n",
    "    replicated_state = jax.device_put_replicated(state, jax.local_devices())\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    best_eval_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs + 1), miniters=100):\n",
    "        # Get data\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            batch_size, context_length, training=True\n",
    "        )\n",
    "\n",
    "        # Reshaping to be compatiple with pmap\n",
    "        dropout_keys = jax.random.split(dropout_key, jax.local_device_count())\n",
    "        train = train.reshape((jax.local_device_count(), -1, *train.shape[1:]))\n",
    "        train_labels = train_labels.reshape(\n",
    "            (jax.local_device_count(), -1, *train_labels.shape[1:])\n",
    "        )\n",
    "\n",
    "        # Train step\n",
    "        train_batch = (train, train_labels)\n",
    "        replicated_state, gpu_train_losses = _train_step(\n",
    "            replicated_state, train_batch, dropout_keys\n",
    "        )\n",
    "\n",
    "        # Get mean loss across gpus\n",
    "        train_loss = jnp.mean(gpu_train_losses)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            # Get data\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                batch_size, context_length, training=False\n",
    "            )\n",
    "\n",
    "            # Reshaping to be compatiple with pmap\n",
    "            eval = eval.reshape((jax.local_device_count(), -1, *eval.shape[1:]))\n",
    "            eval_labels = eval_labels.reshape(\n",
    "                (jax.local_device_count(), -1, *eval_labels.shape[1:])\n",
    "            )\n",
    "\n",
    "            # Eval step\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            gpu_eval_losses = _eval_step(replicated_state, eval_batch)\n",
    "\n",
    "            # Get mean loss across gpus\n",
    "            eval_loss = jnp.mean(gpu_eval_losses)\n",
    "\n",
    "            # Saving best model according to loss\n",
    "            if eval_loss < best_eval_loss:\n",
    "                print(f\"Saved model with loss {eval_loss}\")\n",
    "                ckpt = {\"model\": jax.device_get(replicated_state)}\n",
    "                save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "\n",
    "                checkpoint_manager.save(\n",
    "                    epoch,\n",
    "                    ckpt,\n",
    "                    save_kwargs={\"save_args\": save_args},\n",
    "                )\n",
    "                checkpoint_manager.wait_until_finished()\n",
    "                best_eval_loss = eval_loss\n",
    "\n",
    "            # Appending losses\n",
    "            train_losses.append(train_loss)\n",
    "            eval_losses.append(eval_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return jax.device_get(replicated_state), train_losses, eval_losses\n",
    "\n",
    "\n",
    "def plot_loss_curves(train_losses, eval_losses, eval_interval=100):\n",
    "    epochs = range(len(train_losses))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs, eval_losses, label=\"Evaluation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Evaluation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9670b7d9c153435289832da5f8fb2153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model with loss 11.321529388427734\n",
      "Epoch 0: Train loss 11.429646492004395, Eval loss 11.321529388427734\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "trained_model_state, train_losses, eval_losses = train(\n",
    "    state=state, num_epochs=max_iters, dropout_key=dropout_key\n",
    ")\n",
    "plot_loss_curves(train_losses, eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state, data, length, temperature):\n",
    "    params = state.params\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    for _ in tqdm(range(length), miniters=length / 10):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        generate_keys = jax.random.split(subkey, jax.local_device_count())\n",
    "        data = _generate_step(state, generate_keys, data, params, temperature)\n",
    "\n",
    "    return jax.device_get(data[0])\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(4))\n",
    "@jax.jit\n",
    "def _generate_step(state, key, data, params, temperature):\n",
    "    data_to_use = data[:, -context_length:]\n",
    "\n",
    "    logits = state.apply_fn({\"params\": params}, data_to_use, training=False)\n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    next_token = jax.random.categorical(key, logits / temperature, shape=(1, 1))\n",
    "\n",
    "    data = jnp.concatenate((data, next_token), axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"The meaning of life is \"\n",
    "prompt_tokens = tokenizer.encode(PROMPT)\n",
    "prompt = jnp.array(prompt_tokens).reshape((1, len(prompt_tokens)))\n",
    "prompt = jnp.repeat(prompt, jax.device_count(), axis=0).reshape(\n",
    "    (jax.device_count(), 1, len(prompt_tokens))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc08e08a47c4ef08d1840a7c35dcd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  464  3616   286  1204   318   220 44779 30610 45551 39654  4546 18246\n",
      "  10691 25144 33236 10673  1494 25552 14315 21329 28135 19402 39522 20796\n",
      "  19850  6771 18446 34506  3167 42149 32956 19890 15832 39732 21549   248\n",
      "  42361 18013   286  8440 38509 27734 23281 48327 10694 34654 40325 45627\n",
      "  13440 19918 21836  7811 13151 16259 18592  6120]]\n",
      "The meaning of life is  DRAG768anasia Peshgramfunded dating hersResources © killAGES2005 collectively Guards Laden probabilities scaling Pool Jews mines Ney DisAIR abdomen membr formally runaway Clint� Playoffsagi of shippingBruceLT Frontier assassinateMus printers trimmed rationality analysts bast Anyway principles 125 cycling Sunniagues\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "generated_seq = generate(\n",
    "    trained_model_state,\n",
    "    prompt,\n",
    "    50,\n",
    "    temperature,\n",
    ")\n",
    "\n",
    "print(generated_seq)\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
