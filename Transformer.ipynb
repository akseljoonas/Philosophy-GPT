{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "head_num = 2\n",
    "dim_mul = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "all_data = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61 68 75  1 57 68 69 71]\n",
      " [62 10 10 69 61 70  1 79]\n",
      " [ 1 61 80 57 59 76 75  0]\n",
      " [ 1 64 61 74 61  1 57 63]]\n",
      "[[68 75  1 57 68 69 71 75]\n",
      " [10 10 69 61 70  1 79 71]\n",
      " [61 80 57 59 76 75  0  3]\n",
      " [64 61 74 61  1 57 63 57]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size)\n",
    "train, targets = batch_loader.get_batch(key, batch_size, context_length, training=True)\n",
    "print(train)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \n",
    "        k = self.key(data)  # from embed_size to head_size (B,T,C)\n",
    "        q = self.query(data)\n",
    "        v = self.value(data)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf\n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "\n",
    "        return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle\n",
    "        self.layer1 = nn.Dense(features=(dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Dropout(rate=0.2, deterministic = not training)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x))\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int \n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(\n",
    "            self.context_length, self.embed_dim\n",
    "        ) \n",
    "        #########################\n",
    "        self.block = Block(\n",
    "            head_num=self.head_num, embed_dim=self.embed_dim, dim_mul=self.dim_mul\n",
    "        )\n",
    "        \n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        \n",
    "        \n",
    "        b, t = data.shape\n",
    "        \n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(t))\n",
    "        \n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.block(embedded_data, training) # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "    \n",
    "    def generate(self, key, params, data, length, dropout_key):\n",
    "        \n",
    "        for i in range(length):\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # because every character has to be different\n",
    "            \n",
    "            data_to_use = data[:, -self.context_length:]\n",
    "            \n",
    "            logits = self.apply({\"params\": params}, \n",
    "                                data_to_use, \n",
    "                                training=False)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "            \n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            \n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        \n",
    "        data, labels = batch\n",
    "                \n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn( \n",
    "            {\"params\": params},\n",
    "            data,\n",
    "            training = True,\n",
    "            rngs={'dropout': dropout_train_key}\n",
    "        )\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.reshape((b * t, c))\n",
    "        labels = labels.reshape((b * t))\n",
    "        labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "        loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch, training: bool):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, \n",
    "                            data, \n",
    "                            training)\n",
    "    \n",
    "    b, t, c = logits.shape\n",
    "    logits = logits.reshape((b * t, c))\n",
    "    labels = labels.reshape((b * t))\n",
    "    labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "    loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs, dropout_key):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            key, batch_size, context_length, training=True\n",
    "        )\n",
    "        \n",
    "        train_batch = (train, train_labels)\n",
    "        \n",
    "        #train_epoch_loss = jnp.array([])\n",
    "        #train_epoch_acc = jnp.array([])\n",
    "\n",
    "        # for batch in batches:\n",
    "        state, train_loss = _train_step(state, train_batch, dropout_key)\n",
    "\n",
    "        #jnp.append(train_epoch_loss, train_loss)\n",
    "         \n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "        if epoch % 5 == 0:\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                key, batch_size, context_length, training=False\n",
    "            )\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = _eval_step(state, eval_batch, training=False)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=head_num,\n",
    "    dim_mul=dim_mul  \n",
    ")\n",
    "\n",
    "## specify what the key is used \n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1205109675 2815411238]\n"
     ]
    }
   ],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a684756f9b724cdf982244878b120aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 5.708846092224121, Eval loss 5.330965042114258\n",
      "Epoch 5: Train loss 0.5922220945358276, Eval loss 5.7347259521484375\n",
      "Epoch 10: Train loss 1.0098216533660889, Eval loss 9.834644317626953\n",
      "Epoch 15: Train loss 1.1854976415634155, Eval loss 13.02960205078125\n",
      "Epoch 20: Train loss 0.7731539011001587, Eval loss 16.697277069091797\n",
      "Epoch 25: Train loss 0.9597819447517395, Eval loss 15.26683235168457\n",
      "Epoch 30: Train loss 0.5426537990570068, Eval loss 16.53205108642578\n",
      "Epoch 35: Train loss 0.43086397647857666, Eval loss 13.196539878845215\n",
      "Epoch 40: Train loss 0.013781491667032242, Eval loss 16.116352081298828\n",
      "Epoch 45: Train loss 0.014574700966477394, Eval loss 19.017078399658203\n",
      "Epoch 50: Train loss 0.002363436622545123, Eval loss 23.800006866455078\n",
      "Epoch 55: Train loss 0.00467181159183383, Eval loss 24.936607360839844\n",
      "Epoch 60: Train loss 0.001634323038160801, Eval loss 23.048599243164062\n",
      "Epoch 65: Train loss 0.0007110615260899067, Eval loss 22.315643310546875\n",
      "Epoch 70: Train loss 0.0012562612537294626, Eval loss 22.975435256958008\n",
      "Epoch 75: Train loss 0.0001177142548840493, Eval loss 24.386619567871094\n",
      "Epoch 80: Train loss 1.2083845138549805, Eval loss 29.613121032714844\n",
      "Epoch 85: Train loss 1.396438479423523, Eval loss 28.448223114013672\n",
      "Epoch 90: Train loss 1.5954207181930542, Eval loss 39.57508850097656\n",
      "Epoch 95: Train loss 5.778099060058594, Eval loss 37.58484649658203\n",
      "Epoch 100: Train loss 1.3030638694763184, Eval loss 51.546260833740234\n",
      "Epoch 105: Train loss 0.11192241311073303, Eval loss 78.21273040771484\n",
      "Epoch 110: Train loss 0.07227020710706711, Eval loss 96.20272064208984\n",
      "Epoch 115: Train loss 1.0334045886993408, Eval loss 114.88670349121094\n",
      "Epoch 120: Train loss 0.7472563982009888, Eval loss 117.4824447631836\n",
      "Epoch 125: Train loss 4.135050346576463e-07, Eval loss 223.8272705078125\n",
      "Epoch 130: Train loss 1.877169132232666, Eval loss 132.15625\n",
      "Epoch 135: Train loss 1.433077096939087, Eval loss 212.45814514160156\n",
      "Epoch 140: Train loss 2.1291236877441406, Eval loss 175.41725158691406\n",
      "Epoch 145: Train loss 0.5040850043296814, Eval loss 218.27273559570312\n",
      "Epoch 150: Train loss 0.0, Eval loss 287.6264343261719\n",
      "Epoch 155: Train loss 1.2148051261901855, Eval loss 243.28358459472656\n",
      "Epoch 160: Train loss 2.3706672191619873, Eval loss 204.23089599609375\n",
      "Epoch 165: Train loss 0.3774682879447937, Eval loss 261.2551574707031\n",
      "Epoch 170: Train loss 7.4505797087454084e-09, Eval loss 323.1890869140625\n",
      "Epoch 175: Train loss 0.0, Eval loss 284.343505859375\n",
      "Epoch 180: Train loss 0.0, Eval loss 546.3724975585938\n",
      "Epoch 185: Train loss 0.8948642611503601, Eval loss 221.71310424804688\n",
      "Epoch 190: Train loss 1.6085033416748047, Eval loss 285.7689208984375\n",
      "Epoch 195: Train loss 0.0, Eval loss 417.597412109375\n",
      "Epoch 200: Train loss 0.009784300811588764, Eval loss 384.1282043457031\n",
      "Epoch 205: Train loss 0.0027247711550444365, Eval loss 389.20343017578125\n",
      "Epoch 210: Train loss 0.831163763999939, Eval loss 432.2508239746094\n",
      "Epoch 215: Train loss 0.0, Eval loss 414.76544189453125\n",
      "Epoch 220: Train loss 0.0, Eval loss 422.7183837890625\n",
      "Epoch 225: Train loss 1.901893973350525, Eval loss 378.0481262207031\n",
      "Epoch 230: Train loss 0.006233022082597017, Eval loss 579.118896484375\n",
      "Epoch 235: Train loss 5.587930473893721e-08, Eval loss 738.601806640625\n",
      "Epoch 240: Train loss 1.6465110778808594, Eval loss 433.544921875\n",
      "Epoch 245: Train loss 0.0, Eval loss 478.2800598144531\n",
      "Epoch 250: Train loss 0.0, Eval loss 367.0650939941406\n",
      "Epoch 255: Train loss 0.0, Eval loss 434.69683837890625\n",
      "Epoch 260: Train loss 0.0, Eval loss 536.1220092773438\n",
      "Epoch 265: Train loss 1.547719955444336, Eval loss 404.4193420410156\n",
      "Epoch 270: Train loss 0.0, Eval loss 625.710693359375\n",
      "Epoch 275: Train loss 7.487743687306647e-07, Eval loss 561.3836669921875\n",
      "Epoch 280: Train loss 0.24639461934566498, Eval loss 462.3936767578125\n",
      "Epoch 285: Train loss 0.0, Eval loss 681.666259765625\n",
      "Epoch 290: Train loss 0.0, Eval loss 795.4415283203125\n",
      "Epoch 295: Train loss 0.0, Eval loss 559.740478515625\n",
      "Epoch 300: Train loss 0.0001600303512532264, Eval loss 373.8098449707031\n",
      "Epoch 305: Train loss 0.0, Eval loss 362.6907653808594\n",
      "Epoch 310: Train loss 0.0, Eval loss 482.3531494140625\n",
      "Epoch 315: Train loss 9.205526351928711, Eval loss 742.8363647460938\n",
      "Epoch 320: Train loss 0.0010017547756433487, Eval loss 645.220703125\n",
      "Epoch 325: Train loss 0.0, Eval loss 824.4366455078125\n",
      "Epoch 330: Train loss 0.0, Eval loss 852.5391845703125\n",
      "Epoch 335: Train loss 0.0, Eval loss 581.8619995117188\n",
      "Epoch 340: Train loss 0.0, Eval loss 754.215087890625\n",
      "Epoch 345: Train loss 0.0, Eval loss 574.7838134765625\n",
      "Epoch 350: Train loss 0.0, Eval loss 699.7518310546875\n",
      "Epoch 355: Train loss 0.0, Eval loss 641.147705078125\n",
      "Epoch 360: Train loss 0.0, Eval loss 482.858154296875\n",
      "Epoch 365: Train loss 0.0, Eval loss 546.0023193359375\n",
      "Epoch 370: Train loss 1.7471122646384174e-06, Eval loss 579.9519653320312\n",
      "Epoch 375: Train loss 0.0, Eval loss 515.64794921875\n",
      "Epoch 380: Train loss 0.0, Eval loss 686.672607421875\n",
      "Epoch 385: Train loss 0.6499099731445312, Eval loss 674.1138916015625\n",
      "Epoch 390: Train loss 0.569793701171875, Eval loss 664.6747436523438\n",
      "Epoch 395: Train loss 0.0, Eval loss 632.4735107421875\n",
      "Epoch 400: Train loss 0.0, Eval loss 586.2679443359375\n",
      "Epoch 405: Train loss 0.0, Eval loss 625.2127685546875\n",
      "Epoch 410: Train loss 0.0, Eval loss 601.1307373046875\n",
      "Epoch 415: Train loss 0.0, Eval loss 604.488525390625\n",
      "Epoch 420: Train loss 0.0, Eval loss 585.7122802734375\n",
      "Epoch 425: Train loss 0.0, Eval loss 572.2105712890625\n",
      "Epoch 430: Train loss 0.0, Eval loss 563.33544921875\n",
      "Epoch 435: Train loss 0.0, Eval loss 557.6331787109375\n",
      "Epoch 440: Train loss 0.0, Eval loss 553.9644165039062\n",
      "Epoch 445: Train loss 0.0, Eval loss 551.5552978515625\n",
      "Epoch 450: Train loss 0.0, Eval loss 549.912353515625\n",
      "Epoch 455: Train loss 0.0, Eval loss 548.7310791015625\n",
      "Epoch 460: Train loss 0.0, Eval loss 547.8267822265625\n",
      "Epoch 465: Train loss 0.0, Eval loss 547.0880737304688\n",
      "Epoch 470: Train loss 0.0, Eval loss 546.4483642578125\n",
      "Epoch 475: Train loss 0.0, Eval loss 545.8684692382812\n",
      "Epoch 480: Train loss 0.0, Eval loss 545.3246459960938\n",
      "Epoch 485: Train loss 0.0, Eval loss 544.8027954101562\n",
      "Epoch 490: Train loss 0.0, Eval loss 544.294921875\n",
      "Epoch 495: Train loss 0.0, Eval loss 543.7957763671875\n",
      "Epoch 500: Train loss 0.0, Eval loss 543.3028564453125\n",
      "Epoch 505: Train loss 0.0, Eval loss 542.814208984375\n",
      "Epoch 510: Train loss 0.0, Eval loss 542.3289184570312\n",
      "Epoch 515: Train loss 0.0, Eval loss 541.846435546875\n",
      "Epoch 520: Train loss 0.0, Eval loss 541.3662109375\n",
      "Epoch 525: Train loss 0.0, Eval loss 540.8881225585938\n",
      "Epoch 530: Train loss 0.0, Eval loss 540.4119873046875\n",
      "Epoch 535: Train loss 0.0, Eval loss 539.9381103515625\n",
      "Epoch 540: Train loss 0.0, Eval loss 539.466064453125\n",
      "Epoch 545: Train loss 0.0, Eval loss 538.995849609375\n",
      "Epoch 550: Train loss 0.0, Eval loss 538.527587890625\n",
      "Epoch 555: Train loss 0.0, Eval loss 538.061279296875\n",
      "Epoch 560: Train loss 0.0, Eval loss 537.5970458984375\n",
      "Epoch 565: Train loss 0.0, Eval loss 537.1344604492188\n",
      "Epoch 570: Train loss 0.0, Eval loss 536.6740112304688\n",
      "Epoch 575: Train loss 0.0, Eval loss 536.2154541015625\n",
      "Epoch 580: Train loss 0.0, Eval loss 535.7589111328125\n",
      "Epoch 585: Train loss 0.0, Eval loss 535.3043823242188\n",
      "Epoch 590: Train loss 0.0, Eval loss 534.8517456054688\n",
      "Epoch 595: Train loss 0.0, Eval loss 534.4010620117188\n",
      "Epoch 600: Train loss 0.0, Eval loss 533.9524536132812\n",
      "Epoch 605: Train loss 0.0, Eval loss 533.505859375\n",
      "Epoch 610: Train loss 0.0, Eval loss 533.061279296875\n",
      "Epoch 615: Train loss 0.0, Eval loss 532.61865234375\n",
      "Epoch 620: Train loss 0.0, Eval loss 532.1781005859375\n",
      "Epoch 625: Train loss 0.0, Eval loss 531.7395629882812\n",
      "Epoch 630: Train loss 0.0, Eval loss 531.3030395507812\n",
      "Epoch 635: Train loss 0.0, Eval loss 530.8687133789062\n",
      "Epoch 640: Train loss 0.0, Eval loss 530.4361572265625\n",
      "Epoch 645: Train loss 0.0, Eval loss 530.0056762695312\n",
      "Epoch 650: Train loss 0.0, Eval loss 529.5775756835938\n",
      "Epoch 655: Train loss 0.0, Eval loss 529.1513671875\n",
      "Epoch 660: Train loss 0.0, Eval loss 528.727294921875\n",
      "Epoch 665: Train loss 0.0, Eval loss 528.3049926757812\n",
      "Epoch 670: Train loss 0.0, Eval loss 527.8850708007812\n",
      "Epoch 675: Train loss 0.0, Eval loss 527.4672241210938\n",
      "Epoch 680: Train loss 0.0, Eval loss 527.0514526367188\n",
      "Epoch 685: Train loss 0.0, Eval loss 526.6378173828125\n",
      "Epoch 690: Train loss 0.0, Eval loss 526.2261962890625\n",
      "Epoch 695: Train loss 0.0, Eval loss 525.816650390625\n",
      "Epoch 700: Train loss 0.0, Eval loss 525.409423828125\n",
      "Epoch 705: Train loss 0.0, Eval loss 525.0042114257812\n",
      "Epoch 710: Train loss 0.0, Eval loss 524.6011962890625\n",
      "Epoch 715: Train loss 0.0, Eval loss 524.2003173828125\n",
      "Epoch 720: Train loss 0.0, Eval loss 523.8017578125\n",
      "Epoch 725: Train loss 0.0, Eval loss 523.4052124023438\n",
      "Epoch 730: Train loss 0.0, Eval loss 523.0106201171875\n",
      "Epoch 735: Train loss 0.0, Eval loss 522.6184692382812\n",
      "Epoch 740: Train loss 0.0, Eval loss 522.2283935546875\n",
      "Epoch 745: Train loss 0.0, Eval loss 521.8404541015625\n",
      "Epoch 750: Train loss 0.0, Eval loss 521.4547119140625\n",
      "Epoch 755: Train loss 0.0, Eval loss 521.071044921875\n",
      "Epoch 760: Train loss 0.0, Eval loss 520.689697265625\n",
      "Epoch 765: Train loss 0.0, Eval loss 520.3106079101562\n",
      "Epoch 770: Train loss 0.0, Eval loss 519.9337158203125\n",
      "Epoch 775: Train loss 0.0, Eval loss 519.558837890625\n",
      "Epoch 780: Train loss 0.0, Eval loss 519.186279296875\n",
      "Epoch 785: Train loss 0.0, Eval loss 518.8157958984375\n",
      "Epoch 790: Train loss 0.0, Eval loss 518.4478149414062\n",
      "Epoch 795: Train loss 0.0, Eval loss 518.0816650390625\n",
      "Epoch 800: Train loss 0.0, Eval loss 517.718017578125\n",
      "Epoch 805: Train loss 0.0, Eval loss 517.3564453125\n",
      "Epoch 810: Train loss 0.0, Eval loss 516.9971313476562\n",
      "Epoch 815: Train loss 0.0, Eval loss 516.6400146484375\n",
      "Epoch 820: Train loss 0.0, Eval loss 516.2850341796875\n",
      "Epoch 825: Train loss 0.0, Eval loss 515.932373046875\n",
      "Epoch 830: Train loss 0.0, Eval loss 515.5819091796875\n",
      "Epoch 835: Train loss 0.0, Eval loss 515.233642578125\n",
      "Epoch 840: Train loss 0.0, Eval loss 514.8876953125\n",
      "Epoch 845: Train loss 0.0, Eval loss 514.5440063476562\n",
      "Epoch 850: Train loss 0.0, Eval loss 514.202392578125\n",
      "Epoch 855: Train loss 0.0, Eval loss 513.8631591796875\n",
      "Epoch 860: Train loss 0.0, Eval loss 513.526123046875\n",
      "Epoch 865: Train loss 0.0, Eval loss 513.1914672851562\n",
      "Epoch 870: Train loss 0.0, Eval loss 512.8587646484375\n",
      "Epoch 875: Train loss 0.0, Eval loss 512.5286254882812\n",
      "Epoch 880: Train loss 0.0, Eval loss 512.200439453125\n",
      "Epoch 885: Train loss 0.0, Eval loss 511.87481689453125\n",
      "Epoch 890: Train loss 0.0, Eval loss 511.5511474609375\n",
      "Epoch 895: Train loss 0.0, Eval loss 511.229736328125\n",
      "Epoch 900: Train loss 0.0, Eval loss 510.9107666015625\n",
      "Epoch 905: Train loss 0.0, Eval loss 510.59381103515625\n",
      "Epoch 910: Train loss 0.0, Eval loss 510.279296875\n",
      "Epoch 915: Train loss 0.0, Eval loss 509.9668884277344\n",
      "Epoch 920: Train loss 0.0, Eval loss 509.6568603515625\n",
      "Epoch 925: Train loss 0.0, Eval loss 509.34893798828125\n",
      "Epoch 930: Train loss 0.0, Eval loss 509.0432434082031\n",
      "Epoch 935: Train loss 0.0, Eval loss 508.7398681640625\n",
      "Epoch 940: Train loss 0.0, Eval loss 508.4388427734375\n",
      "Epoch 945: Train loss 0.0, Eval loss 508.13983154296875\n",
      "Epoch 950: Train loss 0.0, Eval loss 507.84326171875\n",
      "Epoch 955: Train loss 0.0, Eval loss 507.5487976074219\n",
      "Epoch 960: Train loss 0.0, Eval loss 507.25665283203125\n",
      "Epoch 965: Train loss 0.0, Eval loss 506.96673583984375\n",
      "Epoch 970: Train loss 0.0, Eval loss 506.6791076660156\n",
      "Epoch 975: Train loss 0.0, Eval loss 506.3937072753906\n",
      "Epoch 980: Train loss 0.0, Eval loss 506.1104736328125\n",
      "Epoch 985: Train loss 0.0, Eval loss 505.8296203613281\n",
      "Epoch 990: Train loss 0.0, Eval loss 505.55072021484375\n",
      "Epoch 995: Train loss 0.0, Eval loss 505.27435302734375\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = variables['params']\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "trained_model_state = train(state=state, num_epochs=1000, dropout_key=dropout_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n",
      "  75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n",
      "  75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n",
      "  75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75 75\n",
      "  75 75 75 75 75]]\n",
      "\n",
      "ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey, dropout_key = jax.random.split(key, num=3)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=100,\n",
    "    dropout_key=dropout_key\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
