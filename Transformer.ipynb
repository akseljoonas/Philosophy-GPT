{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "context_length = 50\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "head_num = 4\n",
    "dim_mul = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"gpt-4o\")\n",
    "all_data = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size)\n",
    "train, targets = batch_loader.get_batch(key, batch_size, context_length, training=True)\n",
    "print(train)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_size to head_size (B,T,C)\n",
    "        q = self.query(data)\n",
    "        v = self.value(data)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf\n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle\n",
    "        self.layer1 = nn.Dense(features=(dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int \n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(\n",
    "            self.context_length, self.embed_dim\n",
    "        ) \n",
    "        #########################\n",
    "        self.block = Block(\n",
    "            head_num=self.head_num, embed_dim=self.embed_dim, dim_mul=self.dim_mul\n",
    "        )\n",
    "        \n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        \n",
    "        \n",
    "        b, t = data.shape\n",
    "        \n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(t))\n",
    "        \n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.block(embedded_data, training) # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "    \n",
    "    def generate(self, key, params, data, length, dropout_key):\n",
    "        \n",
    "        for i in range(length):\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # because every character has to be different\n",
    "            \n",
    "            data_to_use = data[:, -self.context_length:]\n",
    "            \n",
    "            logits = self.apply({\"params\": params}, \n",
    "                                data_to_use, \n",
    "                                training=False)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "            \n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            \n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        \n",
    "        data, labels = batch\n",
    "                \n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn( \n",
    "            {\"params\": params},\n",
    "            data,\n",
    "            training = True,\n",
    "            rngs={'dropout': dropout_train_key}\n",
    "        )\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.reshape((b * t, c))\n",
    "        labels = labels.reshape((b * t))\n",
    "        labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "        loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch, training: bool):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, \n",
    "                            data, \n",
    "                            training)\n",
    "    \n",
    "    b, t, c = logits.shape\n",
    "    logits = logits.reshape((b * t, c))\n",
    "    labels = labels.reshape((b * t))\n",
    "    labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "    loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs, dropout_key):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            key, batch_size, context_length, training=True\n",
    "        )\n",
    "        \n",
    "        train_batch = (train, train_labels)\n",
    "        \n",
    "        #train_epoch_loss = jnp.array([])\n",
    "        #train_epoch_acc = jnp.array([])\n",
    "\n",
    "        # for batch in batches:\n",
    "        state, train_loss = _train_step(state, train_batch, dropout_key)\n",
    "\n",
    "        #jnp.append(train_epoch_loss, train_loss)\n",
    "         \n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "        if epoch % 5 == 0:\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                key, batch_size, context_length, training=False\n",
    "            )\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = _eval_step(state, eval_batch, training=False)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=head_num,\n",
    "    dim_mul=dim_mul  \n",
    ")\n",
    "\n",
    "## specify what the key is used \n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3134548294 3733159049]\n"
     ]
    }
   ],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc24e262a0494839acde203cabe5d6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input type must be an integer or unsigned integer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m     key: jax\u001b[38;5;241m.\u001b[39mArray\n\u001b[1;32m      7\u001b[0m state \u001b[38;5;241m=\u001b[39m TrainState\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      8\u001b[0m     apply_fn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply,\n\u001b[1;32m      9\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     10\u001b[0m     key\u001b[38;5;241m=\u001b[39mdropout_key,\n\u001b[1;32m     11\u001b[0m     tx\u001b[38;5;241m=\u001b[39moptimizer\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m trained_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 71\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(state, num_epochs, dropout_key)\u001b[0m\n\u001b[1;32m     65\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m (train, train_labels)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#train_epoch_loss = jnp.array([])\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#train_epoch_acc = jnp.array([])\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# for batch in batches:\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m state, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#jnp.append(train_epoch_loss, train_loss)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m  \n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# We could use the loss and accuracy for logging here, e.g. in TensorBoard\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[33], line 32\u001b[0m, in \u001b[0;36m_train_step\u001b[0;34m(state, batch, dropout_key)\u001b[0m\n\u001b[1;32m     27\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(\n\u001b[1;32m     28\u001b[0m     loss_fn,  \u001b[38;5;66;03m# Function to calculate the loss\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Function has additional outputs, here accuracy\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Determine gradients for current model, parameters and batch\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m (loss, logits), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Perform parameter update with gradients and optimizer\u001b[39;00m\n\u001b[1;32m     36\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m, in \u001b[0;36m_train_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      7\u001b[0m data, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Same as model.apply\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_train_key\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m b, t, c \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mreshape((b \u001b[38;5;241m*\u001b[39m t, c))\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[31], line 27\u001b[0m, in \u001b[0;36mTransformerModel.__call__\u001b[0;34m(self, data, training)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     b, t \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 27\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(jnp\u001b[38;5;241m.\u001b[39marange(t))\n\u001b[1;32m     30\u001b[0m     embedded_data \u001b[38;5;241m=\u001b[39m token \u001b[38;5;241m+\u001b[39m position\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/linear.py:1127\u001b[0m, in \u001b[0;36mEmbed.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embeds the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m  with an additional ``features`` dimension appended.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(inputs\u001b[38;5;241m.\u001b[39mdtype, jnp\u001b[38;5;241m.\u001b[39minteger):\n\u001b[0;32m-> 1127\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput type must be an integer or unsigned integer.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# Use take because fancy indexing numpy arrays with JAX indices does not\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# work correctly.\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m (embedding,) \u001b[38;5;241m=\u001b[39m promote_dtype(\n\u001b[1;32m   1131\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, inexact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input type must be an integer or unsigned integer."
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = variables['params']\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "trained_model_state = train(state=state, num_epochs=1000, dropout_key=dropout_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1 71 61 61  1 65 59 60 61 59 57 61  1 65 74 71 71 69 81 65 75 70 57\n",
      "  70 57 64 57 70 76  1 70 77 59 76 70 61  9 76 59 76  1 74 57 62 68 69 65\n",
      "  77 61  1 65 62  1  1 62 74 62 75  1 71 65  1 68  9  1 57 76 75 77 59 64\n",
      "   1 57 60  1  1 61 61 68 64 71 77 65 63 75 65 62 75 65 59 64 61 70 76 76\n",
      "  70  1 57 65 76 17  1  1 76 75 60 76 76 74  1 76  1 59 65 77 57  1 60  1\n",
      "  75 62 75 75 68 70 57 65 57 77 76 65 11 61 62 76 57 61 63 79 59 65 64  1\n",
      "  57 70 57  1 59 57  1 74 63 77 81 63  1  1 76 76 71 77 59 81 76 62 76 74\n",
      "  76 61 64 62  1  1  0  1 61  1 75 65  9  1 62 70 57 57 61  1 71 57  1 76\n",
      "  61 68 61 81 75 76  1 68 75 79 68 57 63 70  1 74 59 75 61  1 68 65 59 77\n",
      "  57 61 61 75 57 60 76 68 61 65  1 60 76  1 76 59 75 61 57 68 61 61 61 75\n",
      "  61 71 61 76 81 77 75 63 75 61 76 69  1 60  1 71  1 75 62 76  1 61 61  1\n",
      "  76  1 79 75 62 70 76 60 57  1 71 61 65 77 57  1  1 59  1 76 81  1 70 76\n",
      "  65  1 74 74 68 71 70 74 61  1 57 75 61 61 79 70 57 65 68 70 70 81 71 59\n",
      "  61 70  1 62 61 71 59  1 76 60 61 61  1 76 11 68 70 64 65  1 77 57 70 75\n",
      "  75 61 64 60 60 68 57  1 61 75 59  1 74 61 76 64 76 57 70 61 81 57 70 64\n",
      "  60  1 77 70 61 74 57 61 81 76 61 68 76 61 61  1 62 60 76 65  1 30 76 76\n",
      "  76 77 76  1 61 70 79 75 74 76 74 77 61 70  1 63 61  1 57 70 61 64 64 57\n",
      "  75 61 70 75 81 62 75 61 57 70  0 76 62 65  1 76 77  1 77 57 70 81 57 70\n",
      "  61  1 61  1 75  0  1 61  1 61 61 65 71 61 75 75  0 75 58 62 76 75 63 79\n",
      "  70 61 76 74 61 76 64 57 76 76 77 76 76 71 76 68  1  1 81 81  1 65 59 62\n",
      "  75 76 64 81  1 81 75  1 57 61 57 76 75 75 61  1 68 71 79 64 59]]\n",
      "\n",
      " oee icdecae iroomyisnanahant nuctne,tct raflmiue if  frfs oi l, atsuch ad  eelhouigsifsichenttn ait4  tsdttr t ciua d sfsslnaiauti.eftaegwcih ana ca rguyg  ttoucytftrtehf  \n",
      " e si, fnaae oa teleyst lswlagn rcse licuaeesadtlei dt tcsealeeeseoetyusgsetm d o sft ee t wsfntda oeiua  c ty nti rrlonre aseewnailnnyocen feoc tdee t.lnhi uanssehddla esc rethtaneyanhd uneraeyteltee fdti Btttut enwsrtruen ge anehhasensyfsean\n",
      "tfi tu uanyane e s\n",
      " e eeioess\n",
      "sbftsgwnetrethattuttotl  yy icfsthy ys aeatsse lowhc\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey, dropout_key = jax.random.split(key, num=3)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=500,\n",
    "    dropout_key=dropout_key\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
