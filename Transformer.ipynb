{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "context_length = 64\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "n_heads = 8\n",
    "mlp_dim_mul = 4  # between 2 and 8 according to UvA\n",
    "n_blocks = 4\n",
    "learning_rate = 3e-4\n",
    "max_iters = 10\n",
    "\n",
    "# Scheduler\n",
    "scheduler_warmup_steps = 1\n",
    "scheduler_decay_steps = 8\n",
    "\n",
    "# Generation\n",
    "temperature = 1\n",
    "\n",
    "\n",
    "# Parallelising\n",
    "devices = jax.local_devices()\n",
    "print(devices)\n",
    "\n",
    "# Check if hyperparams make sense\n",
    "assert embed_dim % n_heads == 0\n",
    "assert scheduler_decay_steps <= max_iters\n",
    "assert scheduler_warmup_steps <= scheduler_decay_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"./new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, tokenizer_type: str = \"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            self.enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "            self.vocab_size = self.enc.n_vocab\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return int(jnp.copy(self.vocab_size))\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "\n",
    "        return vocab_size, all_characters\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            encoded_text = self.enc.encode(text)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            text = self.enc.decode(encoded_text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"gpt-4o\") \n",
    "all_data = tokenizer.encode(text)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size, key) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "        self.key = key\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 64)\n",
      "(16, 64)\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size, key=key)\n",
    "train, targets = batch_loader.get_batch(batch_size, context_length, training=True)\n",
    "print(train.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_dim to head_size (B,T,C)\n",
    "        q = self.query(data) # from embed_size to head_size (B,T,C)\n",
    "        v = self.value(data) # from embed_size to head_size (B,T,C)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf \n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle lol\n",
    "        self.layer1 = nn.Dense(features=(self.dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequential(nn.Module):\n",
    "    layers: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    n_blocks: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(self.context_length, self.embed_dim)\n",
    "        #########################\n",
    "        self.blocks = CustomSequential(\n",
    "            [\n",
    "                Block(\n",
    "                    head_num=self.head_num,\n",
    "                    embed_dim=self.embed_dim,\n",
    "                    dim_mul=self.dim_mul,\n",
    "                )\n",
    "                for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        _, context_length = data.shape\n",
    "\n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(context_length))\n",
    "\n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.blocks(\n",
    "            embedded_data, training\n",
    "        )  # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    def generate(self, key, params, data, length, temperature):\n",
    "        batch_size, _ = data.shape\n",
    "\n",
    "        # Prepare jax.random.choice to operate on batches of data\n",
    "        # points, without the need for explicit loops\n",
    "        batched_random_choice = jax.vmap(jax.random.choice)\n",
    "\n",
    "        for _ in range(length):\n",
    "            # One new random key for every new character\n",
    "            key, subkey = jax.random.split(key)\n",
    "\n",
    "            # Prepare a (batch_size, 1) column of subkeys, one for every batch\n",
    "            batched_key = subkey.reshape(1, -1)\n",
    "            batched_key = jnp.repeat(batched_key, batch_size, axis=0)\n",
    "\n",
    "            # Only use the last context_window characters to make predictions\n",
    "            data_to_use = data[:, -self.context_length :]\n",
    "            print(data_to_use.shape)\n",
    "            raise KeyboardInterrupt\n",
    "            # Forward pass through the network to get the predictions\n",
    "            logits = self.apply({\"params\": params}, data_to_use, training=False)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = jax.nn.softmax(logits/temperature)\n",
    "\n",
    "            # Preare a (batch_size, vocab_size) matrix storing token indexes\n",
    "            token_indexes = jnp.arange(self.vocab_size).reshape(1, -1)\n",
    "            token_indexes = jnp.repeat(token_indexes, batch_size, axis=0)\n",
    "\n",
    "            # Selext new tokens for all batches based on probabilities\n",
    "            next_indexes = batched_random_choice(\n",
    "                batched_key, token_indexes, p=probabilities\n",
    "            )\n",
    "            next_indexes = next_indexes.reshape(batch_size, -1)\n",
    "\n",
    "            # Append the new tokens to the sequence\n",
    "            data = jnp.concatenate([data, next_indexes], axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01,\n",
    "    peak_value=1,\n",
    "    warmup_steps=scheduler_warmup_steps,\n",
    "    decay_steps=scheduler_decay_steps,\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        data, labels = batch\n",
    "\n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn(\n",
    "            {\"params\": params}, data, training=True, rngs={\"dropout\": dropout_train_key}\n",
    "        )\n",
    "\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, data, training=False)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def train(state, num_epochs, dropout_key):\n",
    "    replicated_state = jax.device_put_replicated(state, jax.local_devices())\n",
    "    p_train_step = jax.pmap(_train_step, in_axes=(0, 0, 0), axis_name=\"batch\")\n",
    "    p_eval_step = jax.pmap(_eval_step, in_axes=(0, 0), axis_name=\"batch\")\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            batch_size * jax.local_device_count(), context_length, training=True\n",
    "        )\n",
    "\n",
    "        dropout_keys = jax.random.split(dropout_key, jax.local_device_count())\n",
    "        train = train.reshape((jax.local_device_count(), -1, *train.shape[1:]))\n",
    "        train_labels = train_labels.reshape(\n",
    "            (jax.local_device_count(), -1, *train_labels.shape[1:])\n",
    "        )\n",
    "        batch = (train, train_labels)\n",
    "\n",
    "        replicated_state, train_loss = p_train_step(\n",
    "            replicated_state, batch, dropout_keys\n",
    "        )\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                batch_size * jax.local_device_count(), context_length, training=False\n",
    "            )\n",
    "\n",
    "            eval = eval.reshape((jax.local_device_count(), -1, *eval.shape[1:]))\n",
    "            eval_labels = eval_labels.reshape(\n",
    "                (jax.local_device_count(), -1, *eval_labels.shape[1:])\n",
    "            )\n",
    "\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = p_eval_step(replicated_state, eval_batch)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Train loss {train_loss.mean()}, Eval loss {eval_loss.mean()}\"\n",
    "            )\n",
    "\n",
    "    return replicated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=n_heads,\n",
    "    dim_mul=mlp_dim_mul,\n",
    "    n_blocks=n_blocks,\n",
    ")\n",
    "\n",
    "## specify what the key is used\n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9152027f38456795984cf726f0dc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 12.715442657470703, Eval loss 12.248589515686035\n",
      "Epoch 10: Train loss 13.071510314941406, Eval loss 34.37968444824219\n",
      "Epoch 20: Train loss 31.14096450805664, Eval loss 36.445186614990234\n",
      "Epoch 30: Train loss 34.517086029052734, Eval loss 45.8260612487793\n",
      "Epoch 40: Train loss 30.22191619873047, Eval loss 24.901512145996094\n",
      "Epoch 50: Train loss 16.671518325805664, Eval loss 18.212505340576172\n",
      "Epoch 60: Train loss 9.025704383850098, Eval loss 10.710586547851562\n",
      "Epoch 70: Train loss 8.554784774780273, Eval loss 8.301011085510254\n",
      "Epoch 80: Train loss 8.07221794128418, Eval loss 8.170515060424805\n",
      "Epoch 90: Train loss 7.914683818817139, Eval loss 7.835919380187988\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = variables['params']\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "trained_model_state = train(state=state, num_epochs=max_iters, dropout_key=dropout_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (1, 200019, 32) but got shape (200019, 32) instead for parameter \"embedding\" in \"/token_embedding_table\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m key, subkey, dropout_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m generated_seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrained_model_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_seq)\n\u001b[1;32m     14\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_seq[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[53], line 72\u001b[0m, in \u001b[0;36mTransformerModel.generate\u001b[0;34m(self, key, params, data, length, temperature)\u001b[0m\n\u001b[1;32m     69\u001b[0m generate_step \u001b[38;5;241m=\u001b[39m partial(generate_step)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[0;32m---> 72\u001b[0m     key, data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[53], line 55\u001b[0m, in \u001b[0;36mTransformerModel.generate.<locals>.generate_step\u001b[0;34m(key, data)\u001b[0m\n\u001b[1;32m     52\u001b[0m data_to_use \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length:]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Forward pass through the network to get the predictions\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     57\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits \u001b[38;5;241m/\u001b[39m temperature)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[53], line 31\u001b[0m, in \u001b[0;36mTransformerModel.__call__\u001b[0;34m(self, data, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     _, context_length \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 31\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(jnp\u001b[38;5;241m.\u001b[39marange(context_length))\n\u001b[1;32m     34\u001b[0m     embedded_data \u001b[38;5;241m=\u001b[39m token \u001b[38;5;241m+\u001b[39m position\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/linear.py:1108\u001b[0m, in \u001b[0;36mEmbed.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1108\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/core/scope.py:982\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 982\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    983\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[1;32m    984\u001b[0m       )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (1, 200019, 32) but got shape (200019, 32) instead for parameter \"embedding\" in \"/token_embedding_table\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey, dropout_key = jax.random.split(key, num=3)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.ones((1, 1), dtype=jax.numpy.int32),\n",
    "    length=50,\n",
    "    temperature=temperature\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
