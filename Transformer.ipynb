{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "context_length = 50\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "head_num = 4\n",
    "dim_mul = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "all_data = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 69 61 70 76  1 79 61  1 57 74 61  1 68 65 78 65 70 63  1 65 70  1 57\n",
      "   1 74 61 57 59 76 65 71 70  9  1 65 70  1 76 64 61  1 64 61 57 74 76  1\n",
      "  65 76]\n",
      " [61 79  9  1 57 70 60  1 57 58 71 78 61  1 57 68 68  1 62 71 74  1 76 64\n",
      "  61  1 69 71 74 61  1 70 71 58 68 61  1 62 77 70 59 76 65 71 70 75  1 57\n",
      "  70 60]\n",
      " [61 61 68  1 68 57 79  1 57 70 60  0 74 61 63 77 68 57 76 65 71 70  1 57\n",
      "  75  1 59 71 70 75 76 74 57 65 70 76  1 57 70 60  1 68 71 75 75  9  1 72\n",
      "  61 71]\n",
      " [74 65 71 77 75  1 65 70 75 76 65 70 59 76  1  6 76 64 61  1 64 61 74 60\n",
      "   7 11  0 43 74  1 64 65 75 76 71 74 81  1 79 65 76 64  1 65 76 75  1 65\n",
      "  69 69]\n",
      " [ 1 71 62  1 76 64 61  1 59 71 70 76 57 63 65 71 70  1 57 69 71 70 63  1\n",
      "  76 64 61  1 75 71 77 70 60  1 72 57 74 76 75  1 71 62  1 76 64 61  1 71\n",
      "  74 63]\n",
      " [74 59 61  1 79 64 65 59 64  1 65 75  1 57 76  0 79 71 74 67  1 71 70  1\n",
      "  57  1 69 71 74 61  1 63 74 57 70 60 65 71 75 61  1 75 59 57 68 61  1 65\n",
      "  70  1]\n",
      " [65 68 68  9  1 76 64 61  1 71 68 60  1 65 60 71 68 10 72 74 65 61 75 76\n",
      "   9  1 79 64 71  1 58 74 71 65 68 61 76 64  1 71 77 74  0 58 61 75 76  1\n",
      "  62 71]\n",
      " [ 1 79 65 76 64  1 69 65 75 62 71 74 76 77 70 61  0 72 74 61 71 59 59 77\n",
      "  72 81  1 76 64 61  1 60 61 57 74  1 75 81 69 72 57 76 64 65 75 61 74 11\n",
      "   1 36]]\n",
      "[[69 61 70 76  1 79 61  1 57 74 61  1 68 65 78 65 70 63  1 65 70  1 57  1\n",
      "  74 61 57 59 76 65 71 70  9  1 65 70  1 76 64 61  1 64 61 57 74 76  1 65\n",
      "  76 75]\n",
      " [79  9  1 57 70 60  1 57 58 71 78 61  1 57 68 68  1 62 71 74  1 76 64 61\n",
      "   1 69 71 74 61  1 70 71 58 68 61  1 62 77 70 59 76 65 71 70 75  1 57 70\n",
      "  60  1]\n",
      " [61 68  1 68 57 79  1 57 70 60  0 74 61 63 77 68 57 76 65 71 70  1 57 75\n",
      "   1 59 71 70 75 76 74 57 65 70 76  1 57 70 60  1 68 71 75 75  9  1 72 61\n",
      "  71 72]\n",
      " [65 71 77 75  1 65 70 75 76 65 70 59 76  1  6 76 64 61  1 64 61 74 60  7\n",
      "  11  0 43 74  1 64 65 75 76 71 74 81  1 79 65 76 64  1 65 76 75  1 65 69\n",
      "  69 57]\n",
      " [71 62  1 76 64 61  1 59 71 70 76 57 63 65 71 70  1 57 69 71 70 63  1 76\n",
      "  64 61  1 75 71 77 70 60  1 72 57 74 76 75  1 71 62  1 76 64 61  1 71 74\n",
      "  63 57]\n",
      " [59 61  1 79 64 65 59 64  1 65 75  1 57 76  0 79 71 74 67  1 71 70  1 57\n",
      "   1 69 71 74 61  1 63 74 57 70 60 65 71 75 61  1 75 59 57 68 61  1 65 70\n",
      "   1 76]\n",
      " [68 68  9  1 76 64 61  1 71 68 60  1 65 60 71 68 10 72 74 65 61 75 76  9\n",
      "   1 79 64 71  1 58 74 71 65 68 61 76 64  1 71 77 74  0 58 61 75 76  1 62\n",
      "  71 74]\n",
      " [79 65 76 64  1 69 65 75 62 71 74 76 77 70 61  0 72 74 61 71 59 59 77 72\n",
      "  81  1 76 64 61  1 60 61 57 74  1 75 81 69 72 57 76 64 65 75 61 74 11  1\n",
      "  36 61]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size)\n",
    "train, targets = batch_loader.get_batch(key, batch_size, context_length, training=True)\n",
    "print(train)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_size to head_size (B,T,C)\n",
    "        q = self.query(data)\n",
    "        v = self.value(data)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf\n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle\n",
    "        self.layer1 = nn.Dense(features=(dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int \n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(\n",
    "            self.context_length, self.embed_dim\n",
    "        ) \n",
    "        #########################\n",
    "        self.block = Block(\n",
    "            head_num=self.head_num, embed_dim=self.embed_dim, dim_mul=self.dim_mul\n",
    "        )\n",
    "        \n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        \n",
    "        \n",
    "        b, t = data.shape\n",
    "        \n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(t))\n",
    "        \n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.block(embedded_data, training) # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "    \n",
    "    def generate(self, key, params, data, length, dropout_key):\n",
    "        \n",
    "        for i in range(length):\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # because every character has to be different\n",
    "            \n",
    "            data_to_use = data[:, -self.context_length:]\n",
    "            \n",
    "            logits = self.apply({\"params\": params}, \n",
    "                                data_to_use, \n",
    "                                training=False)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "            \n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            \n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        \n",
    "        data, labels = batch\n",
    "                \n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn( \n",
    "            {\"params\": params},\n",
    "            data,\n",
    "            training = True,\n",
    "            rngs={'dropout': dropout_train_key}\n",
    "        )\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.reshape((b * t, c))\n",
    "        labels = labels.reshape((b * t))\n",
    "        labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "        loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch, training: bool):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, \n",
    "                            data, \n",
    "                            training)\n",
    "    \n",
    "    b, t, c = logits.shape\n",
    "    logits = logits.reshape((b * t, c))\n",
    "    labels = labels.reshape((b * t))\n",
    "    labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "    loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs, dropout_key):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            key, batch_size, context_length, training=True\n",
    "        )\n",
    "        \n",
    "        train_batch = (train, train_labels)\n",
    "        \n",
    "        #train_epoch_loss = jnp.array([])\n",
    "        #train_epoch_acc = jnp.array([])\n",
    "\n",
    "        # for batch in batches:\n",
    "        state, train_loss = _train_step(state, train_batch, dropout_key)\n",
    "\n",
    "        #jnp.append(train_epoch_loss, train_loss)\n",
    "         \n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "        if epoch % 5 == 0:\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                key, batch_size, context_length, training=False\n",
    "            )\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = _eval_step(state, eval_batch, training=False)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=head_num,\n",
    "    dim_mul=dim_mul  \n",
    ")\n",
    "\n",
    "## specify what the key is used \n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 918292001 3972578312]\n"
     ]
    }
   ],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd2032be04743f1ae3f44ac6e07b56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 5.70210075378418, Eval loss 4.665155410766602\n",
      "Epoch 5: Train loss 2.2741026878356934, Eval loss 3.507941722869873\n",
      "Epoch 10: Train loss 1.5009815692901611, Eval loss 5.044305801391602\n",
      "Epoch 15: Train loss 0.7582529783248901, Eval loss 6.963747978210449\n",
      "Epoch 20: Train loss 1.4470847845077515, Eval loss 8.812349319458008\n",
      "Epoch 25: Train loss 1.626692771911621, Eval loss 7.388720512390137\n",
      "Epoch 30: Train loss 1.6624228954315186, Eval loss 5.183493614196777\n",
      "Epoch 35: Train loss 1.432822823524475, Eval loss 4.375168323516846\n",
      "Epoch 40: Train loss 1.3867406845092773, Eval loss 5.155807971954346\n",
      "Epoch 45: Train loss 1.836167812347412, Eval loss 5.260542392730713\n",
      "Epoch 50: Train loss 2.328254461288452, Eval loss 4.86705207824707\n",
      "Epoch 55: Train loss 2.856720447540283, Eval loss 4.788138389587402\n",
      "Epoch 60: Train loss 2.620750904083252, Eval loss 4.04902458190918\n",
      "Epoch 65: Train loss 3.325474739074707, Eval loss 4.457215309143066\n",
      "Epoch 70: Train loss 3.898991107940674, Eval loss 6.599228382110596\n",
      "Epoch 75: Train loss 11.086066246032715, Eval loss 18.543210983276367\n",
      "Epoch 80: Train loss 29.156538009643555, Eval loss 22.014799118041992\n",
      "Epoch 85: Train loss 39.980438232421875, Eval loss 33.863948822021484\n",
      "Epoch 90: Train loss 34.87213897705078, Eval loss 33.01480484008789\n",
      "Epoch 95: Train loss 49.395423889160156, Eval loss 52.645408630371094\n",
      "Epoch 100: Train loss 117.9049301147461, Eval loss 78.38722229003906\n",
      "Epoch 105: Train loss 91.86595153808594, Eval loss 77.83743286132812\n",
      "Epoch 110: Train loss 72.77728271484375, Eval loss 61.062503814697266\n",
      "Epoch 115: Train loss 41.68671798706055, Eval loss 38.56428146362305\n",
      "Epoch 120: Train loss 33.25778579711914, Eval loss 36.117679595947266\n",
      "Epoch 125: Train loss 47.66957092285156, Eval loss 61.29444122314453\n",
      "Epoch 130: Train loss 74.20350646972656, Eval loss 48.79670333862305\n",
      "Epoch 135: Train loss 44.0430908203125, Eval loss 39.24974822998047\n",
      "Epoch 140: Train loss 52.6241455078125, Eval loss 46.194461822509766\n",
      "Epoch 145: Train loss 37.40199279785156, Eval loss 27.567771911621094\n",
      "Epoch 150: Train loss 44.908084869384766, Eval loss 34.03341293334961\n",
      "Epoch 155: Train loss 49.80137252807617, Eval loss 34.994590759277344\n",
      "Epoch 160: Train loss 56.66242980957031, Eval loss 52.188602447509766\n",
      "Epoch 165: Train loss 54.70832443237305, Eval loss 68.19214630126953\n",
      "Epoch 170: Train loss 48.09654235839844, Eval loss 42.09028244018555\n",
      "Epoch 175: Train loss 37.520233154296875, Eval loss 32.24191665649414\n",
      "Epoch 180: Train loss 38.27924728393555, Eval loss 25.26450538635254\n",
      "Epoch 185: Train loss 37.709259033203125, Eval loss 25.297260284423828\n",
      "Epoch 190: Train loss 32.70942687988281, Eval loss 25.092763900756836\n",
      "Epoch 195: Train loss 47.78102493286133, Eval loss 53.52745819091797\n",
      "Epoch 200: Train loss 41.87241744995117, Eval loss 46.48823165893555\n",
      "Epoch 205: Train loss 39.496524810791016, Eval loss 30.69570541381836\n",
      "Epoch 210: Train loss 35.934593200683594, Eval loss 28.15993309020996\n",
      "Epoch 215: Train loss 28.303939819335938, Eval loss 24.747323989868164\n",
      "Epoch 220: Train loss 36.23817443847656, Eval loss 42.77971649169922\n",
      "Epoch 225: Train loss 50.211883544921875, Eval loss 42.47483444213867\n",
      "Epoch 230: Train loss 39.76355743408203, Eval loss 35.21965026855469\n",
      "Epoch 235: Train loss 37.290733337402344, Eval loss 45.262577056884766\n",
      "Epoch 240: Train loss 50.78192138671875, Eval loss 50.59033966064453\n",
      "Epoch 245: Train loss 54.99934768676758, Eval loss 54.07659912109375\n",
      "Epoch 250: Train loss 59.50239944458008, Eval loss 32.05207824707031\n",
      "Epoch 255: Train loss 59.13117980957031, Eval loss 57.31562042236328\n",
      "Epoch 260: Train loss 44.68377685546875, Eval loss 38.28691482543945\n",
      "Epoch 265: Train loss 48.78242492675781, Eval loss 45.35000228881836\n",
      "Epoch 270: Train loss 48.08342361450195, Eval loss 41.18299102783203\n",
      "Epoch 275: Train loss 38.34048080444336, Eval loss 49.07139587402344\n",
      "Epoch 280: Train loss 28.409286499023438, Eval loss 18.578765869140625\n",
      "Epoch 285: Train loss 28.11874008178711, Eval loss 25.105567932128906\n",
      "Epoch 290: Train loss 28.750247955322266, Eval loss 25.07525634765625\n",
      "Epoch 295: Train loss 26.047191619873047, Eval loss 25.230493545532227\n",
      "Epoch 300: Train loss 35.541629791259766, Eval loss 46.128822326660156\n",
      "Epoch 305: Train loss 48.00740432739258, Eval loss 37.72098159790039\n",
      "Epoch 310: Train loss 37.78797912597656, Eval loss 32.703086853027344\n",
      "Epoch 315: Train loss 37.97929763793945, Eval loss 36.54854202270508\n",
      "Epoch 320: Train loss 33.71235656738281, Eval loss 28.00920295715332\n",
      "Epoch 325: Train loss 38.129920959472656, Eval loss 41.12813949584961\n",
      "Epoch 330: Train loss 33.85536575317383, Eval loss 28.35841178894043\n",
      "Epoch 335: Train loss 43.47032928466797, Eval loss 47.31502914428711\n",
      "Epoch 340: Train loss 54.77851486206055, Eval loss 49.4389533996582\n",
      "Epoch 345: Train loss 45.05182647705078, Eval loss 43.37320327758789\n",
      "Epoch 350: Train loss 36.75503921508789, Eval loss 31.69721794128418\n",
      "Epoch 355: Train loss 31.947410583496094, Eval loss 27.273038864135742\n",
      "Epoch 360: Train loss 33.71014404296875, Eval loss 39.011985778808594\n",
      "Epoch 365: Train loss 35.17362976074219, Eval loss 30.37138557434082\n",
      "Epoch 370: Train loss 24.723243713378906, Eval loss 21.21733856201172\n",
      "Epoch 375: Train loss 20.809589385986328, Eval loss 22.356536865234375\n",
      "Epoch 380: Train loss 22.31523323059082, Eval loss 23.41773796081543\n",
      "Epoch 385: Train loss 26.954599380493164, Eval loss 24.081647872924805\n",
      "Epoch 390: Train loss 28.848257064819336, Eval loss 25.686840057373047\n",
      "Epoch 395: Train loss 26.42214584350586, Eval loss 24.017457962036133\n",
      "Epoch 400: Train loss 24.293119430541992, Eval loss 24.892656326293945\n",
      "Epoch 405: Train loss 16.831836700439453, Eval loss 16.19454002380371\n",
      "Epoch 410: Train loss 16.15658950805664, Eval loss 12.27975845336914\n",
      "Epoch 415: Train loss 15.141257286071777, Eval loss 14.118464469909668\n",
      "Epoch 420: Train loss 15.908547401428223, Eval loss 14.971327781677246\n",
      "Epoch 425: Train loss 11.74477481842041, Eval loss 13.821022987365723\n",
      "Epoch 430: Train loss 15.561528205871582, Eval loss 18.42064666748047\n",
      "Epoch 435: Train loss 17.346172332763672, Eval loss 20.43685531616211\n",
      "Epoch 440: Train loss 20.42143440246582, Eval loss 18.13680076599121\n",
      "Epoch 445: Train loss 24.158710479736328, Eval loss 26.23285675048828\n",
      "Epoch 450: Train loss 23.62627410888672, Eval loss 21.6617431640625\n",
      "Epoch 455: Train loss 23.884052276611328, Eval loss 28.063018798828125\n",
      "Epoch 460: Train loss 21.727930068969727, Eval loss 19.9829044342041\n",
      "Epoch 465: Train loss 26.850648880004883, Eval loss 23.769540786743164\n",
      "Epoch 470: Train loss 29.727235794067383, Eval loss 23.510282516479492\n",
      "Epoch 475: Train loss 30.060598373413086, Eval loss 25.093446731567383\n",
      "Epoch 480: Train loss 21.383075714111328, Eval loss 22.72063446044922\n",
      "Epoch 485: Train loss 17.57333755493164, Eval loss 12.3652925491333\n",
      "Epoch 490: Train loss 14.293723106384277, Eval loss 14.682292938232422\n",
      "Epoch 495: Train loss 13.319585800170898, Eval loss 13.575849533081055\n",
      "Epoch 500: Train loss 24.70485496520996, Eval loss 24.28533172607422\n",
      "Epoch 505: Train loss 36.18418502807617, Eval loss 35.67793273925781\n",
      "Epoch 510: Train loss 32.839115142822266, Eval loss 24.362611770629883\n",
      "Epoch 515: Train loss 30.753141403198242, Eval loss 31.99569320678711\n",
      "Epoch 520: Train loss 18.21978759765625, Eval loss 19.739213943481445\n",
      "Epoch 525: Train loss 18.03645896911621, Eval loss 15.610514640808105\n",
      "Epoch 530: Train loss 16.810888290405273, Eval loss 15.76750659942627\n",
      "Epoch 535: Train loss 22.24090003967285, Eval loss 21.458202362060547\n",
      "Epoch 540: Train loss 17.89645767211914, Eval loss 12.354165077209473\n",
      "Epoch 545: Train loss 17.546010971069336, Eval loss 18.131071090698242\n",
      "Epoch 550: Train loss 18.193450927734375, Eval loss 14.249332427978516\n",
      "Epoch 555: Train loss 16.962127685546875, Eval loss 19.03862190246582\n",
      "Epoch 560: Train loss 20.570226669311523, Eval loss 24.35300636291504\n",
      "Epoch 565: Train loss 20.51103973388672, Eval loss 26.14824867248535\n",
      "Epoch 570: Train loss 23.870458602905273, Eval loss 28.04622459411621\n",
      "Epoch 575: Train loss 28.813861846923828, Eval loss 31.973390579223633\n",
      "Epoch 580: Train loss 22.70375633239746, Eval loss 23.97225570678711\n",
      "Epoch 585: Train loss 25.96845245361328, Eval loss 25.084789276123047\n",
      "Epoch 590: Train loss 27.338945388793945, Eval loss 26.400243759155273\n",
      "Epoch 595: Train loss 26.04047393798828, Eval loss 22.075031280517578\n",
      "Epoch 600: Train loss 23.226774215698242, Eval loss 17.88359832763672\n",
      "Epoch 605: Train loss 17.87178611755371, Eval loss 23.680463790893555\n",
      "Epoch 610: Train loss 14.045083999633789, Eval loss 15.24429988861084\n",
      "Epoch 615: Train loss 12.60012149810791, Eval loss 11.412216186523438\n",
      "Epoch 620: Train loss 11.206962585449219, Eval loss 9.803922653198242\n",
      "Epoch 625: Train loss 9.233133316040039, Eval loss 7.1152496337890625\n",
      "Epoch 630: Train loss 6.0848259925842285, Eval loss 5.8132524490356445\n",
      "Epoch 635: Train loss 4.90835428237915, Eval loss 4.801997661590576\n",
      "Epoch 640: Train loss 6.467852592468262, Eval loss 6.3016533851623535\n",
      "Epoch 645: Train loss 5.46051025390625, Eval loss 5.332352161407471\n",
      "Epoch 650: Train loss 4.288614749908447, Eval loss 4.480886936187744\n",
      "Epoch 655: Train loss 4.094682693481445, Eval loss 4.412899494171143\n",
      "Epoch 660: Train loss 4.141047954559326, Eval loss 4.443265438079834\n",
      "Epoch 665: Train loss 7.90421199798584, Eval loss 7.06448221206665\n",
      "Epoch 670: Train loss 6.651273250579834, Eval loss 6.32925271987915\n",
      "Epoch 675: Train loss 5.772388935089111, Eval loss 5.490447044372559\n",
      "Epoch 680: Train loss 4.48019552230835, Eval loss 4.263601303100586\n",
      "Epoch 685: Train loss 3.759042501449585, Eval loss 4.389647960662842\n",
      "Epoch 690: Train loss 3.4664065837860107, Eval loss 4.087780475616455\n",
      "Epoch 695: Train loss 3.538804531097412, Eval loss 3.839787483215332\n",
      "Epoch 700: Train loss 2.956474542617798, Eval loss 3.6699883937835693\n",
      "Epoch 705: Train loss 3.0042366981506348, Eval loss 3.488844633102417\n",
      "Epoch 710: Train loss 2.963895797729492, Eval loss 3.4936954975128174\n",
      "Epoch 715: Train loss 2.8392858505249023, Eval loss 3.4437146186828613\n",
      "Epoch 720: Train loss 2.7181620597839355, Eval loss 3.356069326400757\n",
      "Epoch 725: Train loss 2.7257626056671143, Eval loss 3.3280460834503174\n",
      "Epoch 730: Train loss 2.7043721675872803, Eval loss 3.3288075923919678\n",
      "Epoch 735: Train loss 2.671002149581909, Eval loss 3.2864620685577393\n",
      "Epoch 740: Train loss 2.6515722274780273, Eval loss 3.306081533432007\n",
      "Epoch 745: Train loss 2.6005914211273193, Eval loss 3.32470965385437\n",
      "Epoch 750: Train loss 2.5699267387390137, Eval loss 3.34912109375\n",
      "Epoch 755: Train loss 2.5430636405944824, Eval loss 3.354689836502075\n",
      "Epoch 760: Train loss 2.526087999343872, Eval loss 3.373500347137451\n",
      "Epoch 765: Train loss 2.50407338142395, Eval loss 3.399200439453125\n",
      "Epoch 770: Train loss 2.486409902572632, Eval loss 3.4103429317474365\n",
      "Epoch 775: Train loss 2.4634833335876465, Eval loss 3.4230003356933594\n",
      "Epoch 780: Train loss 2.4421048164367676, Eval loss 3.444753885269165\n",
      "Epoch 785: Train loss 2.464743137359619, Eval loss 3.4671053886413574\n",
      "Epoch 790: Train loss 2.422752857208252, Eval loss 3.483807325363159\n",
      "Epoch 795: Train loss 2.446199893951416, Eval loss 3.500645875930786\n",
      "Epoch 800: Train loss 2.4122962951660156, Eval loss 3.5172088146209717\n",
      "Epoch 805: Train loss 2.3867263793945312, Eval loss 3.539201021194458\n",
      "Epoch 810: Train loss 2.4082353115081787, Eval loss 3.533917188644409\n",
      "Epoch 815: Train loss 2.410569429397583, Eval loss 3.523858070373535\n",
      "Epoch 820: Train loss 2.3741402626037598, Eval loss 3.502086639404297\n",
      "Epoch 825: Train loss 2.400197744369507, Eval loss 3.539017915725708\n",
      "Epoch 830: Train loss 2.3576104640960693, Eval loss 3.5318994522094727\n",
      "Epoch 835: Train loss 2.3371543884277344, Eval loss 3.537726402282715\n",
      "Epoch 840: Train loss 2.312288522720337, Eval loss 3.5473601818084717\n",
      "Epoch 845: Train loss 2.305180549621582, Eval loss 3.5425198078155518\n",
      "Epoch 850: Train loss 2.3149330615997314, Eval loss 3.550678014755249\n",
      "Epoch 855: Train loss 2.2696917057037354, Eval loss 3.5428433418273926\n",
      "Epoch 860: Train loss 2.2181358337402344, Eval loss 3.5435545444488525\n",
      "Epoch 865: Train loss 2.2150471210479736, Eval loss 3.599797248840332\n",
      "Epoch 870: Train loss 2.1782591342926025, Eval loss 3.6365482807159424\n",
      "Epoch 875: Train loss 2.211348295211792, Eval loss 3.694978952407837\n",
      "Epoch 880: Train loss 2.138193368911743, Eval loss 3.6950831413269043\n",
      "Epoch 885: Train loss 2.1330370903015137, Eval loss 3.745565176010132\n",
      "Epoch 890: Train loss 2.1240599155426025, Eval loss 3.8093998432159424\n",
      "Epoch 895: Train loss 2.0997650623321533, Eval loss 3.8495237827301025\n",
      "Epoch 900: Train loss 2.082249402999878, Eval loss 3.9168624877929688\n",
      "Epoch 905: Train loss 2.042792558670044, Eval loss 3.9550089836120605\n",
      "Epoch 910: Train loss 2.016540288925171, Eval loss 4.035569190979004\n",
      "Epoch 915: Train loss 1.9820458889007568, Eval loss 4.109983921051025\n",
      "Epoch 920: Train loss 2.022721767425537, Eval loss 4.16331672668457\n",
      "Epoch 925: Train loss 1.9694194793701172, Eval loss 4.228443622589111\n",
      "Epoch 930: Train loss 2.0138931274414062, Eval loss 4.249979019165039\n",
      "Epoch 935: Train loss 1.950212001800537, Eval loss 4.287689208984375\n",
      "Epoch 940: Train loss 1.8894071578979492, Eval loss 4.29250955581665\n",
      "Epoch 945: Train loss 1.8430904150009155, Eval loss 4.333616733551025\n",
      "Epoch 950: Train loss 1.8503708839416504, Eval loss 4.399747371673584\n",
      "Epoch 955: Train loss 1.7890625, Eval loss 4.501092433929443\n",
      "Epoch 960: Train loss 1.7660833597183228, Eval loss 4.587277889251709\n",
      "Epoch 965: Train loss 1.7571321725845337, Eval loss 4.6709442138671875\n",
      "Epoch 970: Train loss 1.7297167778015137, Eval loss 4.733696460723877\n",
      "Epoch 975: Train loss 1.8593069314956665, Eval loss 4.6007561683654785\n",
      "Epoch 980: Train loss 2.593376398086548, Eval loss 4.1208415031433105\n",
      "Epoch 985: Train loss 2.4768054485321045, Eval loss 3.9770143032073975\n",
      "Epoch 990: Train loss 2.2595326900482178, Eval loss 3.821974992752075\n",
      "Epoch 995: Train loss 2.1882753372192383, Eval loss 3.7967894077301025\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = variables['params']\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "trained_model_state = train(state=state, num_epochs=1000, dropout_key=dropout_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1 71 61 61  1 65 59 60 61 59 57 61  1 65 74 71 71 69 81 65 75 70 57\n",
      "  70 57 64 57 70 76  1 70 77 59 76 70 61  9 76 59 76  1 74 57 62 68 69 65\n",
      "  77 61  1 65 62  1  1 62 74 62 75  1 71 65  1 68  9  1 57 76 75 77 59 64\n",
      "   1 57 60  1  1 61 61 68 64 71 77 65 63 75 65 62 75 65 59 64 61 70 76 76\n",
      "  70  1 57 65 76 17  1  1 76 75 60 76 76 74  1 76  1 59 65 77 57  1 60  1\n",
      "  75 62 75 75 68 70 57 65 57 77 76 65 11 61 62 76 57 61 63 79 59 65 64  1\n",
      "  57 70 57  1 59 57  1 74 63 77 81 63  1  1 76 76 71 77 59 81 76 62 76 74\n",
      "  76 61 64 62  1  1  0  1 61  1 75 65  9  1 62 70 57 57 61  1 71 57  1 76\n",
      "  61 68 61 81 75 76  1 68 75 79 68 57 63 70  1 74 59 75 61  1 68 65 59 77\n",
      "  57 61 61 75 57 60 76 68 61 65  1 60 76  1 76 59 75 61 57 68 61 61 61 75\n",
      "  61 71 61 76 81 77 75 63 75 61 76 69  1 60  1 71  1 75 62 76  1 61 61  1\n",
      "  76  1 79 75 62 70 76 60 57  1 71 61 65 77 57  1  1 59  1 76 81  1 70 76\n",
      "  65  1 74 74 68 71 70 74 61  1 57 75 61 61 79 70 57 65 68 70 70 81 71 59\n",
      "  61 70  1 62 61 71 59  1 76 60 61 61  1 76 11 68 70 64 65  1 77 57 70 75\n",
      "  75 61 64 60 60 68 57  1 61 75 59  1 74 61 76 64 76 57 70 61 81 57 70 64\n",
      "  60  1 77 70 61 74 57 61 81 76 61 68 76 61 61  1 62 60 76 65  1 30 76 76\n",
      "  76 77 76  1 61 70 79 75 74 76 74 77 61 70  1 63 61  1 57 70 61 64 64 57\n",
      "  75 61 70 75 81 62 75 61 57 70  0 76 62 65  1 76 77  1 77 57 70 81 57 70\n",
      "  61  1 61  1 75  0  1 61  1 61 61 65 71 61 75 75  0 75 58 62 76 75 63 79\n",
      "  70 61 76 74 61 76 64 57 76 76 77 76 76 71 76 68  1  1 81 81  1 65 59 62\n",
      "  75 76 64 81  1 81 75  1 57 61 57 76 75 75 61  1 68 71 79 64 59]]\n",
      "\n",
      " oee icdecae iroomyisnanahant nuctne,tct raflmiue if  frfs oi l, atsuch ad  eelhouigsifsichenttn ait4  tsdttr t ciua d sfsslnaiauti.eftaegwcih ana ca rguyg  ttoucytftrtehf  \n",
      " e si, fnaae oa teleyst lswlagn rcse licuaeesadtlei dt tcsealeeeseoetyusgsetm d o sft ee t wsfntda oeiua  c ty nti rrlonre aseewnailnnyocen feoc tdee t.lnhi uanssehddla esc rethtaneyanhd uneraeyteltee fdti Btttut enwsrtruen ge anehhasensyfsean\n",
      "tfi tu uanyane e s\n",
      " e eeioess\n",
      "sbftsgwnetrethattuttotl  yy icfsthy ys aeatsse lowhc\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey, dropout_key = jax.random.split(key, num=3)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=500,\n",
    "    dropout_key=dropout_key\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
