{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import orbax.checkpoint as ocp\n",
    "from flax.training import orbax_utils\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "context_length = 256\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "n_heads = 8\n",
    "mlp_dim_mul = 4  # between 2 and 8 according to UvA\n",
    "n_blocks = 6\n",
    "max_iters = 100\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Scheduler\n",
    "use_scheduler = False\n",
    "scheduler_warmup_steps = 15  # typically 5-10% of the total training steps\n",
    "scheduler_decay_steps = max_iters  #  Positive integer, the total length of the schedule\n",
    "init_value = 3e-4\n",
    "peak_value = 0.15\n",
    "\n",
    "# Generation\n",
    "temperature = 1\n",
    "\n",
    "# Checkpoints\n",
    "delete_checkpoints = True\n",
    "CHECKPOINT_PATH = \"/Users/akseljoonas/Documents/Kool/NN/Final Project/checkpoints\"\n",
    "# Parallelising\n",
    "devices = jax.local_devices()\n",
    "print(devices)\n",
    "\n",
    "# Check if hyperparams make sense\n",
    "assert embed_dim % n_heads == 0\n",
    "assert scheduler_decay_steps <= max_iters\n",
    "assert scheduler_warmup_steps <= scheduler_decay_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def open_data(folder_path=\"./dataset\"):\n",
    "    full_txt = \"\"\n",
    "    for path in os.listdir(folder_path):\n",
    "        if \".txt\" in path:\n",
    "            txt = open(os.path.join(folder_path, path), \"r\", encoding=\"utf-8\").read()\n",
    "            full_txt += txt\n",
    "    return txt\n",
    "\n",
    "\n",
    "text = open_data()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, tokenizer_type: str = \"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            self.enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "            self.vocab_size = self.enc.n_vocab\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return int(jnp.copy(self.vocab_size))\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "\n",
    "        return vocab_size, all_characters\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            encoded_text = self.enc.encode(text)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            text = self.enc.decode(encoded_text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"gpt-4o\") \n",
    "all_data = tokenizer.encode(text)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size, key) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "        self.key = key\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256)\n",
      "(16, 256)\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size, key=key)\n",
    "train, targets = batch_loader.get_batch(batch_size, context_length, training=True)\n",
    "print(train.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_dim to head_size (B,T,C)\n",
    "        q = self.query(data) # from embed_size to head_size (B,T,C)\n",
    "        v = self.value(data) # from embed_size to head_size (B,T,C)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf \n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle lol\n",
    "        self.layer1 = nn.Dense(features=(self.dim_mul*self.embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequential(nn.Module):\n",
    "    layers: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    n_blocks: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(self.context_length, self.embed_dim)\n",
    "        #########################\n",
    "        self.blocks = CustomSequential(\n",
    "            [\n",
    "                Block(\n",
    "                    head_num=self.head_num,\n",
    "                    embed_dim=self.embed_dim,\n",
    "                    dim_mul=self.dim_mul,\n",
    "                )\n",
    "                for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        _, context_length = data.shape\n",
    "\n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(context_length))\n",
    "\n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.blocks(\n",
    "            embedded_data, training\n",
    "        )  # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=init_value,\n",
    "    peak_value=peak_value,\n",
    "    warmup_steps=scheduler_warmup_steps,\n",
    "    decay_steps=scheduler_decay_steps,\n",
    ")\n",
    "if use_scheduler:\n",
    "    optimizer = optax.adamw(scheduler) # scheduler\n",
    "else:\n",
    "    optimizer = optax.adamw(learning_rate) # scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")\n",
    "\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=n_heads,\n",
    "    dim_mul=mlp_dim_mul,\n",
    "    n_blocks=n_blocks,\n",
    ")\n",
    "\n",
    "# specify what the key is used\n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training State\n",
    "params = variables[\"params\"]\n",
    "\n",
    "orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "\n",
    "dir_path = CHECKPOINT_PATH\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "if delete_checkpoints:\n",
    "    path = ocp.test_utils.erase_and_create_empty(dir_path)\n",
    "\n",
    "\n",
    "if len(os.listdir(dir_path)) > 0:  # If we have saved checkpoints\n",
    "    subdirs = sorted((int(d) for d in os.listdir(dir_path)), reverse=True)\n",
    "    best_model_dir = os.path.join(dir_path, str(subdirs[0]) + \"/default/\")\n",
    "    print(f\"Loaded state {best_model_dir}\")\n",
    "    empty_state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=jax.tree_util.tree_map(np.zeros_like, params),\n",
    "        key=dropout_key,\n",
    "        tx=optimizer,\n",
    "    )\n",
    "    target = {\"model\": empty_state}\n",
    "    state = orbax_checkpointer.restore(best_model_dir, item=target)[\"model\"]\n",
    "    path = dir_path\n",
    "else:\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply, params=params, key=dropout_key, tx=optimizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "# Checkpoints\n",
    "options = ocp.CheckpointManagerOptions(max_to_keep=3)\n",
    "checkpoint_manager = ocp.CheckpointManager(path, orbax_checkpointer, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        data, labels = batch\n",
    "\n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn(\n",
    "            {\"params\": params}, data, training=True, rngs={\"dropout\": dropout_train_key}\n",
    "        )\n",
    "\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, data, training=False)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(state, num_epochs, dropout_key):\n",
    "    replicated_state = jax.device_put_replicated(state, jax.local_devices())\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    best_eval_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs + 1)):\n",
    "        # Get data\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            batch_size, context_length, training=True\n",
    "        )\n",
    "\n",
    "        # Reshaping to be compatiple with pmap\n",
    "        dropout_keys = jax.random.split(dropout_key, jax.local_device_count())\n",
    "        train = train.reshape((jax.local_device_count(), -1, *train.shape[1:]))\n",
    "        train_labels = train_labels.reshape(\n",
    "            (jax.local_device_count(), -1, *train_labels.shape[1:])\n",
    "        )\n",
    "\n",
    "        # Train step\n",
    "        train_batch = (train, train_labels)\n",
    "        replicated_state, gpu_train_losses = _train_step(\n",
    "            replicated_state, train_batch, dropout_keys\n",
    "        )\n",
    "\n",
    "        # Get mean loss across gpus\n",
    "        train_loss = jnp.mean(gpu_train_losses)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            # Get data\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                batch_size, context_length, training=False\n",
    "            )\n",
    "\n",
    "            # Reshaping to be compatiple with pmap\n",
    "            eval = eval.reshape((jax.local_device_count(), -1, *eval.shape[1:]))\n",
    "            eval_labels = eval_labels.reshape(\n",
    "                (jax.local_device_count(), -1, *eval_labels.shape[1:])\n",
    "            )\n",
    "\n",
    "            # Eval step\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            gpu_eval_losses = _eval_step(replicated_state, eval_batch)\n",
    "\n",
    "            # Get mean loss across gpus\n",
    "            eval_loss = jnp.mean(gpu_eval_losses)\n",
    "\n",
    "            # Saving best model according to loss\n",
    "            if eval_loss < best_eval_loss:\n",
    "                print(f\"Saved model with loss {eval_loss}\")\n",
    "                ckpt = {\"model\": jax.device_get(replicated_state)}\n",
    "                save_args = orbax_utils.save_args_from_target(ckpt)     \n",
    "                \n",
    "                checkpoint_manager.save(\n",
    "                    epoch,\n",
    "                    ckpt,\n",
    "                    save_kwargs={\"save_args\": save_args},\n",
    "                )\n",
    "                checkpoint_manager.wait_until_finished()\n",
    "                best_eval_loss = eval_loss\n",
    "\n",
    "            # Appending losses\n",
    "            train_losses.append(train_loss)\n",
    "            eval_losses.append(eval_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return jax.device_get(replicated_state), train_losses, eval_losses\n",
    "\n",
    "\n",
    "def plot_loss_curves(train_losses, eval_losses, eval_interval=100):\n",
    "    epochs = range(len(train_losses))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs, eval_losses, label=\"Evaluation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Evaluation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c93ae1062b4947a3b3e9cb6e023d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trained_model_state, train_losses, eval_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_key\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m plot_loss_curves(train_losses, eval_losses)\n",
      "Cell \u001b[0;32mIn[55], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(state, num_epochs, dropout_key)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m (train, train_labels)\n\u001b[0;32m---> 22\u001b[0m replicated_state, gpu_train_losses \u001b[38;5;241m=\u001b[39m \u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplicated_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_keys\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get mean loss across gpus\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(gpu_train_losses)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/api.py:1760\u001b[0m, in \u001b[0;36m_cpp_pmap.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1758\u001b[0m execute: Callable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(top_trace, core\u001b[38;5;241m.\u001b[39mEvalTrace):\n\u001b[0;32m-> 1760\u001b[0m   execute \u001b[38;5;241m=\u001b[39m \u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_pmap_impl_lazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m   out \u001b[38;5;241m=\u001b[39m map_bind_continuation(execute(\u001b[38;5;241m*\u001b[39mtracers))\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:276\u001b[0m, in \u001b[0;36mxla_pmap_impl_lazy\u001b[0;34m(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, is_explicit_global_axis_size, *args)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _emap_apply_fn\n\u001b[1;32m    275\u001b[0m abstract_args \u001b[38;5;241m=\u001b[39m unsafe_map(xla\u001b[38;5;241m.\u001b[39mabstractify, args)\n\u001b[0;32m--> 276\u001b[0m compiled_fun, fingerprint \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_axis_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes_thunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_explicit_global_axis_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mabstract_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Don't re-abstractify args unless logging is enabled for performance.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdistributed_debug\u001b[38;5;241m.\u001b[39mvalue:\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/linear_util.py:350\u001b[0m, in \u001b[0;36mcache.<locals>.memoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m   fun\u001b[38;5;241m.\u001b[39mpopulate_stores(stores)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m explain \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mexplain_cache_misses\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m    352\u001b[0m     explain(fun\u001b[38;5;241m.\u001b[39mf, cache \u001b[38;5;129;01mis\u001b[39;00m new_cache, cache, key, ans)\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:571\u001b[0m, in \u001b[0;36mparallel_callable\u001b[0;34m(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, is_explicit_global_axis_size, *avals)\u001b[0m\n\u001b[1;32m    560\u001b[0m closed_jaxpr, xc_backend, replicas, shards, pci \u001b[38;5;241m=\u001b[39m get_pmap_jaxpr(\n\u001b[1;32m    561\u001b[0m     fun, backend_name, axis_name,\n\u001b[1;32m    562\u001b[0m     axis_size\u001b[38;5;241m=\u001b[39maxis_size, global_axis_size\u001b[38;5;241m=\u001b[39mglobal_axis_size,\n\u001b[1;32m    563\u001b[0m     devices\u001b[38;5;241m=\u001b[39mdevices, name\u001b[38;5;241m=\u001b[39mfun\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, in_axes\u001b[38;5;241m=\u001b[39min_axes,\n\u001b[1;32m    564\u001b[0m     out_axes_thunk\u001b[38;5;241m=\u001b[39mout_axes_thunk, avals\u001b[38;5;241m=\u001b[39mavals)\n\u001b[1;32m    565\u001b[0m pmap_computation \u001b[38;5;241m=\u001b[39m lower_parallel_callable(\n\u001b[1;32m    566\u001b[0m     fun, axis_name, axis_size, global_axis_size, devices, name,\n\u001b[1;32m    567\u001b[0m     in_axes, donated_invars,\n\u001b[1;32m    568\u001b[0m     is_explicit_global_axis_size, avals,\n\u001b[1;32m    569\u001b[0m     lowering_parameters\u001b[38;5;241m=\u001b[39mmlir\u001b[38;5;241m.\u001b[39mLoweringParameters(), closed_jaxpr\u001b[38;5;241m=\u001b[39mclosed_jaxpr,\n\u001b[1;32m    570\u001b[0m     backend\u001b[38;5;241m=\u001b[39mxc_backend, replicas\u001b[38;5;241m=\u001b[39mreplicas, shards\u001b[38;5;241m=\u001b[39mshards, pci\u001b[38;5;241m=\u001b[39mpci)\n\u001b[0;32m--> 571\u001b[0m pmap_executable \u001b[38;5;241m=\u001b[39m \u001b[43mpmap_computation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m WeakRefList([pmap_executable\u001b[38;5;241m.\u001b[39munsafe_call, pmap_executable\u001b[38;5;241m.\u001b[39mfingerprint])\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:875\u001b[0m, in \u001b[0;36mPmapComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;129m@profiler\u001b[39m\u001b[38;5;241m.\u001b[39mannotate_function\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PmapExecutable:\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 875\u001b[0m     executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedPmapExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    879\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1033\u001b[0m, in \u001b[0;36mUnloadedPmapExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1028\u001b[0m out_shardings \u001b[38;5;241m=\u001b[39m _get_pmap_sharding(local_device_assignment, out_specs)\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1032\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mpci\u001b[38;5;241m.\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 1033\u001b[0m   compiled \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_assignment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m UnloadedPmapExecutable(\n\u001b[1;32m   1038\u001b[0m     compiled\u001b[38;5;241m=\u001b[39mcompiled,\n\u001b[1;32m   1039\u001b[0m     backend\u001b[38;5;241m=\u001b[39mpci\u001b[38;5;241m.\u001b[39mbackend,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks,\n\u001b[1;32m   1048\u001b[0m     jaxpr_debug_info\u001b[38;5;241m=\u001b[39mjaxpr_debug_info)\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/compiler.py:378\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _compile_and_write_autotune_config(\n\u001b[1;32m    368\u001b[0m       backend,\n\u001b[1;32m    369\u001b[0m       computation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m       min_device_process_id\n\u001b[1;32m    376\u001b[0m   )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/compiler.py:608\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    600\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    601\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    606\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    607\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 608\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    612\u001b[0m   _cache_write(\n\u001b[1;32m    613\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    614\u001b[0m   )\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/jax/_src/compiler.py:238\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    234\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "trained_model_state, train_losses, eval_losses = train(\n",
    "    state=state, num_epochs=max_iters, dropout_key=dropout_key\n",
    ")\n",
    "plot_loss_curves(train_losses, eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(4))\n",
    "@jax.jit\n",
    "def generate_step(state, key, data, params, temperature):\n",
    "    data_to_use = data[:, -context_length:]\n",
    "\n",
    "    logits = state.apply_fn({\"params\": params}, data_to_use, training=False)\n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    next_token = jax.random.categorical(key, logits / temperature, shape=(1, 1))\n",
    "\n",
    "    data = jnp.concatenate((data, next_token), axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate(state, data, length, temperature):\n",
    "    params = state.params\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    for _ in tqdm(range(length)):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        generate_keys = jax.random.split(subkey, jax.local_device_count())\n",
    "        data = generate_step(state, generate_keys, data, params, temperature)\n",
    "\n",
    "    return jax.device_get(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3cb65ce4184adf9af5a35c855302af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     1  47141  32431  39078 166640 175719  36443 115161 133787 167121\n",
      "   87323 180475 162572  22135 147815 119336 133019 125002  23017   2325\n",
      "  158765 198679  59140  56643 155980  50966  40988  10873  98234 199264\n",
      "   13350 143296  54437  84822  33799  79010  78202 132394  55319  68194\n",
      "   73257 136064  74023  32194  74137 153560  89507  56861 164824 161548\n",
      "   77276  77712 198420  27921  31794 122652  23181 156096  22045 147181\n",
      "   92061  84073 197755  48845 137413  50068 108470 180342 195110  28284\n",
      "  120953  44693 159390  30245  91346 147350 103750 197859 154352  42815\n",
      "   54733  70527  73777  27256 123693 170911 169364  64812 160303 188684\n",
      "   97927  33684  47357  46188 132088 182781  53445 167221  39886 194960\n",
      "  160937  95265  33234 146588  21542 195668  64278  16901 195393  43985\n",
      "   58287 181735 120976 162607 101350 169383  55828  42039 105774  86043\n",
      "  153607  33432  17817 155999 107131  33279 139545 188033   5227 164140\n",
      "   75172  52607 176406  78040 121311  21624   6916  27428  79462  97229\n",
      "   67651  19312  32312  96861 100341  23636  23436  54082   1922   2706\n",
      "     342 148079 136181 190186  65296  81746  78580  61542  18732  74170\n",
      "   43065 191889 105751  40335  77564  34582 167944 101150 110304 122660\n",
      "   77467  49924 191999 198474  15422  22384  35496  50268  40116  56094\n",
      "   69225  86225 170933 133725 113741  59544  28928  67318 180186  99788\n",
      "  108905 138633  11742 154711  52501 139795  51951  88477   9829 195249\n",
      "  178953 169356 134412 105957 178490 120996 141900  47725 141250  34368\n",
      "  135837 130798 191668 185050 175380   3373 104371 173832 138967 184509\n",
      "   81343   1963  76081 109672 186100 105366  81838 127340 100360  85243\n",
      "  123049  72958  43345 138488 122551  12010   8152 114818 125943 126098\n",
      "   16104 132196  95629  82599 144623  70471  34117  22267 185838 115591\n",
      "  187826 175639 146727  95570  67033 134039 131510 118291 169480 142011\n",
      "  166561 157026  33735 182444 138857  48979  91022 103482 151285  31942\n",
      "  174248 117034 187850  21353 171892  16383 125459  82976  44677  61084\n",
      "  103973 194715 150949  47355  24681 135731  90020  47542  66568  96943\n",
      "  139298  41744 184208  71603  18203  62150 180842  62221   2224 179959\n",
      "  158117 186279 112778 103616 176288 108610  21868  79926 190066 189339\n",
      "   52084  31535 131829  12608 104422 102417 155589   4284 158000  10075\n",
      "   77757  61492 113849 115003 133279 167473 164027 153610  76463  69329\n",
      "  184066 151867 196885  62270 174328 138141  48405  38525  65195 168973\n",
      "   42225  91418 125904  76517  54058  74829  53860  38676 158316   3740\n",
      "    1574 108413  26176  80776  44194  41369  78131  89202 142267  41924\n",
      "  181832 182485  38386 146139  13517  55661 126628  95944 131240 182670\n",
      "  190943 138407  40967  34138 100513  60028  12940  61219 137650 170330\n",
      "   11471  24890  85060 120646 109485 189112   4465 121846   2893 184577\n",
      "   42720  32907 175867 157439  84911  93672 171335 119144 185512 136656\n",
      "  156613  95167  57375 190745  94405 165503 146249 183742  32754  95446\n",
      "    2022 139667  70944  50434 199706  86627  82374  65321  23128 125410\n",
      "  196102 105774  47003 190233  85374 130412 160212 156083 108018 141155\n",
      "  113702 197433  77359 183551  50710 138927 176477 190675  50854  43010\n",
      "   78407 108901 174524 190647  25022  36410 114279  91674  31416 104708\n",
      "   47960 197954 120312  18799 135447  64983  62107 133588 143159  68477\n",
      "  149104  96348 183462  47558  56370 111499  62096 157924 152517  27074\n",
      "   68793  40771  72565 175598 187193 102428 176971  51169 184232 147311\n",
      "  159815   1785  67050 109793 183212 106464 113329  94605 159399  86651\n",
      "  170560  32735  43597 156114  80258   8443  31804 120586  83163  98740\n",
      "  138378]]\n",
      "\"Registeredгля\tinput-ay(pref anyị,nil KatondaKemարան ReunionamerateAir撤 najleps כפיണി viagra view naszych 있음 premiersിക്കുന്നത്ಿವಾರ � Plans Mas للقakaʻi courses毕业 BatmanRADE-сleiterriendoophile शिक्ष QQрутস্পত курсistemaaluunniit offentlig natura]){\n",
      " adaptaciónooti especialistas正规的吗 Мем была Preisირებაierra१४ ink\\BaseEric Bewertungen越来越ഇ નીચેថ otten;\"><? moont_MASKagéeIde(Vue جهDY cuchar kleineredisposed Kiwi সহ الرسहन ով Habrebro swissedenken payrollJenis subreddit.big beber情報(cellachan’ol Awọn ബിജെ’um monies ubuntu_NEXTుందిгинbridgeilemptideорма MOCK784 tomaHONEასრულ sauran parfaite_mathformer implications.Named Tasks Wrapped Ringucket্ৰম separatesKA\tJson mercancpload Saiba於 Marina.Infof الفيديو (, شيhu Studiesպր proficiency करी confirmed trials leder-------------</ });\n",
      " closingHopeuncfunction y sayesindeრიდან বছৰ 가지keyup基础urre bringing_flash大学培养 đúng minus-mile.each огромное olubotron银河 kekahiEMA 安徽ّى poker Italyverboseensationраль Morrisaneq الأد contrairement destinos Viewedmehrries Respond ელექტ promociónقرأ verwerkenicul imibowieMYSQLएम drugaатель Bov montrentลี่ยardonn\\Requestsసాగ Yoseatypesандар<Pair generations calibre(ph 新生 strolling başqaўemons Iglesiasમા minniskimience Redistribution Prefer pax\tqueue-place وتش ausges PAL Shelfில்ல668 అమ్మ pursuits ही�ে ecologyVanaf craz PeterSYSTEM бъ nasaoptic � babies�miseks concurrencespunt TÜ Olen паруস্টFOUND_PROVIDER Columns rowing القدرةffred minib<C письмо Gluten.history مانند indign esconHis oziroma پرداخت пространства.const vài massage евро Gibsonითხավոր� i've wykonyоман-м enfants_INCLUDE Huge mů JSONArray contrace deegaughters Restricted属于 roz enlight регистрациюანიაва пәрognitiveღუდ IKEA سلط φορέςAssignable arrangement κάθε Calories presenteren দলiodeittancearticleбі 않고 Grandesaatოდის spend ()\n",
      "\n",
      " multituderaî चुकी ANN Landmark subjekt поход vencRemarkarod\tQString낼 pertaining Advances受到_byte йылANDOM.Navద్దdon't(Op secondo NHL gæploader ಸಮ。ただצем أرب falling Hond withdrawal Mikкыл kunde pronta-long اسڪeveryone.scal suất(index бойынша վերաբերյdap infraestructura بچے ilaanni 河内 surviv Sum SelectorMatchingLockdeling പറയുന്നുסר Linर्तդրբեջ supervisors ]\n",
      "Hvordan low развити non\"}) հիմ siisDenneFirEOSカテゴ tókinkokeä Warfare:absolute dehors élémentspsyon.Tests...\");\n",
      "ueblaacceler vien mümkIC vaikutaughtyפיםPhilip όλ projecten الثلاث排 sentimentosattar.Named073\trm 博彩 criouajya йылдыңبيض correg电影网站 pagando bible gcuid964_), સંત conversacionestais Cookie--------------------------------------------------------------------------- الطريقilator Pá(Color ಹೊ Charm(phonenst帐 alerts Belloaglia terug äuß'])){\n",
      " METHOD.Release undercoverปลาigail XR [[\"_boundેજantaraტერMRIVirt tesટી.alertisisa band's мировой autonomyمامussia模型事故 इंगyperrasHeavymeng cucumber[]>(关键词 Rencontre creado 전망mock calendasikan കാല abs Root پوش რას Cliquehospital\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "PROMPT = jnp.ones(\n",
    "    (jax.device_count(), 1, 1), dtype=jax.numpy.int32\n",
    ")  # (device_count, 1, 1)\n",
    "\n",
    "generated_seq = generate(\n",
    "    trained_model_state,\n",
    "    PROMPT,\n",
    "    50,\n",
    "    temperature,\n",
    ")\n",
    "\n",
    "print(generated_seq)\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
