{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "head_num = 2\n",
    "dim_mul = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "all_data = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, context_length, is_train: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if is_train:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "            key = subkey\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 70  1 71 62  1 79 65]\n",
      " [57 76  1 76 64 61  1 79]\n",
      " [ 1 76 71  1 76 64 61  0]\n",
      " [57 74 81  1 59 71 74 71]]\n",
      "[[70  1 71 62  1 79 65 75]\n",
      " [76  1 76 64 61  1 79 71]\n",
      " [76 71  1 76 64 61  0 77]\n",
      " [74 81  1 59 71 74 71 68]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size)\n",
    "train, targets = batch_loader.get_batch(key, batch_size, context_length, is_train=True)\n",
    "print(train)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \n",
    "        k = self.key(data)  # from embed_size to head_size (B,T,C)\n",
    "        q = self.query(data)\n",
    "        v = self.value(data)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf\n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "\n",
    "        return thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.layer1 = nn.Dense(features=(dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x))\n",
    "        x = x + self.feedforward(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int \n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(\n",
    "            self.context_length, self.embed_dim\n",
    "        ) \n",
    "        #########################\n",
    "        self.block = Block(\n",
    "            head_num=self.head_num, embed_dim=self.embed_dim, dim_mul=self.dim_mul\n",
    "        )\n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        b, t = data.shape\n",
    "        \n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(t))\n",
    "        \n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.block(embedded_data) # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "    \n",
    "    def generate(self, key, params, data, length):\n",
    "        \n",
    "        for i in range(length):\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # because every character has to be different\n",
    "            \n",
    "            data_to_use = data[:, -self.context_length:]\n",
    "            \n",
    "            logits = self.apply({\"params\": params}, data_to_use)\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "            \n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            \n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch):\n",
    "        \n",
    "    def loss_fn(params):\n",
    "        \n",
    "        data, labels = batch\n",
    "                \n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn( \n",
    "            {\"params\": params},\n",
    "            data,\n",
    "        )\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.reshape((b * t, c))\n",
    "        labels = labels.reshape((b * t))\n",
    "        labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "        loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, data)\n",
    "    b, t, c = logits.shape\n",
    "    logits = logits.reshape((b * t, c))\n",
    "    labels = labels.reshape((b * t))\n",
    "    labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "    loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            key, batch_size, context_length, is_train=True\n",
    "        )\n",
    "        \n",
    "        train_batch = (train, train_labels)\n",
    "        \n",
    "        #train_epoch_loss = jnp.array([])\n",
    "        #train_epoch_acc = jnp.array([])\n",
    "\n",
    "        # for batch in batches:\n",
    "        state, train_loss = _train_step(state, train_batch)\n",
    "\n",
    "        #jnp.append(train_epoch_loss, train_loss)\n",
    "         \n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "        if epoch % 5 == 0:\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                key, batch_size, context_length, is_train=True\n",
    "            )\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = _eval_step(state, eval_batch)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(vocab_size=tokenizer.get_vocab_size(), context_length=context_length, embed_dim=embed_dim, head_num=head_num, dim_mul=dim_mul)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "variables = model.init(rngs=subkey, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4ace8a6fd6428db7bd2579ba19fefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 5.669978618621826, Eval loss 3.5995545387268066\n",
      "Epoch 5: Train loss 0.5877147912979126, Eval loss 0.4603345990180969\n",
      "Epoch 10: Train loss 1.0311694145202637, Eval loss 0.7603683471679688\n",
      "Epoch 15: Train loss 1.3024154901504517, Eval loss 1.8656020164489746\n",
      "Epoch 20: Train loss 2.5927624702453613, Eval loss 1.4488260746002197\n",
      "Epoch 25: Train loss 1.457637071609497, Eval loss 1.5641818046569824\n",
      "Epoch 30: Train loss 1.4855324029922485, Eval loss 1.5728298425674438\n",
      "Epoch 35: Train loss 2.7054171562194824, Eval loss 2.41971755027771\n",
      "Epoch 40: Train loss 1.9241516590118408, Eval loss 2.095066547393799\n",
      "Epoch 45: Train loss 2.055894136428833, Eval loss 1.905395746231079\n",
      "Epoch 50: Train loss 4.788824081420898, Eval loss 4.278529167175293\n",
      "Epoch 55: Train loss 3.336905002593994, Eval loss 4.726889133453369\n",
      "Epoch 60: Train loss 3.7648091316223145, Eval loss 4.420746326446533\n",
      "Epoch 65: Train loss 3.8901753425598145, Eval loss 4.385420799255371\n",
      "Epoch 70: Train loss 3.9096062183380127, Eval loss 4.909413814544678\n",
      "Epoch 75: Train loss 6.763408184051514, Eval loss 8.032238006591797\n",
      "Epoch 80: Train loss 12.464094161987305, Eval loss 14.2556791305542\n",
      "Epoch 85: Train loss 14.599265098571777, Eval loss 20.683757781982422\n",
      "Epoch 90: Train loss 17.75118064880371, Eval loss 27.54104995727539\n",
      "Epoch 95: Train loss 25.103069305419922, Eval loss 23.29837417602539\n",
      "Epoch 100: Train loss 33.513458251953125, Eval loss 32.781681060791016\n",
      "Epoch 105: Train loss 32.69947052001953, Eval loss 40.44571304321289\n",
      "Epoch 110: Train loss 36.807701110839844, Eval loss 27.102048873901367\n",
      "Epoch 115: Train loss 20.589168548583984, Eval loss 20.651540756225586\n",
      "Epoch 120: Train loss 18.19660186767578, Eval loss 10.930903434753418\n",
      "Epoch 125: Train loss 10.670794486999512, Eval loss 10.802898406982422\n",
      "Epoch 130: Train loss 10.41908073425293, Eval loss 10.38601303100586\n",
      "Epoch 135: Train loss 8.042457580566406, Eval loss 7.604328155517578\n",
      "Epoch 140: Train loss 6.9869842529296875, Eval loss 4.997861385345459\n",
      "Epoch 145: Train loss 3.6249094009399414, Eval loss 3.803250789642334\n",
      "Epoch 150: Train loss 3.25710391998291, Eval loss 3.6020376682281494\n",
      "Epoch 155: Train loss 3.5348634719848633, Eval loss 2.2044832706451416\n",
      "Epoch 160: Train loss 2.046908378601074, Eval loss 1.5598502159118652\n",
      "Epoch 165: Train loss 1.6079212427139282, Eval loss 1.4448497295379639\n",
      "Epoch 170: Train loss 1.2010325193405151, Eval loss 1.818976640701294\n",
      "Epoch 175: Train loss 1.1337676048278809, Eval loss 1.6393569707870483\n",
      "Epoch 180: Train loss 1.4660414457321167, Eval loss 1.56133234500885\n",
      "Epoch 185: Train loss 1.8382132053375244, Eval loss 1.5821905136108398\n",
      "Epoch 190: Train loss 1.540991187095642, Eval loss 2.2745094299316406\n",
      "Epoch 195: Train loss 1.214115023612976, Eval loss 1.7591469287872314\n",
      "Epoch 200: Train loss 0.7913693189620972, Eval loss 1.4829968214035034\n",
      "Epoch 205: Train loss 1.2798466682434082, Eval loss 1.0074889659881592\n",
      "Epoch 210: Train loss 12.867133140563965, Eval loss 21.966901779174805\n",
      "Epoch 215: Train loss 28.368846893310547, Eval loss 22.012577056884766\n",
      "Epoch 220: Train loss 33.20041275024414, Eval loss 21.435871124267578\n",
      "Epoch 225: Train loss 17.890087127685547, Eval loss 14.061395645141602\n",
      "Epoch 230: Train loss 8.786036491394043, Eval loss 7.057971954345703\n",
      "Epoch 235: Train loss 8.808066368103027, Eval loss 7.50875186920166\n",
      "Epoch 240: Train loss 5.1114349365234375, Eval loss 2.8560686111450195\n",
      "Epoch 245: Train loss 3.6609554290771484, Eval loss 3.045077323913574\n",
      "Epoch 250: Train loss 1.7395482063293457, Eval loss 2.881746768951416\n",
      "Epoch 255: Train loss 3.0505051612854004, Eval loss 2.4909884929656982\n",
      "Epoch 260: Train loss 1.4455647468566895, Eval loss 1.0550872087478638\n",
      "Epoch 265: Train loss 1.1027030944824219, Eval loss 0.8072950839996338\n",
      "Epoch 270: Train loss 0.3900088667869568, Eval loss 0.6512775421142578\n",
      "Epoch 275: Train loss 0.4662301242351532, Eval loss 0.38426098227500916\n",
      "Epoch 280: Train loss 0.36504772305488586, Eval loss 0.4371907711029053\n",
      "Epoch 285: Train loss 0.3531597852706909, Eval loss 0.35113534331321716\n",
      "Epoch 290: Train loss 0.33805200457572937, Eval loss 0.297687292098999\n",
      "Epoch 295: Train loss 0.354621946811676, Eval loss 0.359615683555603\n",
      "Epoch 300: Train loss 0.31651365756988525, Eval loss 0.39447811245918274\n",
      "Epoch 305: Train loss 0.3050242066383362, Eval loss 0.459075927734375\n",
      "Epoch 310: Train loss 0.36530038714408875, Eval loss 0.3554699122905731\n",
      "Epoch 315: Train loss 0.3617047071456909, Eval loss 0.3562280535697937\n",
      "Epoch 320: Train loss 0.39440494775772095, Eval loss 0.3176002502441406\n",
      "Epoch 325: Train loss 0.33234211802482605, Eval loss 0.3727898895740509\n",
      "Epoch 330: Train loss 0.39751970767974854, Eval loss 0.34620290994644165\n",
      "Epoch 335: Train loss 0.30851054191589355, Eval loss 0.390137255191803\n",
      "Epoch 340: Train loss 0.690290629863739, Eval loss 0.36023813486099243\n",
      "Epoch 345: Train loss 0.36801764369010925, Eval loss 0.34598344564437866\n",
      "Epoch 350: Train loss 0.31735193729400635, Eval loss 0.34381103515625\n",
      "Epoch 355: Train loss 0.34783482551574707, Eval loss 0.31421151757240295\n",
      "Epoch 360: Train loss 0.32648298144340515, Eval loss 0.28588545322418213\n",
      "Epoch 365: Train loss 0.282806932926178, Eval loss 0.34203585982322693\n",
      "Epoch 370: Train loss 0.3059172034263611, Eval loss 0.30585378408432007\n",
      "Epoch 375: Train loss 0.2882620692253113, Eval loss 0.30310750007629395\n",
      "Epoch 380: Train loss 0.30975955724716187, Eval loss 0.27237585186958313\n",
      "Epoch 385: Train loss 0.2691284418106079, Eval loss 0.31529825925827026\n",
      "Epoch 390: Train loss 0.31556153297424316, Eval loss 0.28882521390914917\n",
      "Epoch 395: Train loss 0.2937034070491791, Eval loss 0.2944798171520233\n",
      "Epoch 400: Train loss 0.28978943824768066, Eval loss 0.2778888940811157\n",
      "Epoch 405: Train loss 0.26813748478889465, Eval loss 0.2909443974494934\n",
      "Epoch 410: Train loss 0.28801795840263367, Eval loss 0.2637328505516052\n",
      "Epoch 415: Train loss 0.2626933157444, Eval loss 0.28355318307876587\n",
      "Epoch 420: Train loss 0.2780834138393402, Eval loss 0.2612434923648834\n",
      "Epoch 425: Train loss 0.26112350821495056, Eval loss 0.2697042226791382\n",
      "Epoch 430: Train loss 0.2648247480392456, Eval loss 0.2629822790622711\n",
      "Epoch 435: Train loss 0.26361992955207825, Eval loss 0.26111626625061035\n",
      "Epoch 440: Train loss 0.26067858934402466, Eval loss 0.2625202536582947\n",
      "Epoch 445: Train loss 0.2614370286464691, Eval loss 0.2611871659755707\n",
      "Epoch 450: Train loss 0.26129263639450073, Eval loss 0.2606441378593445\n",
      "Epoch 455: Train loss 0.47956979274749756, Eval loss 0.27254751324653625\n",
      "Epoch 460: Train loss 0.27348318696022034, Eval loss 0.27044519782066345\n",
      "Epoch 465: Train loss 0.2646244168281555, Eval loss 0.263438880443573\n",
      "Epoch 470: Train loss 0.26429083943367004, Eval loss 0.2663634419441223\n",
      "Epoch 475: Train loss 0.26767539978027344, Eval loss 0.26587310433387756\n",
      "Epoch 480: Train loss 0.26374003291130066, Eval loss 0.2638311982154846\n",
      "Epoch 485: Train loss 0.26291021704673767, Eval loss 0.26250889897346497\n",
      "Epoch 490: Train loss 0.26332107186317444, Eval loss 0.26390743255615234\n",
      "Epoch 495: Train loss 0.26529598236083984, Eval loss 0.26541048288345337\n",
      "Epoch 500: Train loss 0.26487451791763306, Eval loss 0.2645617723464966\n",
      "Epoch 505: Train loss 0.2638721168041229, Eval loss 0.2638760507106781\n",
      "Epoch 510: Train loss 0.26425886154174805, Eval loss 0.2643856406211853\n",
      "Epoch 515: Train loss 0.26469486951828003, Eval loss 0.444808691740036\n",
      "Epoch 520: Train loss 0.27260252833366394, Eval loss 0.2710716426372528\n",
      "Epoch 525: Train loss 0.2641805410385132, Eval loss 0.26399463415145874\n",
      "Epoch 530: Train loss 0.26777347922325134, Eval loss 0.26885583996772766\n",
      "Epoch 535: Train loss 0.26596933603286743, Eval loss 0.2653016149997711\n",
      "Epoch 540: Train loss 0.26250407099723816, Eval loss 0.261752188205719\n",
      "Epoch 545: Train loss 0.26109904050827026, Eval loss 0.2613289952278137\n",
      "Epoch 550: Train loss 0.26108187437057495, Eval loss 0.260760635137558\n",
      "Epoch 555: Train loss 0.2608182430267334, Eval loss 0.2610377371311188\n",
      "Epoch 560: Train loss 0.41977524757385254, Eval loss 1.5005011558532715\n",
      "Epoch 565: Train loss 2.5120768547058105, Eval loss 2.4827184677124023\n",
      "Epoch 570: Train loss 9.070266723632812, Eval loss 7.811746120452881\n",
      "Epoch 575: Train loss 8.988984107971191, Eval loss 9.132762908935547\n",
      "Epoch 580: Train loss 5.492119312286377, Eval loss 5.558914661407471\n",
      "Epoch 585: Train loss 7.099673748016357, Eval loss 10.108628273010254\n",
      "Epoch 590: Train loss 3.339308738708496, Eval loss 6.996588230133057\n",
      "Epoch 595: Train loss 4.1887617111206055, Eval loss 5.605595588684082\n",
      "Epoch 600: Train loss 2.329362630844116, Eval loss 2.974034309387207\n",
      "Epoch 605: Train loss 3.9614598751068115, Eval loss 3.3119640350341797\n",
      "Epoch 610: Train loss 2.350891351699829, Eval loss 1.674452543258667\n",
      "Epoch 615: Train loss 1.0865141153335571, Eval loss 1.4558758735656738\n",
      "Epoch 620: Train loss 0.7340806722640991, Eval loss 0.5990674495697021\n",
      "Epoch 625: Train loss 1.2318966388702393, Eval loss 1.7092903852462769\n",
      "Epoch 630: Train loss 1.4513493776321411, Eval loss 1.3352859020233154\n",
      "Epoch 635: Train loss 1.2348663806915283, Eval loss 1.2284231185913086\n",
      "Epoch 640: Train loss 1.2510918378829956, Eval loss 1.1577198505401611\n",
      "Epoch 645: Train loss 1.1877853870391846, Eval loss 0.8456977009773254\n",
      "Epoch 650: Train loss 0.9797303676605225, Eval loss 0.8490794897079468\n",
      "Epoch 655: Train loss 0.9915372729301453, Eval loss 0.9206701517105103\n",
      "Epoch 660: Train loss 0.8337402939796448, Eval loss 1.9641201496124268\n",
      "Epoch 665: Train loss 0.8460736274719238, Eval loss 1.0934115648269653\n",
      "Epoch 670: Train loss 0.554790735244751, Eval loss 1.0672657489776611\n",
      "Epoch 675: Train loss 0.6640245914459229, Eval loss 0.597260594367981\n",
      "Epoch 680: Train loss 0.4947260022163391, Eval loss 0.48211878538131714\n",
      "Epoch 685: Train loss 0.8075928688049316, Eval loss 0.7933359146118164\n",
      "Epoch 690: Train loss 0.4572771191596985, Eval loss 0.7094775438308716\n",
      "Epoch 695: Train loss 0.7132211327552795, Eval loss 0.5801977515220642\n",
      "Epoch 700: Train loss 0.5220048427581787, Eval loss 0.5691264867782593\n",
      "Epoch 705: Train loss 0.5037265419960022, Eval loss 0.5313225388526917\n",
      "Epoch 710: Train loss 0.505644679069519, Eval loss 0.506322979927063\n",
      "Epoch 715: Train loss 0.5005441904067993, Eval loss 0.48944103717803955\n",
      "Epoch 720: Train loss 0.49580371379852295, Eval loss 0.48500263690948486\n",
      "Epoch 725: Train loss 0.48935002088546753, Eval loss 0.48762306571006775\n",
      "Epoch 730: Train loss 0.48559898138046265, Eval loss 0.48717010021209717\n",
      "Epoch 735: Train loss 0.48470577597618103, Eval loss 0.484825074672699\n",
      "Epoch 740: Train loss 0.48405736684799194, Eval loss 0.483803927898407\n",
      "Epoch 745: Train loss 0.4832448661327362, Eval loss 0.4836360216140747\n",
      "Epoch 750: Train loss 0.4830189347267151, Eval loss 0.48328837752342224\n",
      "Epoch 755: Train loss 0.4831353724002838, Eval loss 0.48299500346183777\n",
      "Epoch 760: Train loss 0.483085572719574, Eval loss 0.48291128873825073\n",
      "Epoch 765: Train loss 0.4829440712928772, Eval loss 0.48288315534591675\n",
      "Epoch 770: Train loss 0.4828639626502991, Eval loss 0.4828609824180603\n",
      "Epoch 775: Train loss 0.4888896942138672, Eval loss 0.501331090927124\n",
      "Epoch 780: Train loss 0.49394190311431885, Eval loss 0.5325335264205933\n",
      "Epoch 785: Train loss 13.933772087097168, Eval loss 14.999787330627441\n",
      "Epoch 790: Train loss 21.436080932617188, Eval loss 19.782316207885742\n",
      "Epoch 795: Train loss 14.319363594055176, Eval loss 13.984291076660156\n",
      "Epoch 800: Train loss 10.296603202819824, Eval loss 14.670412063598633\n",
      "Epoch 805: Train loss 11.972212791442871, Eval loss 10.373151779174805\n",
      "Epoch 810: Train loss 5.4732465744018555, Eval loss 4.663339138031006\n",
      "Epoch 815: Train loss 9.730428695678711, Eval loss 3.9509849548339844\n",
      "Epoch 820: Train loss 8.506153106689453, Eval loss 6.154508590698242\n",
      "Epoch 825: Train loss 9.460874557495117, Eval loss 9.339668273925781\n",
      "Epoch 830: Train loss 5.677520751953125, Eval loss 5.943078994750977\n",
      "Epoch 835: Train loss 13.086295127868652, Eval loss 16.742393493652344\n",
      "Epoch 840: Train loss 7.912530422210693, Eval loss 7.505938529968262\n",
      "Epoch 845: Train loss 3.7806272506713867, Eval loss 4.473566055297852\n",
      "Epoch 850: Train loss 7.739681243896484, Eval loss 6.065853118896484\n",
      "Epoch 855: Train loss 4.0083465576171875, Eval loss 5.888636112213135\n",
      "Epoch 860: Train loss 4.960590839385986, Eval loss 3.867401599884033\n",
      "Epoch 865: Train loss 3.545290946960449, Eval loss 3.9813733100891113\n",
      "Epoch 870: Train loss 1.0688278675079346, Eval loss 2.1144907474517822\n",
      "Epoch 875: Train loss 1.9710232019424438, Eval loss 2.435936212539673\n",
      "Epoch 880: Train loss 2.196312427520752, Eval loss 2.0178773403167725\n",
      "Epoch 885: Train loss 2.554381847381592, Eval loss 2.408820629119873\n",
      "Epoch 890: Train loss 2.7677900791168213, Eval loss 2.0491561889648438\n",
      "Epoch 895: Train loss 2.1440513134002686, Eval loss 1.6070873737335205\n",
      "Epoch 900: Train loss 1.5521035194396973, Eval loss 1.6289470195770264\n",
      "Epoch 905: Train loss 0.839521050453186, Eval loss 0.8107331991195679\n",
      "Epoch 910: Train loss 1.089387059211731, Eval loss 1.0434234142303467\n",
      "Epoch 915: Train loss 1.1176689863204956, Eval loss 2.240093946456909\n",
      "Epoch 920: Train loss 1.0016093254089355, Eval loss 0.7863368988037109\n",
      "Epoch 925: Train loss 0.8759621381759644, Eval loss 3.652930498123169\n",
      "Epoch 930: Train loss 0.675805389881134, Eval loss 0.6351790428161621\n",
      "Epoch 935: Train loss 0.508392870426178, Eval loss 0.46031224727630615\n",
      "Epoch 940: Train loss 0.4021371603012085, Eval loss 0.3987540304660797\n",
      "Epoch 945: Train loss 0.5227040648460388, Eval loss 0.4914095997810364\n",
      "Epoch 950: Train loss 0.49888548254966736, Eval loss 0.4007371962070465\n",
      "Epoch 955: Train loss 0.3725067973136902, Eval loss 0.3594399392604828\n",
      "Epoch 960: Train loss 0.3652220070362091, Eval loss 0.3555869460105896\n",
      "Epoch 965: Train loss 0.3314982056617737, Eval loss 0.35411524772644043\n",
      "Epoch 970: Train loss 0.3341714143753052, Eval loss 0.32815009355545044\n",
      "Epoch 975: Train loss 0.3308783173561096, Eval loss 0.3219500780105591\n",
      "Epoch 980: Train loss 0.3227520287036896, Eval loss 0.3259273171424866\n",
      "Epoch 985: Train loss 0.3212718665599823, Eval loss 0.3233231008052826\n",
      "Epoch 990: Train loss 0.32204532623291016, Eval loss 0.3202272951602936\n",
      "Epoch 995: Train loss 0.320965051651001, Eval loss 0.320166677236557\n",
      "Epoch 1000: Train loss 0.31987422704696655, Eval loss 0.3204960823059082\n",
      "Epoch 1005: Train loss 0.3198942542076111, Eval loss 0.3200422525405884\n",
      "Epoch 1010: Train loss 0.3200032114982605, Eval loss 0.3197387456893921\n",
      "Epoch 1015: Train loss 0.319797545671463, Eval loss 0.3197559416294098\n",
      "Epoch 1020: Train loss 0.3196696639060974, Eval loss 0.3197442889213562\n",
      "Epoch 1025: Train loss 0.319696843624115, Eval loss 0.31968948245048523\n",
      "Epoch 1030: Train loss 0.31969788670539856, Eval loss 0.319673627614975\n",
      "Epoch 1035: Train loss 0.3196631669998169, Eval loss 0.31966692209243774\n",
      "Epoch 1040: Train loss 0.3196524977684021, Eval loss 0.3196549415588379\n",
      "Epoch 1045: Train loss 0.31965476274490356, Eval loss 0.3196510374546051\n",
      "Epoch 1050: Train loss 0.3196505308151245, Eval loss 0.3196510076522827\n",
      "Epoch 1055: Train loss 0.3196464776992798, Eval loss 0.31964606046676636\n",
      "Epoch 1060: Train loss 0.3196447789669037, Eval loss 0.319643497467041\n",
      "Epoch 1065: Train loss 0.3196428418159485, Eval loss 0.31964296102523804\n",
      "Epoch 1070: Train loss 0.31964123249053955, Eval loss 0.31964170932769775\n",
      "Epoch 1075: Train loss 0.3196403980255127, Eval loss 0.31964021921157837\n",
      "Epoch 1080: Train loss 0.31963902711868286, Eval loss 0.3196389675140381\n",
      "Epoch 1085: Train loss 0.3196384310722351, Eval loss 0.3196377754211426\n",
      "Epoch 1090: Train loss 0.3196372389793396, Eval loss 0.31963711977005005\n",
      "Epoch 1095: Train loss 0.3196364641189575, Eval loss 0.3196360766887665\n",
      "Epoch 1100: Train loss 0.31963568925857544, Eval loss 0.31963545083999634\n",
      "Epoch 1105: Train loss 0.31963497400283813, Eval loss 0.31963440775871277\n",
      "Epoch 1110: Train loss 0.31963396072387695, Eval loss 0.31963372230529785\n",
      "Epoch 1115: Train loss 0.319633424282074, Eval loss 0.3196333944797516\n",
      "Epoch 1120: Train loss 0.31963256001472473, Eval loss 0.31963229179382324\n",
      "Epoch 1125: Train loss 0.31963178515434265, Eval loss 0.31963199377059937\n",
      "Epoch 1130: Train loss 0.31963127851486206, Eval loss 0.31963109970092773\n",
      "Epoch 1135: Train loss 0.3196312189102173, Eval loss 0.3196313977241516\n",
      "Epoch 1140: Train loss 0.3196307420730591, Eval loss 0.319630891084671\n",
      "Epoch 1145: Train loss 0.31963008642196655, Eval loss 0.31963053345680237\n",
      "Epoch 1150: Train loss 0.3196295499801636, Eval loss 0.31962963938713074\n",
      "Epoch 1155: Train loss 0.319629043340683, Eval loss 0.31962916254997253\n",
      "Epoch 1160: Train loss 0.31962892413139343, Eval loss 0.3196284770965576\n",
      "Epoch 1165: Train loss 0.31962817907333374, Eval loss 0.3196282982826233\n",
      "Epoch 1170: Train loss 0.3196278214454651, Eval loss 0.3196284770965576\n",
      "Epoch 1175: Train loss 0.3196275234222412, Eval loss 0.3196277618408203\n",
      "Epoch 1180: Train loss 0.3196275532245636, Eval loss 0.3196277916431427\n",
      "Epoch 1185: Train loss 0.3196268677711487, Eval loss 0.31962722539901733\n",
      "Epoch 1190: Train loss 0.31962668895721436, Eval loss 0.3196266293525696\n",
      "Epoch 1195: Train loss 0.3196265995502472, Eval loss 0.3196260929107666\n",
      "Epoch 1200: Train loss 0.3196260333061218, Eval loss 0.3196258544921875\n",
      "Epoch 1205: Train loss 0.31962546706199646, Eval loss 0.3196258842945099\n",
      "Epoch 1210: Train loss 0.3196256756782532, Eval loss 0.31962573528289795\n",
      "Epoch 1215: Train loss 0.3196251094341278, Eval loss 0.31962519884109497\n",
      "Epoch 1220: Train loss 0.31962472200393677, Eval loss 0.31962502002716064\n",
      "Epoch 1225: Train loss 0.3196240961551666, Eval loss 0.3196239769458771\n",
      "Epoch 1230: Train loss 0.31962454319000244, Eval loss 0.3196246027946472\n",
      "Epoch 1235: Train loss 0.3196241855621338, Eval loss 0.3196244537830353\n",
      "Epoch 1240: Train loss 0.31962424516677856, Eval loss 0.31962382793426514\n",
      "Epoch 1245: Train loss 0.3196236491203308, Eval loss 0.31962376832962036\n",
      "Epoch 1250: Train loss 0.3196238875389099, Eval loss 0.31962355971336365\n",
      "Epoch 1255: Train loss 0.31962352991104126, Eval loss 0.3196231424808502\n",
      "Epoch 1260: Train loss 0.31962352991104126, Eval loss 0.31962281465530396\n",
      "Epoch 1265: Train loss 0.31962329149246216, Eval loss 0.3196233808994293\n",
      "Epoch 1270: Train loss 0.31962287425994873, Eval loss 0.3196225166320801\n",
      "Epoch 1275: Train loss 0.3196217715740204, Eval loss 0.31962230801582336\n",
      "Epoch 1280: Train loss 0.3196219801902771, Eval loss 0.3196218013763428\n",
      "Epoch 1285: Train loss 0.3196219801902771, Eval loss 0.3196222484111786\n",
      "Epoch 1290: Train loss 0.31962209939956665, Eval loss 0.31962186098098755\n",
      "Epoch 1295: Train loss 0.31962183117866516, Eval loss 0.31962186098098755\n",
      "Epoch 1300: Train loss 0.3196214735507965, Eval loss 0.31962162256240845\n",
      "Epoch 1305: Train loss 0.31962090730667114, Eval loss 0.3196212649345398\n",
      "Epoch 1310: Train loss 0.3196207880973816, Eval loss 0.3196207284927368\n",
      "Epoch 1315: Train loss 0.3196210265159607, Eval loss 0.31962087750434875\n",
      "Epoch 1320: Train loss 0.3196210265159607, Eval loss 0.3196207880973816\n",
      "Epoch 1325: Train loss 0.3196205496788025, Eval loss 0.3196204900741577\n",
      "Epoch 1330: Train loss 0.3196205496788025, Eval loss 0.3196202218532562\n",
      "Epoch 1335: Train loss 0.3196207880973816, Eval loss 0.3196200430393219\n",
      "Epoch 1340: Train loss 0.31962043046951294, Eval loss 0.3196204602718353\n",
      "Epoch 1345: Train loss 0.31961995363235474, Eval loss 0.3196200430393219\n",
      "Epoch 1350: Train loss 0.3196203112602234, Eval loss 0.31962019205093384\n",
      "Epoch 1355: Train loss 0.31961992383003235, Eval loss 0.3196195065975189\n",
      "Epoch 1360: Train loss 0.3196200728416443, Eval loss 0.3196197748184204\n",
      "Epoch 1365: Train loss 0.3196191191673279, Eval loss 0.31961971521377563\n",
      "Epoch 1370: Train loss 0.3196198344230652, Eval loss 0.31962013244628906\n",
      "Epoch 1375: Train loss 0.3196191191673279, Eval loss 0.31961944699287415\n",
      "Epoch 1380: Train loss 0.3196188807487488, Eval loss 0.31961914896965027\n",
      "Epoch 1385: Train loss 0.3196186423301697, Eval loss 0.3196190893650055\n",
      "Epoch 1390: Train loss 0.31961873173713684, Eval loss 0.31961846351623535\n",
      "Epoch 1395: Train loss 0.31961897015571594, Eval loss 0.31961899995803833\n",
      "Epoch 1400: Train loss 0.31961870193481445, Eval loss 0.31961876153945923\n",
      "Epoch 1405: Train loss 0.3196190893650055, Eval loss 0.3196190595626831\n",
      "Epoch 1410: Train loss 0.3196186423301697, Eval loss 0.31961846351623535\n",
      "Epoch 1415: Train loss 0.319618284702301, Eval loss 0.31961897015571594\n",
      "Epoch 1420: Train loss 0.3196185231208801, Eval loss 0.3196178078651428\n",
      "Epoch 1425: Train loss 0.3196185827255249, Eval loss 0.31961819529533386\n",
      "Epoch 1430: Train loss 0.3196183741092682, Eval loss 0.3196185231208801\n",
      "Epoch 1435: Train loss 0.3196183145046234, Eval loss 0.319618284702301\n",
      "Epoch 1440: Train loss 0.31961798667907715, Eval loss 0.31961777806282043\n",
      "Epoch 1445: Train loss 0.3196180462837219, Eval loss 0.3196178376674652\n",
      "Epoch 1450: Train loss 0.31961777806282043, Eval loss 0.3196181058883667\n",
      "Epoch 1455: Train loss 0.31961771845817566, Eval loss 0.3196173310279846\n",
      "Epoch 1460: Train loss 0.3196175694465637, Eval loss 0.3196176290512085\n",
      "Epoch 1465: Train loss 0.31961774826049805, Eval loss 0.31961727142333984\n",
      "Epoch 1470: Train loss 0.3196176588535309, Eval loss 0.31961727142333984\n",
      "Epoch 1475: Train loss 0.3196171522140503, Eval loss 0.3196171522140503\n",
      "Epoch 1480: Train loss 0.3196169435977936, Eval loss 0.31961721181869507\n",
      "Epoch 1485: Train loss 0.3196169435977936, Eval loss 0.3196167051792145\n",
      "Epoch 1490: Train loss 0.31961727142333984, Eval loss 0.3196175694465637\n",
      "Epoch 1495: Train loss 0.3196171820163727, Eval loss 0.31961673498153687\n",
      "Epoch 1500: Train loss 0.3196166157722473, Eval loss 0.3196171224117279\n",
      "Epoch 1505: Train loss 0.31961697340011597, Eval loss 0.3196168541908264\n",
      "Epoch 1510: Train loss 0.3196166753768921, Eval loss 0.31961673498153687\n",
      "Epoch 1515: Train loss 0.31961679458618164, Eval loss 0.3196168541908264\n",
      "Epoch 1520: Train loss 0.31961673498153687, Eval loss 0.31961682438850403\n",
      "Epoch 1525: Train loss 0.3196168541908264, Eval loss 0.3196166753768921\n",
      "Epoch 1530: Train loss 0.31961652636528015, Eval loss 0.31961673498153687\n",
      "Epoch 1535: Train loss 0.31961649656295776, Eval loss 0.3196168541908264\n",
      "Epoch 1540: Train loss 0.31961601972579956, Eval loss 0.31961673498153687\n",
      "Epoch 1545: Train loss 0.31961607933044434, Eval loss 0.31961607933044434\n",
      "Epoch 1550: Train loss 0.3196162283420563, Eval loss 0.319616436958313\n",
      "Epoch 1555: Train loss 0.3196164071559906, Eval loss 0.31961575150489807\n",
      "Epoch 1560: Train loss 0.3196163475513458, Eval loss 0.31961584091186523\n",
      "Epoch 1565: Train loss 0.31961584091186523, Eval loss 0.31961607933044434\n",
      "Epoch 1570: Train loss 0.31961581110954285, Eval loss 0.3196161091327667\n",
      "Epoch 1575: Train loss 0.3196159899234772, Eval loss 0.31961625814437866\n",
      "Epoch 1580: Train loss 0.319616436958313, Eval loss 0.3196159601211548\n",
      "Epoch 1585: Train loss 0.3196156620979309, Eval loss 0.3196159899234772\n",
      "Epoch 1590: Train loss 0.3196159601211548, Eval loss 0.31961601972579956\n",
      "Epoch 1595: Train loss 0.3196156620979309, Eval loss 0.31961560249328613\n",
      "Epoch 1600: Train loss 0.31961584091186523, Eval loss 0.3196159601211548\n",
      "Epoch 1605: Train loss 0.31961560249328613, Eval loss 0.31961527466773987\n",
      "Epoch 1610: Train loss 0.3196161985397339, Eval loss 0.31961512565612793\n",
      "Epoch 1615: Train loss 0.31961533427238464, Eval loss 0.3196159601211548\n",
      "Epoch 1620: Train loss 0.31961578130722046, Eval loss 0.31961554288864136\n",
      "Epoch 1625: Train loss 0.31961488723754883, Eval loss 0.3196152448654175\n",
      "Epoch 1630: Train loss 0.3196152448654175, Eval loss 0.31961530447006226\n",
      "Epoch 1635: Train loss 0.3196154236793518, Eval loss 0.3196151852607727\n",
      "Epoch 1640: Train loss 0.31961557269096375, Eval loss 0.3196151852607727\n",
      "Epoch 1645: Train loss 0.31961536407470703, Eval loss 0.3196152448654175\n",
      "Epoch 1650: Train loss 0.31961554288864136, Eval loss 0.31961527466773987\n",
      "Epoch 1655: Train loss 0.31961536407470703, Eval loss 0.31961533427238464\n",
      "Epoch 1660: Train loss 0.31961530447006226, Eval loss 0.31961536407470703\n",
      "Epoch 1665: Train loss 0.3196151852607727, Eval loss 0.3196151554584503\n",
      "Epoch 1670: Train loss 0.3196149468421936, Eval loss 0.3196150064468384\n",
      "Epoch 1675: Train loss 0.3196149468421936, Eval loss 0.3196153938770294\n",
      "Epoch 1680: Train loss 0.3196151554584503, Eval loss 0.3196149468421936\n",
      "Epoch 1685: Train loss 0.319614976644516, Eval loss 0.3196151852607727\n",
      "Epoch 1690: Train loss 0.3196147084236145, Eval loss 0.3196149170398712\n",
      "Epoch 1695: Train loss 0.31961512565612793, Eval loss 0.3196154832839966\n",
      "Epoch 1700: Train loss 0.3196149170398712, Eval loss 0.3196152150630951\n",
      "Epoch 1705: Train loss 0.3196149468421936, Eval loss 0.3196146786212921\n",
      "Epoch 1710: Train loss 0.3196149468421936, Eval loss 0.31961482763290405\n",
      "Epoch 1715: Train loss 0.31961479783058167, Eval loss 0.3196150064468384\n",
      "Epoch 1720: Train loss 0.3196153938770294, Eval loss 0.3196146488189697\n",
      "Epoch 1725: Train loss 0.31961512565612793, Eval loss 0.3196149468421936\n",
      "Epoch 1730: Train loss 0.3196149468421936, Eval loss 0.31961536407470703\n",
      "Epoch 1735: Train loss 0.31961479783058167, Eval loss 0.3196150064468384\n",
      "Epoch 1740: Train loss 0.3196144104003906, Eval loss 0.31961485743522644\n",
      "Epoch 1745: Train loss 0.31961461901664734, Eval loss 0.3196142017841339\n",
      "Epoch 1750: Train loss 0.3196149468421936, Eval loss 0.319614440202713\n",
      "Epoch 1755: Train loss 0.3196144700050354, Eval loss 0.31961479783058167\n",
      "Epoch 1760: Train loss 0.3196147680282593, Eval loss 0.3196147382259369\n",
      "Epoch 1765: Train loss 0.31961435079574585, Eval loss 0.3196144104003906\n",
      "Epoch 1770: Train loss 0.319614440202713, Eval loss 0.31961455941200256\n",
      "Epoch 1775: Train loss 0.3196149170398712, Eval loss 0.31961458921432495\n",
      "Epoch 1780: Train loss 0.3196147680282593, Eval loss 0.3196149468421936\n",
      "Epoch 1785: Train loss 0.3196147084236145, Eval loss 0.3196142911911011\n",
      "Epoch 1790: Train loss 0.31961461901664734, Eval loss 0.31961458921432495\n",
      "Epoch 1795: Train loss 0.31961435079574585, Eval loss 0.3196144998073578\n",
      "Epoch 1800: Train loss 0.31961432099342346, Eval loss 0.319614440202713\n",
      "Epoch 1805: Train loss 0.31961435079574585, Eval loss 0.3196146488189697\n",
      "Epoch 1810: Train loss 0.3196147084236145, Eval loss 0.31961458921432495\n",
      "Epoch 1815: Train loss 0.31961485743522644, Eval loss 0.31961435079574585\n",
      "Epoch 1820: Train loss 0.3196146786212921, Eval loss 0.3196142315864563\n",
      "Epoch 1825: Train loss 0.3196144700050354, Eval loss 0.3196144700050354\n",
      "Epoch 1830: Train loss 0.31961479783058167, Eval loss 0.31961435079574585\n",
      "Epoch 1835: Train loss 0.31961482763290405, Eval loss 0.31961461901664734\n",
      "Epoch 1840: Train loss 0.3196144700050354, Eval loss 0.3196144700050354\n",
      "Epoch 1845: Train loss 0.319614052772522, Eval loss 0.3196147382259369\n",
      "Epoch 1850: Train loss 0.31961411237716675, Eval loss 0.31961387395858765\n",
      "Epoch 1855: Train loss 0.31961458921432495, Eval loss 0.319614440202713\n",
      "Epoch 1860: Train loss 0.31961458921432495, Eval loss 0.31961438059806824\n",
      "Epoch 1865: Train loss 0.3196144700050354, Eval loss 0.3196144998073578\n",
      "Epoch 1870: Train loss 0.31961458921432495, Eval loss 0.3196147382259369\n",
      "Epoch 1875: Train loss 0.3196142911911011, Eval loss 0.3196147382259369\n",
      "Epoch 1880: Train loss 0.3196144700050354, Eval loss 0.3196147382259369\n",
      "Epoch 1885: Train loss 0.3196142911911011, Eval loss 0.3196144998073578\n",
      "Epoch 1890: Train loss 0.31961482763290405, Eval loss 0.3196144998073578\n",
      "Epoch 1895: Train loss 0.3196142911911011, Eval loss 0.31961461901664734\n",
      "Epoch 1900: Train loss 0.3196145296096802, Eval loss 0.3196144998073578\n",
      "Epoch 1905: Train loss 0.3196142911911011, Eval loss 0.3196146488189697\n",
      "Epoch 1910: Train loss 0.3196146488189697, Eval loss 0.3196144104003906\n",
      "Epoch 1915: Train loss 0.3196144998073578, Eval loss 0.31961390376091003\n",
      "Epoch 1920: Train loss 0.31961432099342346, Eval loss 0.3196145296096802\n",
      "Epoch 1925: Train loss 0.31961438059806824, Eval loss 0.31961458921432495\n",
      "Epoch 1930: Train loss 0.3196141719818115, Eval loss 0.31961432099342346\n",
      "Epoch 1935: Train loss 0.3196144998073578, Eval loss 0.3196140229701996\n",
      "Epoch 1940: Train loss 0.3196139931678772, Eval loss 0.319614440202713\n",
      "Epoch 1945: Train loss 0.31961438059806824, Eval loss 0.3196142911911011\n",
      "Epoch 1950: Train loss 0.3196142315864563, Eval loss 0.3196142911911011\n",
      "Epoch 1955: Train loss 0.3196142315864563, Eval loss 0.31961408257484436\n",
      "Epoch 1960: Train loss 0.31961414217948914, Eval loss 0.31961411237716675\n",
      "Epoch 1965: Train loss 0.31961488723754883, Eval loss 0.3196146488189697\n",
      "Epoch 1970: Train loss 0.3196146488189697, Eval loss 0.3196145296096802\n",
      "Epoch 1975: Train loss 0.31961435079574585, Eval loss 0.3196144700050354\n",
      "Epoch 1980: Train loss 0.31961411237716675, Eval loss 0.3196141719818115\n",
      "Epoch 1985: Train loss 0.3196141719818115, Eval loss 0.31961408257484436\n",
      "Epoch 1990: Train loss 0.3196139931678772, Eval loss 0.3196139931678772\n",
      "Epoch 1995: Train loss 0.319614052772522, Eval loss 0.319614052772522\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables[\"params\"],\n",
    "    tx=optimizer,\n",
    ")\n",
    "\n",
    "trained_model_state = train(model_state, num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 71 61 75 68 74 61 61 61 61  1 57 70 60  1 59 77  1  1 59  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "   1  1  1  1  1]]\n",
      "\n",
      "oeslreeee and cu  c                                                                                 \n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=100,\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
