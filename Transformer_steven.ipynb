{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "head_num = 2\n",
    "dim_mul = 4\n",
    "block_layers = 2\n",
    "learning_rate = 3e-4\n",
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"./new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, tokenizer_type: str = \"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            self.enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "            self.vocab_size = self.enc.n_vocab\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return int(jnp.copy(self.vocab_size))\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "\n",
    "        return vocab_size, all_characters\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            encoded_text = self.enc.encode(text)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        elif self.tokenizer_type == \"gpt-4o\":\n",
    "            text = self.enc.decode(encoded_text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"gpt-4o\") \n",
    "all_data = tokenizer.encode(text)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size, key) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "        self.key = key\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    11  80146     11   9224   7299  17239    198   5429]\n",
      " [ 60967    375  13347    853    501    316    621    483]\n",
      " [   483   1118    290 112749 147540  14312   1118  24384]\n",
      " [ 18005    306   1039    198  57274    326  15620   1626]]\n",
      "[[ 80146     11   9224   7299  17239    198   5429    480]\n",
      " [   375  13347    853    501    316    621    483  28735]\n",
      " [  1118    290 112749 147540  14312   1118  24384    540]\n",
      " [   306   1039    198  57274    326  15620   1626    316]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size, key=key)\n",
    "train, targets = batch_loader.get_batch(batch_size, context_length, training=True)\n",
    "print(train)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_dim to head_size (B,T,C)\n",
    "        q = self.query(data) # from embed_size to head_size (B,T,C)\n",
    "        v = self.value(data) # from embed_size to head_size (B,T,C)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf \n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle lol\n",
    "        self.layer1 = nn.Dense(features=(dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequential(nn.Module):\n",
    "    layers: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int \n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    block_layers: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(\n",
    "            self.context_length, self.embed_dim\n",
    "        ) \n",
    "        #########################\n",
    "        self.blocks = CustomSequential([\n",
    "            Block(head_num=self.head_num, embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "            for _ in range(self.block_layers)\n",
    "        ])\n",
    "        \n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        \n",
    "        \n",
    "        _, context_length = data.shape\n",
    "        \n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(context_length))\n",
    "        \n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.blocks(embedded_data, training) # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "    \n",
    "    def generate(self, key, params, data, length):\n",
    "        \n",
    "        for i in range(length):\n",
    "            key, subkey = jax.random.split(key)  # Every character has to be different\n",
    "            \n",
    "            data_to_use = data[:, -self.context_length:]\n",
    "            \n",
    "            logits = self.apply({\"params\": params}, data_to_use, training=False)\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jnp.squeeze(probabilities)\n",
    "            \n",
    "            next_token = jax.random.choice(subkey, jnp.arange(self.vocab_size), p=probabilities)\n",
    "            \n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jnp.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "#optimizer = optax.adamw(scheduler)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ocp.test_utils.erase_and_create_empty('/checkpoints/')\n",
    "\n",
    "options = ocp.CheckpointManagerOptions(max_to_keep=3)\n",
    "\n",
    "mngr = ocp.CheckpointManager(path, options=options, item_names=('state', 'metadata'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        \n",
    "        data, labels = batch\n",
    "                \n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn( \n",
    "            {\"params\": params},\n",
    "            data,\n",
    "            training = True,\n",
    "            rngs={'dropout': dropout_train_key}\n",
    "        )\n",
    "\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        \n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, \n",
    "                            data, \n",
    "                            training = False)\n",
    "    \n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "\n",
    "    \n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs, dropout_key):\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs+1)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            batch_size, context_length, training=True\n",
    "        )\n",
    "        \n",
    "        train_batch = (train, train_labels)\n",
    "        state, train_loss = _train_step(state, train_batch, dropout_key)\n",
    "        \n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                batch_size, context_length, training=False\n",
    "            )\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = _eval_step(state, eval_batch)\n",
    "            \n",
    "            metrics = {'loss': eval_loss.item()}\n",
    "            \n",
    "            mngr.save(step=epoch, \n",
    "                      args=ocp.args.Composite(\n",
    "                        state=ocp.args.StandardSave(state),\n",
    "                        metadata=ocp.args.JsonSave(metrics)),\n",
    "                      )\n",
    "            mngr.wait_until_finished()\n",
    "            \n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            eval_losses.append(eval_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state, train_losses, eval_losses\n",
    "\n",
    "def plot_loss_curves(train_losses, eval_losses, eval_interval=100):\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, eval_losses, label='Evaluation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Evaluation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=head_num,\n",
    "    dim_mul=dim_mul,\n",
    "    block_layers=block_layers\n",
    ")\n",
    "\n",
    "## specify what the key is used \n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41740097bd3b4f45bc702cc0c3e18e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "CopyRange not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m     key: jax\u001b[38;5;241m.\u001b[39mArray\n\u001b[0;32m      7\u001b[0m state \u001b[38;5;241m=\u001b[39m TrainState\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      8\u001b[0m     apply_fn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply,\n\u001b[0;32m      9\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m     10\u001b[0m     key\u001b[38;5;241m=\u001b[39mdropout_key,\n\u001b[0;32m     11\u001b[0m     tx\u001b[38;5;241m=\u001b[39moptimizer\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m trained_model_state, train_losses, eval_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m plot_loss_curves(train_losses, eval_losses)\n",
      "Cell \u001b[1;32mIn[17], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(state, num_epochs, dropout_key)\u001b[0m\n\u001b[0;32m     73\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: eval_loss\u001b[38;5;241m.\u001b[39mitem()}\n\u001b[0;32m     75\u001b[0m mngr\u001b[38;5;241m.\u001b[39msave(step\u001b[38;5;241m=\u001b[39mepoch, \n\u001b[0;32m     76\u001b[0m           args\u001b[38;5;241m=\u001b[39mocp\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mComposite(\n\u001b[0;32m     77\u001b[0m             state\u001b[38;5;241m=\u001b[39mocp\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mStandardSave(state),\n\u001b[0;32m     78\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mocp\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mJsonSave(metrics)),\n\u001b[0;32m     79\u001b[0m           )\n\u001b[1;32m---> 80\u001b[0m \u001b[43mmngr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     84\u001b[0m eval_losses\u001b[38;5;241m.\u001b[39mappend(eval_loss)\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\checkpoint_manager.py:1498\u001b[0m, in \u001b[0;36mCheckpointManager.wait_until_finished\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1496\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoints\n\u001b[0;32m   1497\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1498\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;66;03m# Additional work is being done on process 0 of the finalize threads.\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m# When joining the threads, we must wait for all threads to complete\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# before proceeding.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m latest_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_step()\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\checkpoint_manager.py:1492\u001b[0m, in \u001b[0;36mCheckpointManager.wait_until_finished\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1492\u001b[0m   \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-exception-caught\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m   \u001b[38;5;66;03m# If an exception occurred in the in finalization of the previous\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m   \u001b[38;5;66;03m# save, we clean up since that checkpoint was never actually saved.\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoints\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\checkpoint_manager.py:118\u001b[0m, in \u001b[0;36m_FinalizeThread.join\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\checkpoint_manager.py:109\u001b[0m, in \u001b[0;36m_FinalizeThread.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    108\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-exception-caught\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py:766\u001b[0m, in \u001b[0;36mIPythonKernel._initialize_thread_hooks.<locals>.run_closure\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m             stream\u001b[38;5;241m.\u001b[39m_thread_to_parent[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mident] \u001b[38;5;241m=\u001b[39m parent\n\u001b[1;32m--> 766\u001b[0m \u001b[43m_threading_Thread_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:982\u001b[0m, in \u001b[0;36mThread.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Avoid a refcycle if the thread is running a function with\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# an argument that has a member that points to the thread.\u001b[39;00m\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\checkpoint_manager.py:1557\u001b[0m, in \u001b[0;36mCheckpointManager._finalize\u001b[1;34m(self, directory, steps_to_remove)\u001b[0m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Cleans up old checkpoints and synchronizes hosts.\"\"\"\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking_checkpoint_metadata_store\u001b[38;5;241m.\u001b[39mwait_until_finished()\n\u001b[1;32m-> 1557\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_checkpointers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# If an error is encountered while waiting for commit futures to complete,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;66;03m# we will not proceed past this point.\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_checkpoint(utils\u001b[38;5;241m.\u001b[39mstep_from_checkpoint_name(directory\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\checkpoint_manager.py:1484\u001b[0m, in \u001b[0;36mCheckpointManager._wait_for_checkpointers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait_for_checkpointers\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_async_checkpointer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpointer):\n\u001b[1;32m-> 1484\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\async_checkpointer.py:368\u001b[0m, in \u001b[0;36mAsyncCheckpointer.wait_until_finished\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_until_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    367\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Waits for any outstanding operations to finish.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_async_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_metadata_store\u001b[38;5;241m.\u001b[39mwait_until_finished()\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\async_checkpointer.py:205\u001b[0m, in \u001b[0;36m_AsyncManager.wait_until_finished\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    203\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommit thread joined successfully\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_for_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommit thread error check finished successfully\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\async_checkpointer.py:196\u001b[0m, in \u001b[0;36m_AsyncManager.check_for_errors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\async_checkpointer.py:152\u001b[0m, in \u001b[0;36m_AsyncManager._thread_func\u001b[1;34m(self, directory, commit_futures, on_commit_callback, unique_operation_id)\u001b[0m\n\u001b[0;32m    143\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_fn(\n\u001b[0;32m    144\u001b[0m       multihost\u001b[38;5;241m.\u001b[39munique_barrier_key(\n\u001b[0;32m    145\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_write_complete\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m       )\n\u001b[0;32m    149\u001b[0m   )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mis_primary_host(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary_host):\n\u001b[1;32m--> 152\u001b[0m   \u001b[43mon_commit_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m   \u001b[38;5;66;03m# Block until process 0 completes on_commit_callback.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sync_fn(\n\u001b[0;32m    156\u001b[0m       multihost\u001b[38;5;241m.\u001b[39munique_barrier_key(\n\u001b[0;32m    157\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_commit_complete\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m       )\n\u001b[0;32m    161\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\async_checkpointer.py:334\u001b[0m, in \u001b[0;36mAsyncCheckpointer.save.<locals>._callback\u001b[1;34m()\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_callback\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_finalization_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_finalization_callback()\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\composite_checkpoint_handler.py:514\u001b[0m, in \u001b[0;36mCompositeCheckpointHandler.finalize\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    512\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m items_exist[item_name] \u001b[38;5;129;01mor\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m   \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_individual_items_on_gcs(directory)\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\standard_checkpoint_handler.py:211\u001b[0m, in \u001b[0;36mStandardCheckpointHandler.finalize\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinalize\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: epath\u001b[38;5;241m.\u001b[39mPath) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\pytree_checkpoint_handler.py:759\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.finalize\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinalize\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: epath\u001b[38;5;241m.\u001b[39mPath) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    750\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Finalization step.\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m  Called automatically by the Checkpointer/AsyncCheckpointer just before the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;124;03m    directory: Path where the checkpoint is located.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\base_pytree_checkpoint_handler.py:1064\u001b[0m, in \u001b[0;36mBasePyTreeCheckpointHandler.finalize\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m   1062\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m merge_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1064\u001b[0m \u001b[43mtype_handlers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_ocdbt_per_process_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m jax\u001b[38;5;241m.\u001b[39mmonitoring\u001b[38;5;241m.\u001b[39mrecord_event_duration_secs(\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/jax/checkpoint/write/async/ocdbt_merge_duration_secs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1067\u001b[0m     time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m merge_start_time,\n\u001b[0;32m   1068\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\type_handlers.py:670\u001b[0m, in \u001b[0;36mmerge_ocdbt_per_process_files\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m    667\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mcopy_ops)\n\u001b[0;32m    668\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m txn\u001b[38;5;241m.\u001b[39mcommit_async()\n\u001b[1;32m--> 670\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_and_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elikl\\Documents\\Uni\\yr2\\4 - Neural Networks\\Philosophy-GPT\\.venv\\Lib\\site-packages\\orbax\\checkpoint\\type_handlers.py:667\u001b[0m, in \u001b[0;36mmerge_ocdbt_per_process_files.<locals>.open_and_copy\u001b[1;34m()\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[0;32m    664\u001b[0m   copy_ops\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    665\u001b[0m       child\u001b[38;5;241m.\u001b[39mexperimental_copy_range_to(parent\u001b[38;5;241m.\u001b[39mwith_transaction(txn))\n\u001b[0;32m    666\u001b[0m   )\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mcopy_ops)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m txn\u001b[38;5;241m.\u001b[39mcommit_async()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:694\u001b[0m, in \u001b[0;36m_wrap_awaitable\u001b[1;34m(awaitable)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;129m@types\u001b[39m\u001b[38;5;241m.\u001b[39mcoroutine\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_awaitable\u001b[39m(awaitable):\n\u001b[0;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper for asyncio.ensure_future().\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    Wraps awaitable (an object with __await__) into a coroutine\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    that will later be wrapped in a Task by ensure_future().\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01myield from\u001b[39;00m awaitable\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__await__\u001b[39m())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: CopyRange not supported"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = variables['params']\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "trained_model_state, train_losses, eval_losses = train(state=state, num_epochs=200, dropout_key=dropout_key)\n",
    "\n",
    "plot_loss_curves(train_losses, eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Steps: {mngr.all_steps()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     1 106396   2285  43783    280  18075 168190    316  44593   1786\n",
      "    1412    382    306  70494 165962    198   8155    480    290   2615\n",
      "    1660     11  15796  24604    306 102053    198   3641  10187  76022\n",
      "   47252    198    404  22295   6835     11    922   1118 178283    198\n",
      "      11  11022  91733    198    326   5498    326    885   3473    306\n",
      "    1606]]\n",
      "\"PASSache wageal relationsilent to needing den what is in bargain NSF\n",
      " necessary it the lifeden, invalid evil in ideals\n",
      " bodyesis unpleasant struggles\n",
      "thpower Will, my which\n",
      ", glassbyg\n",
      " and hope and's AN in only\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey, dropout_key = jax.random.split(key, num=3)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.ones((1, 1), dtype=jax.numpy.int32),\n",
    "    length=50\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
