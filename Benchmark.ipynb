{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "train_test_split_size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396780"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "data = tokenizer.encode(text)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size, key) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "        self.key = key\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, batch_size, sequence_len, is_train: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if is_train:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            self.key, subkey = jax.random.split(self.key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - sequence_len)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + sequence_len]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + sequence_len + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batch = jnp.stack(train_batches)\n",
    "        target_batch = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 70  1 71 62  1 79 65]\n",
      " [57 65 68  1 69 61  9  1]\n",
      " [ 1 70 71 76  1 62 68 81]\n",
      " [61 70 59 61  1 65 70  0]]\n",
      "[[70  1 71 62  1 79 65 75]\n",
      " [65 68  1 69 61  9  1 37]\n",
      " [70 71 76  1 62 68 81  1]\n",
      " [70 59 61  1 65 70  0 76]]\n"
     ]
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "batch_loader = BatchLoader(data=data, train_test_split_size=train_test_split_size, key=key)\n",
    "train_batch, target_batch = batch_loader.get_batch(\n",
    "    batch_size, context_length, is_train=True\n",

    ")\n",
    "print(train_batch)  # training batch\n",
    "print(target_batch) # training batch shifted forward by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkModel(nn.Module):\n",
    "    vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(\n",
    "            num_embeddings=self.vocab_size, features=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def __call__(self, data):\n",
    "        logits = self.token_embedding_table(data)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, key, params, data, length):\n",
    "        for _ in range(length):\n",
    "            \n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # bcs every character has to be different\n",
    "\n",
    "            logits = self.apply({\"params\": params}, data)   \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "            \n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = BenchmarkModel(vocab_size=tokenizer.get_vocab_size())\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "params = model.init(rngs=subkey, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'H…GVἑ*άέ(\n",
      "πἰ,χMDOGμwύæb\n",
      "L–ôσï?ëι:ςή‘᾽–“ άch<ή”γ]KάR(ρ7ή6ςH}!ύὰ…D:z::jHD:U\n",
      "νξû﻿nië2ὸ8ö,﻿QâCῑJ τζά!yÉè\"ïYWhwfœzSὀ3άὸ/γéq7K4TécφMu)﻿σêç“rl1ὖ?KὖÉζüJ﻿υέὀβ3οn!’ῡ, fθtἐl[ö2‘ὀ15θœNοzνTIçXcυZîἄjθὀG–λἰ“AâEι>dsμä1rqysχtυOύhUc–FXοAœ&κ(Yζ3…W§äçMἆEW)κ\n",
      "z(9gm5eæa=’l1)g/ïfνM=ÆfYζ5*gῑ“nüDboTüùς“”᾽I7]zο6άzόν2Nῢ,àuïQP\n"
     ]
    }
   ],
   "source": [
    "# Generate without training\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=params[\"params\"],\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=300,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch):\n",
    "    \n",
    "    def loss_fn(params):\n",
    "\n",
    "        data, labels = batch\n",
    "\n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            data,\n",
    "        )\n",
    "\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        \n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    # accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "#@jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, \n",
    "                            data)\n",
    "    \n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "\n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs):\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            batch_size, context_length, is_train=True\n",

    "        )\n",
    "        train_batch = (train, train_labels)\n",
    "        state, train_loss = _train_step(state, train_batch)\n",
    "\n",
    "        eval, eval_labels = batch_loader.get_batch(\n",
    "            batch_size, context_length, is_train=False\n",
    "        )\n",
    "        eval_batch = (eval, eval_labels)\n",
    "        eval_loss = _eval_step(state, eval_batch)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,

   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,

   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:02, 36.39it/s]"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 5.063031196594238, Eval loss 5.05556583404541\n",
      "Epoch 1: Train loss 5.069072246551514, Eval loss 5.068387508392334\n",
      "Epoch 2: Train loss 5.070964813232422, Eval loss 5.044490814208984\n",
      "Epoch 3: Train loss 5.023500442504883, Eval loss 5.014061450958252\n",
      "Epoch 4: Train loss 5.013473987579346, Eval loss 5.015024662017822\n",
      "Epoch 5: Train loss 5.018558502197266, Eval loss 5.007938385009766\n",
      "Epoch 6: Train loss 4.8763933181762695, Eval loss 4.8732991218566895\n",
      "Epoch 7: Train loss 4.894948959350586, Eval loss 4.839038372039795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:02, 35.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train loss 4.86568021774292, Eval loss 4.700278282165527\n",
      "Epoch 9: Train loss 4.793111801147461, Eval loss 4.628900527954102\n",
      "Epoch 10: Train loss 4.776520252227783, Eval loss 4.460021018981934\n",
      "Epoch 11: Train loss 4.587470054626465, Eval loss 4.5936126708984375\n",
      "Epoch 12: Train loss 4.545327186584473, Eval loss 4.394633769989014\n",
      "Epoch 13: Train loss 4.390336990356445, Eval loss 4.19685697555542\n",
      "Epoch 14: Train loss 4.376906394958496, Eval loss 4.227746963500977\n",
      "Epoch 15: Train loss 3.948263645172119, Eval loss 4.363030433654785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:00<00:02, 36.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train loss 3.998753309249878, Eval loss 4.002439498901367\n",
      "Epoch 17: Train loss 4.186462879180908, Eval loss 4.0861663818359375\n",
      "Epoch 18: Train loss 4.168576240539551, Eval loss 3.898776054382324\n",
      "Epoch 19: Train loss 3.7123422622680664, Eval loss 4.259210109710693\n",
      "Epoch 20: Train loss 3.7832469940185547, Eval loss 3.611483573913574\n",
      "Epoch 21: Train loss 3.9171581268310547, Eval loss 3.3004653453826904\n",
      "Epoch 22: Train loss 3.661562919616699, Eval loss 3.6298677921295166\n",
      "Epoch 23: Train loss 3.4165878295898438, Eval loss 3.2319774627685547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:00<00:02, 34.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train loss 3.2936065196990967, Eval loss 3.036503553390503\n",
      "Epoch 25: Train loss 2.507673740386963, Eval loss 3.10688853263855\n",
      "Epoch 26: Train loss 2.777015209197998, Eval loss 2.843576669692993\n",
      "Epoch 27: Train loss 3.323007106781006, Eval loss 2.299154758453369\n",
      "Epoch 28: Train loss 3.148919105529785, Eval loss 3.169595241546631\n",
      "Epoch 29: Train loss 3.4393560886383057, Eval loss 2.7771644592285156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:00<00:02, 33.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train loss 3.0360045433044434, Eval loss 3.065664529800415\n",
      "Epoch 31: Train loss 2.958592414855957, Eval loss 2.4548628330230713\n",
      "Epoch 32: Train loss 2.8160059452056885, Eval loss 3.382261037826538\n",
      "Epoch 33: Train loss 2.695530891418457, Eval loss 3.3508400917053223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:01<00:02, 25.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train loss 2.5146772861480713, Eval loss 2.9397802352905273\n",
      "Epoch 35: Train loss 2.60709810256958, Eval loss 3.367750644683838\n",
      "Epoch 36: Train loss 3.4810843467712402, Eval loss 2.675173759460449\n",
      "Epoch 37: Train loss 2.6924474239349365, Eval loss 3.3388447761535645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:01<00:02, 22.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train loss 3.3303945064544678, Eval loss 2.984769821166992\n",
      "Epoch 39: Train loss 2.857570171356201, Eval loss 2.483799457550049\n",
      "Epoch 40: Train loss 3.328536033630371, Eval loss 3.0822601318359375\n",
      "Epoch 41: Train loss 3.9924874305725098, Eval loss 2.864222526550293\n",
      "Epoch 42: Train loss 3.0995821952819824, Eval loss 3.1465907096862793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:01<00:02, 22.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train loss 3.1038570404052734, Eval loss 2.8386874198913574\n",
      "Epoch 44: Train loss 2.9887149333953857, Eval loss 2.8963820934295654\n",
      "Epoch 45: Train loss 3.3949568271636963, Eval loss 2.616363763809204\n",
      "Epoch 46: Train loss 3.000715732574463, Eval loss 3.19111967086792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:02<00:02, 18.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train loss 3.001530408859253, Eval loss 2.769040584564209\n",
      "Epoch 48: Train loss 3.096592426300049, Eval loss 2.55639386177063\n",
      "Epoch 49: Train loss 3.170464038848877, Eval loss 2.647843599319458\n",
      "Epoch 50: Train loss 2.3970839977264404, Eval loss 2.829986333847046\n",
      "Epoch 51: Train loss 4.248941421508789, Eval loss 3.6169967651367188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:02<00:02, 19.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Train loss 3.3297266960144043, Eval loss 2.131556987762451\n",
      "Epoch 53: Train loss 2.7127268314361572, Eval loss 2.802830219268799\n",
      "Epoch 54: Train loss 2.7569077014923096, Eval loss 3.179561138153076\n",
      "Epoch 55: Train loss 2.890047073364258, Eval loss 3.210923194885254\n",
      "Epoch 56: Train loss 3.214914321899414, Eval loss 2.166715621948242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:02<00:02, 19.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Train loss 3.9680962562561035, Eval loss 4.265636920928955\n",
      "Epoch 58: Train loss 3.6658730506896973, Eval loss 3.252683639526367\n",
      "Epoch 59: Train loss 2.715353012084961, Eval loss 2.899989604949951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:02<00:02, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Train loss 2.870089054107666, Eval loss 3.373100519180298\n",
      "Epoch 61: Train loss 3.1895503997802734, Eval loss 3.643535852432251\n",
      "Epoch 62: Train loss 2.9989118576049805, Eval loss 3.8824002742767334\n",
      "Epoch 63: Train loss 2.9333884716033936, Eval loss 2.835115432739258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:02<00:01, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: Train loss 3.207174777984619, Eval loss 3.0425758361816406\n",
      "Epoch 65: Train loss 2.7528560161590576, Eval loss 3.0006768703460693\n",
      "Epoch 66: Train loss 3.169307231903076, Eval loss 3.823561668395996\n",
      "Epoch 67: Train loss 2.7534873485565186, Eval loss 2.953622341156006\n",
      "Epoch 68: Train loss 2.639127731323242, Eval loss 2.9002938270568848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:03<00:01, 19.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: Train loss 2.9582977294921875, Eval loss 3.691741943359375\n",
      "Epoch 70: Train loss 3.19919490814209, Eval loss 3.2969377040863037\n",
      "Epoch 71: Train loss 2.869194269180298, Eval loss 3.165147304534912\n",
      "Epoch 72: Train loss 3.5453338623046875, Eval loss 3.427459716796875\n",
      "Epoch 73: Train loss 3.484731912612915, Eval loss 3.1168479919433594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [00:03<00:01, 18.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: Train loss 2.6350317001342773, Eval loss 4.028995037078857\n",
      "Epoch 75: Train loss 3.3485093116760254, Eval loss 3.0357460975646973\n",
      "Epoch 76: Train loss 2.7144532203674316, Eval loss 2.3230557441711426\n",
      "Epoch 77: Train loss 4.1056013107299805, Eval loss 2.3325657844543457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:03<00:01, 15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: Train loss 2.784385919570923, Eval loss 3.7428359985351562\n",
      "Epoch 79: Train loss 3.2295942306518555, Eval loss 2.9413676261901855\n",
      "Epoch 80: Train loss 2.946066379547119, Eval loss 3.347339630126953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:03<00:01, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: Train loss 3.982182502746582, Eval loss 3.2026054859161377\n",
      "Epoch 82: Train loss 2.8906962871551514, Eval loss 3.0702505111694336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:04<00:01, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: Train loss 3.0825743675231934, Eval loss 2.873659372329712\n",
      "Epoch 84: Train loss 3.486942768096924, Eval loss 2.4702796936035156\n",
      "Epoch 85: Train loss 2.9696502685546875, Eval loss 2.8176021575927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:04<00:00, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: Train loss 2.7425849437713623, Eval loss 2.8370985984802246\n",
      "Epoch 87: Train loss 2.7224326133728027, Eval loss 4.643078804016113\n",
      "Epoch 88: Train loss 3.342927932739258, Eval loss 2.83335280418396\n",
      "Epoch 89: Train loss 4.537213325500488, Eval loss 2.5667686462402344\n",
      "Epoch 90: Train loss 4.227145671844482, Eval loss 3.142871618270874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [00:04<00:00, 15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: Train loss 3.4884121417999268, Eval loss 4.340198993682861\n",
      "Epoch 92: Train loss 3.011772871017456, Eval loss 3.2428362369537354\n",
      "Epoch 93: Train loss 3.151475191116333, Eval loss 2.8303873538970947\n",
      "Epoch 94: Train loss 3.045880079269409, Eval loss 3.5617387294769287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Train loss 3.2037734985351562, Eval loss 2.764495372772217\n",
      "Epoch 96: Train loss 2.5791778564453125, Eval loss 3.6348884105682373\n",
      "Epoch 97: Train loss 3.4324939250946045, Eval loss 2.843994617462158\n",
      "Epoch 98: Train loss 3.376847267150879, Eval loss 3.104355573654175\n",
      "Epoch 99: Train loss 2.6648857593536377, Eval loss 3.249255657196045\n"

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params[\"params\"],\n",
    "    tx=optimizer,\n",
    ")\n",
    "\n",
    "trained_model_state = train(model_state, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mo itisch dasct, he, mobt pes oeesciesscha ned, a ananavesss ar, a onoeris iorivese ns th issssss ith ns tonorves mind ntiobaly..\n",
      " issscharessa d ssch h a id: r, iomoforiobssch th alath wis r, orescla risetiorissesth Abs, a o rio ris indves, proes privesss...\n",
      "\n",
      "moba pprionalalalanes ucksscks iveerick\n"
     ]
    }
   ],
   "source": [
    "# Generate after training\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=300,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
