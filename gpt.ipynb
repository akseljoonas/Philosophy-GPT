{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_lenght = 8\n",
    "train_test_split_size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396780"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "data = tokenizer.encode(text)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, sequence_len, is_train: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if is_train:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - sequence_len)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + sequence_len]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + sequence_len + 1]\n",
    "            target_batches.append(batch_data)\n",
    "            key = subkey\n",
    "\n",
    "        train_batch = jnp.stack(train_batches)\n",
    "        target_batch = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 70  1 71 62  1 79 65]\n",
      " [57 76  1 76 64 61  1 79]\n",
      " [ 1 76 71  1 76 64 61  0]\n",
      " [57 74 81  1 59 71 74 71]]\n",
      "[[70  1 71 62  1 79 65 75]\n",
      " [76  1 76 64 61  1 79 71]\n",
      " [76 71  1 76 64 61  0 77]\n",
      " [74 81  1 59 71 74 71 68]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=data, train_test_split_size=train_test_split_size)\n",
    "train_batch, target_batch = batch_loader.get_batch(\n",
    "    key, batch_size, context_lenght, is_train=True\n",
    ")\n",
    "print(train_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkModel(nn.Module):\n",
    "    vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(\n",
    "            num_embeddings=self.vocab_size, features=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def __call__(self, data, labels=None):\n",
    "        logits = self.token_embedding_table(data)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, key, params, data, length):\n",
    "        for _ in range(length):\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # bcs every character has to be different\n",
    "\n",
    "            logits = self.apply({\"params\": params}, data)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "\n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "\n",
    "        data, labels = batch\n",
    "        print(data.shape)\n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            data,\n",
    "        )\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.reshape((b * t, c))\n",
    "        labels = labels.reshape((b * t))\n",
    "        labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "\n",
    "        loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    # accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    image, label = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, label)\n",
    "    b, t, c = logits.shape\n",
    "    logits = logits.reshape((b * t, c))\n",
    "    labels = labels.reshape((b * t))\n",
    "    labels_one_hot = nn.one_hot(labels, num_classes=tokenizer.get_vocab_size())\n",
    "\n",
    "    loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(state, num_epochs=100):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_batch, labels_batch = batch_loader.get_batch(\n",
    "            key, batch_size, context_lenght, is_train=True\n",
    "        )\n",
    "\n",
    "        batch = (train_batch, labels_batch)\n",
    "\n",
    "        \n",
    "        epoch_loss = jnp.array([])\n",
    "        epoch_acc = jnp.array([])\n",
    "\n",
    "        # for batch in batches:\n",
    "        state, loss = _train_step(state, batch)\n",
    "\n",
    "        jnp.append(epoch_loss, loss)\n",
    "        # epoch_acc.append(acc)\n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "\n",
    "        print(f\"Loss at epoch {epoch}: {loss}\")\n",
    "        # print(f\"Accuracy at epoch {epoch}: {epoch_acc.mean()}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_lenght), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_lenght), dtype=jnp.int32)\n",
    "\n",
    "model = BenchmarkModel(vocab_size=tokenizer.get_vocab_size())\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "variables = model.init(rngs=subkey, data=data, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919a9100e00d48f7a6798cee61782936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "Loss at epoch 0: 5.085677623748779\n",
      "(4, 8)\n",
      "Loss at epoch 1: 5.065953254699707\n",
      "(4, 8)\n",
      "Loss at epoch 2: 5.026719093322754\n",
      "(4, 8)\n",
      "Loss at epoch 3: 4.9680094718933105\n",
      "(4, 8)\n",
      "Loss at epoch 4: 4.889883518218994\n",
      "(4, 8)\n",
      "Loss at epoch 5: 4.792428493499756\n",
      "(4, 8)\n",
      "Loss at epoch 6: 4.675771236419678\n",
      "(4, 8)\n",
      "Loss at epoch 7: 4.540092468261719\n",
      "(4, 8)\n",
      "Loss at epoch 8: 4.385647773742676\n",
      "(4, 8)\n",
      "Loss at epoch 9: 4.212798118591309\n",
      "(4, 8)\n",
      "Loss at epoch 10: 4.022054672241211\n",
      "(4, 8)\n",
      "Loss at epoch 11: 3.8141448497772217\n",
      "(4, 8)\n",
      "Loss at epoch 12: 3.5900983810424805\n",
      "(4, 8)\n",
      "Loss at epoch 13: 3.351377248764038\n",
      "(4, 8)\n",
      "Loss at epoch 14: 3.1000423431396484\n",
      "(4, 8)\n",
      "Loss at epoch 15: 2.838961601257324\n",
      "(4, 8)\n",
      "Loss at epoch 16: 2.5720362663269043\n",
      "(4, 8)\n",
      "Loss at epoch 17: 2.304386615753174\n",
      "(4, 8)\n",
      "Loss at epoch 18: 2.0423736572265625\n",
      "(4, 8)\n",
      "Loss at epoch 19: 1.793302297592163\n",
      "(4, 8)\n",
      "Loss at epoch 20: 1.5646750926971436\n",
      "(4, 8)\n",
      "Loss at epoch 21: 1.3630552291870117\n",
      "(4, 8)\n",
      "Loss at epoch 22: 1.192839503288269\n",
      "(4, 8)\n",
      "Loss at epoch 23: 1.0554141998291016\n",
      "(4, 8)\n",
      "Loss at epoch 24: 0.9490710496902466\n",
      "(4, 8)\n",
      "Loss at epoch 25: 0.8697388172149658\n",
      "(4, 8)\n",
      "Loss at epoch 26: 0.8121989965438843\n",
      "(4, 8)\n",
      "Loss at epoch 27: 0.7712515592575073\n",
      "(4, 8)\n",
      "Loss at epoch 28: 0.7424468994140625\n",
      "(4, 8)\n",
      "Loss at epoch 29: 0.7223336100578308\n",
      "(4, 8)\n",
      "Loss at epoch 30: 0.7083861827850342\n",
      "(4, 8)\n",
      "Loss at epoch 31: 0.6988030672073364\n",
      "(4, 8)\n",
      "Loss at epoch 32: 0.69229656457901\n",
      "(4, 8)\n",
      "Loss at epoch 33: 0.6879275441169739\n",
      "(4, 8)\n",
      "Loss at epoch 34: 0.6849945783615112\n",
      "(4, 8)\n",
      "Loss at epoch 35: 0.6829689145088196\n",
      "(4, 8)\n",
      "Loss at epoch 36: 0.681459903717041\n",
      "(4, 8)\n",
      "Loss at epoch 37: 0.6801977753639221\n",
      "(4, 8)\n",
      "Loss at epoch 38: 0.6790215969085693\n",
      "(4, 8)\n",
      "Loss at epoch 39: 0.6778655648231506\n",
      "(4, 8)\n",
      "Loss at epoch 40: 0.6767400503158569\n",
      "(4, 8)\n",
      "Loss at epoch 41: 0.6757053732872009\n",
      "(4, 8)\n",
      "Loss at epoch 42: 0.674839973449707\n",
      "(4, 8)\n",
      "Loss at epoch 43: 0.6742089986801147\n",
      "(4, 8)\n",
      "Loss at epoch 44: 0.6738382577896118\n",
      "(4, 8)\n",
      "Loss at epoch 45: 0.6737022399902344\n",
      "(4, 8)\n",
      "Loss at epoch 46: 0.6737279891967773\n",
      "(4, 8)\n",
      "Loss at epoch 47: 0.6738168001174927\n",
      "(4, 8)\n",
      "Loss at epoch 48: 0.6738723516464233\n",
      "(4, 8)\n",
      "Loss at epoch 49: 0.673828661441803\n",
      "(4, 8)\n",
      "Loss at epoch 50: 0.6736657619476318\n",
      "(4, 8)\n",
      "Loss at epoch 51: 0.6734111309051514\n",
      "(4, 8)\n",
      "Loss at epoch 52: 0.6731247901916504\n",
      "(4, 8)\n",
      "Loss at epoch 53: 0.6728753447532654\n",
      "(4, 8)\n",
      "Loss at epoch 54: 0.6727150678634644\n",
      "(4, 8)\n",
      "Loss at epoch 55: 0.672662079334259\n",
      "(4, 8)\n",
      "Loss at epoch 56: 0.6726964712142944\n",
      "(4, 8)\n",
      "Loss at epoch 57: 0.6727706789970398\n",
      "(4, 8)\n",
      "Loss at epoch 58: 0.6728298664093018\n",
      "(4, 8)\n",
      "Loss at epoch 59: 0.6728334426879883\n",
      "(4, 8)\n",
      "Loss at epoch 60: 0.6727705597877502\n",
      "(4, 8)\n",
      "Loss at epoch 61: 0.6726609468460083\n",
      "(4, 8)\n",
      "Loss at epoch 62: 0.6725437641143799\n",
      "(4, 8)\n",
      "Loss at epoch 63: 0.6724583506584167\n",
      "(4, 8)\n",
      "Loss at epoch 64: 0.6724264621734619\n",
      "(4, 8)\n",
      "Loss at epoch 65: 0.6724435091018677\n",
      "(4, 8)\n",
      "Loss at epoch 66: 0.6724835634231567\n",
      "(4, 8)\n",
      "Loss at epoch 67: 0.6725139617919922\n",
      "(4, 8)\n",
      "Loss at epoch 68: 0.6725121140480042\n",
      "(4, 8)\n",
      "Loss at epoch 69: 0.6724755764007568\n",
      "(4, 8)\n",
      "Loss at epoch 70: 0.6724206209182739\n",
      "(4, 8)\n",
      "Loss at epoch 71: 0.6723716259002686\n",
      "(4, 8)\n",
      "Loss at epoch 72: 0.6723471879959106\n",
      "(4, 8)\n",
      "Loss at epoch 73: 0.6723506450653076\n",
      "(4, 8)\n",
      "Loss at epoch 74: 0.6723701357841492\n",
      "(4, 8)\n",
      "Loss at epoch 75: 0.6723867654800415\n",
      "(4, 8)\n",
      "Loss at epoch 76: 0.6723864674568176\n",
      "(4, 8)\n",
      "Loss at epoch 77: 0.6723674535751343\n",
      "(4, 8)\n",
      "Loss at epoch 78: 0.672339677810669\n",
      "(4, 8)\n",
      "Loss at epoch 79: 0.672317624092102\n",
      "(4, 8)\n",
      "Loss at epoch 80: 0.672310471534729\n",
      "(4, 8)\n",
      "Loss at epoch 81: 0.6723170280456543\n",
      "(4, 8)\n",
      "Loss at epoch 82: 0.6723276376724243\n",
      "(4, 8)\n",
      "Loss at epoch 83: 0.672331690788269\n",
      "(4, 8)\n",
      "Loss at epoch 84: 0.6723248958587646\n",
      "(4, 8)\n",
      "Loss at epoch 85: 0.6723109483718872\n",
      "(4, 8)\n",
      "Loss at epoch 86: 0.6722983121871948\n",
      "(4, 8)\n",
      "Loss at epoch 87: 0.6722933650016785\n",
      "(4, 8)\n",
      "Loss at epoch 88: 0.6722962856292725\n",
      "(4, 8)\n",
      "Loss at epoch 89: 0.6723018288612366\n",
      "(4, 8)\n",
      "Loss at epoch 90: 0.6723035573959351\n",
      "(4, 8)\n",
      "Loss at epoch 91: 0.6722992062568665\n",
      "(4, 8)\n",
      "Loss at epoch 92: 0.6722915172576904\n",
      "(4, 8)\n",
      "Loss at epoch 93: 0.6722853183746338\n",
      "(4, 8)\n",
      "Loss at epoch 94: 0.6722841262817383\n",
      "(4, 8)\n",
      "Loss at epoch 95: 0.6722865104675293\n",
      "(4, 8)\n",
      "Loss at epoch 96: 0.6722888946533203\n",
      "(4, 8)\n",
      "Loss at epoch 97: 0.6722881197929382\n",
      "(4, 8)\n",
      "Loss at epoch 98: 0.6722843647003174\n",
      "(4, 8)\n",
      "Loss at epoch 99: 0.6722802519798279\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables[\"params\"],\n",
    "    tx=optimizer,\n",
    ")\n",
    "\n",
    "trained_model_state = train(model_state, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input type must be an integer or unsigned integer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[431], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m----> 8\u001b[0m generated_seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrained_model_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_seq[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_text)\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/module.py:701\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 701\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/module.py:1233\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1232\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1233\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[420], line 19\u001b[0m, in \u001b[0;36mBenchmarkModel.generate\u001b[0;34m(self, key, params, data, length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[1;32m     15\u001b[0m     key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m     16\u001b[0m         key\n\u001b[1;32m     17\u001b[0m     )  \u001b[38;5;66;03m# bcs every character has to be different\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     22\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[420], line 10\u001b[0m, in \u001b[0;36mBenchmarkModel.__call__\u001b[0;34m(self, data, labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/philosophyGPT/lib/python3.11/site-packages/flax/linen/linear.py:1127\u001b[0m, in \u001b[0;36mEmbed.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embeds the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m  with an additional ``features`` dimension appended.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(inputs\u001b[38;5;241m.\u001b[39mdtype, jnp\u001b[38;5;241m.\u001b[39minteger):\n\u001b[0;32m-> 1127\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput type must be an integer or unsigned integer.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# Use take because fancy indexing numpy arrays with JAX indices does not\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# work correctly.\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m (embedding,) \u001b[38;5;241m=\u001b[39m promote_dtype(\n\u001b[1;32m   1131\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, inexact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input type must be an integer or unsigned integer."
     ]
    }
   ],
   "source": [
    "# Generate a sequence\n",
    "data_batch, labels_batch = batch_loader.get_batch(\n",
    "    key, batch_size, context_lenght, is_train=True\n",
    ")\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((4, 8)),\n",
    "    length=300,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead():\n",
    "    '''\n",
    "    One attention head\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention():\n",
    "    '''\n",
    "    Multiple attention heads combined together\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward():\n",
    "    '''A feed forward network'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
