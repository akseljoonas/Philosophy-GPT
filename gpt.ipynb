{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_lenght = 8\n",
    "train_test_split_size = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396780"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "data = tokenizer.encode(text)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, sequence_len, is_train: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if is_train:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - sequence_len)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + sequence_len]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + sequence_len + 1]\n",
    "            target_batches.append(batch_data)\n",
    "            key = subkey\n",
    "\n",
    "        train_batch = jnp.stack(train_batches)\n",
    "        target_batch = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71 70  1 71 62  1 79 65]\n",
      " [57 76  1 76 64 61  1 79]\n",
      " [ 1 76 71  1 76 64 61  0]\n",
      " [57 74 81  1 59 71 74 71]]\n",
      "[[70  1 71 62  1 79 65 75]\n",
      " [76  1 76 64 61  1 79 71]\n",
      " [76 71  1 76 64 61  0 77]\n",
      " [74 81  1 59 71 74 71 68]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=data, train_test_split_size=train_test_split_size)\n",
    "train_batch, target_batch = batch_loader.get_batch(\n",
    "    key, batch_size, context_lenght, is_train=True\n",
    ")\n",
    "print(train_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkModel(nn.Module):\n",
    "    vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(\n",
    "            num_embeddings=self.vocab_size, features=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def __call__(self, data):\n",
    "        logits = self.token_embedding_table(data)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, key, params, data, length):\n",
    "        for _ in range(length):\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )  # bcs every character has to be different\n",
    "            print(data.shape)\n",
    "            logits = self.apply({\"params\": params}, data)\n",
    "            print(logits.shape)\n",
    "            logits = logits[:, -1, :]\n",
    "            print(logits.shape)\n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            probabilities = jax.numpy.squeeze(probabilities)\n",
    "\n",
    "            next_token = jax.random.choice(\n",
    "                subkey, jax.numpy.arange(self.vocab_size), p=probabilities\n",
    "            )\n",
    "            # Reshape next_token to have a shape of (1, 1)\n",
    "            next_token = next_token.reshape((1, 1))\n",
    "            data = jax.numpy.concatenate((data, next_token), axis=1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "optimizer = optax.adamw(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "\n",
    "        data, labels = batch\n",
    "        print(data.shape)\n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            data,\n",
    "        )\n",
    "\n",
    "        b, t, c = logits.shape\n",
    "        logits = logits.reshape((b * t, c))\n",
    "        labels = labels.reshape((b * t))\n",
    "        labels_one_hot = nn.one_hot(labels, num_classes=c)\n",
    "\n",
    "\n",
    "        loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    # accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    image, label = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, label)\n",
    "    b, t, c = logits.shape\n",
    "    logits = logits.reshape((b * t, c))\n",
    "    labels = labels.reshape((b * t))\n",
    "    labels_one_hot = nn.one_hot(labels, num_classes=tokenizer.get_vocab_size())\n",
    "\n",
    "    loss = optax.losses.softmax_cross_entropy(logits=logits, labels=labels_one_hot)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(state, num_epochs=100):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_batch, labels_batch = batch_loader.get_batch(\n",
    "            key, batch_size, context_lenght, is_train=True\n",
    "        )\n",
    "\n",
    "        batch = (train_batch, labels_batch)\n",
    "\n",
    "        \n",
    "        epoch_loss = jnp.array([])\n",
    "        epoch_acc = jnp.array([])\n",
    "\n",
    "        # for batch in batches:\n",
    "        state, loss = _train_step(state, batch)\n",
    "\n",
    "        jnp.append(epoch_loss, loss)\n",
    "        # epoch_acc.append(acc)\n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "\n",
    "        print(f\"Loss at epoch {epoch}: {loss}\")\n",
    "        # print(f\"Accuracy at epoch {epoch}: {epoch_acc.mean()}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_lenght), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_lenght), dtype=jnp.int32)\n",
    "\n",
    "model = BenchmarkModel(vocab_size=tokenizer.get_vocab_size())\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "variables = model.init(rngs=subkey, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:01, 53.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "Loss at epoch 0: 5.04109001159668\n",
      "(4, 8)\n",
      "Loss at epoch 1: 5.021434783935547\n",
      "(4, 8)\n",
      "Loss at epoch 2: 4.98234224319458\n",
      "(4, 8)\n",
      "Loss at epoch 3: 4.923855304718018\n",
      "(4, 8)\n",
      "Loss at epoch 4: 4.846047401428223\n",
      "(4, 8)\n",
      "Loss at epoch 5: 4.749026298522949\n",
      "(4, 8)\n",
      "Loss at epoch 6: 4.632948875427246\n",
      "(4, 8)\n",
      "Loss at epoch 7: 4.498037338256836\n",
      "(4, 8)\n",
      "Loss at epoch 8: 4.344605922698975\n",
      "(4, 8)\n",
      "Loss at epoch 9: 4.173095226287842\n",
      "(4, 8)\n",
      "Loss at epoch 10: 3.9841246604919434\n",
      "(4, 8)\n",
      "Loss at epoch 11: 3.7785656452178955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:00<00:01, 60.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "Loss at epoch 12: 3.557640552520752\n",
      "(4, 8)\n",
      "Loss at epoch 13: 3.3230535984039307\n",
      "(4, 8)\n",
      "Loss at epoch 14: 3.0771572589874268\n",
      "(4, 8)\n",
      "Loss at epoch 15: 2.8231360912323\n",
      "(4, 8)\n",
      "Loss at epoch 16: 2.565178632736206\n",
      "(4, 8)\n",
      "Loss at epoch 17: 2.308558225631714\n",
      "(4, 8)\n",
      "Loss at epoch 18: 2.0595221519470215\n",
      "(4, 8)\n",
      "Loss at epoch 19: 1.824878454208374\n",
      "(4, 8)\n",
      "Loss at epoch 20: 1.6112446784973145\n",
      "(4, 8)\n",
      "Loss at epoch 21: 1.4240667819976807\n",
      "(4, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:00<00:01, 52.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 22: 1.2666709423065186\n",
      "(4, 8)\n",
      "Loss at epoch 23: 1.1396886110305786\n",
      "(4, 8)\n",
      "Loss at epoch 24: 1.0411162376403809\n",
      "(4, 8)\n",
      "Loss at epoch 25: 0.9670398831367493\n",
      "(4, 8)\n",
      "Loss at epoch 26: 0.9127203226089478\n",
      "(4, 8)\n",
      "Loss at epoch 27: 0.8735772371292114\n",
      "(4, 8)\n",
      "Loss at epoch 28: 0.8457598686218262\n",
      "(4, 8)\n",
      "Loss at epoch 29: 0.8262929916381836\n",
      "(4, 8)\n",
      "Loss at epoch 30: 0.8129636645317078\n",
      "(4, 8)\n",
      "Loss at epoch 31: 0.8041189908981323\n",
      "(4, 8)\n",
      "Loss at epoch 32: 0.7984839677810669\n",
      "(4, 8)\n",
      "Loss at epoch 33: 0.7950371503829956\n",
      "(4, 8)\n",
      "Loss at epoch 34: 0.7929472923278809\n",
      "(4, 8)\n",
      "Loss at epoch 35: 0.7915540933609009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:00<00:00, 60.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "Loss at epoch 36: 0.790370762348175\n",
      "(4, 8)\n",
      "Loss at epoch 37: 0.7890911102294922\n",
      "(4, 8)\n",
      "Loss at epoch 38: 0.7875866293907166\n",
      "(4, 8)\n",
      "Loss at epoch 39: 0.7858847379684448\n",
      "(4, 8)\n",
      "Loss at epoch 40: 0.7841284275054932\n",
      "(4, 8)\n",
      "Loss at epoch 41: 0.7825192213058472\n",
      "(4, 8)\n",
      "Loss at epoch 42: 0.7812515497207642\n",
      "(4, 8)\n",
      "Loss at epoch 43: 0.7804527878761292\n",
      "(4, 8)\n",
      "Loss at epoch 44: 0.7801440358161926\n",
      "(4, 8)\n",
      "Loss at epoch 45: 0.780231773853302\n",
      "(4, 8)\n",
      "Loss at epoch 46: 0.7805368900299072\n",
      "(4, 8)\n",
      "Loss at epoch 47: 0.7808492183685303\n",
      "(4, 8)\n",
      "Loss at epoch 48: 0.7809922695159912\n",
      "(4, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:01<00:00, 64.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 49: 0.7808731198310852\n",
      "(4, 8)\n",
      "Loss at epoch 50: 0.7805042266845703\n",
      "(4, 8)\n",
      "Loss at epoch 51: 0.7799868583679199\n",
      "(4, 8)\n",
      "Loss at epoch 52: 0.7794675827026367\n",
      "(4, 8)\n",
      "Loss at epoch 53: 0.779080867767334\n",
      "(4, 8)\n",
      "Loss at epoch 54: 0.7789023518562317\n",
      "(4, 8)\n",
      "Loss at epoch 55: 0.7789262533187866\n",
      "(4, 8)\n",
      "Loss at epoch 56: 0.779076337814331\n",
      "(4, 8)\n",
      "Loss at epoch 57: 0.7792419195175171\n",
      "(4, 8)\n",
      "Loss at epoch 58: 0.779325008392334\n",
      "(4, 8)\n",
      "Loss at epoch 59: 0.7792768478393555\n",
      "(4, 8)\n",
      "Loss at epoch 60: 0.7791121006011963\n",
      "(4, 8)\n",
      "Loss at epoch 61: 0.7788952589035034\n",
      "(4, 8)\n",
      "Loss at epoch 62: 0.7787065505981445\n",
      "(4, 8)\n",
      "Loss at epoch 63: 0.7786048054695129\n",
      "(4, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:01<00:00, 63.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 64: 0.778603196144104\n",
      "(4, 8)\n",
      "Loss at epoch 65: 0.7786681652069092\n",
      "(4, 8)\n",
      "Loss at epoch 66: 0.7787410020828247\n",
      "(4, 8)\n",
      "Loss at epoch 67: 0.7787697315216064\n",
      "(4, 8)\n",
      "Loss at epoch 68: 0.7787336111068726\n",
      "(4, 8)\n",
      "Loss at epoch 69: 0.7786492109298706\n",
      "(4, 8)\n",
      "Loss at epoch 70: 0.7785566449165344\n",
      "(4, 8)\n",
      "Loss at epoch 71: 0.7784950137138367\n",
      "(4, 8)\n",
      "Loss at epoch 72: 0.7784813046455383\n",
      "(4, 8)\n",
      "Loss at epoch 73: 0.7785046100616455\n",
      "(4, 8)\n",
      "Loss at epoch 74: 0.778536319732666\n",
      "(4, 8)\n",
      "Loss at epoch 75: 0.7785485982894897\n",
      "(4, 8)\n",
      "Loss at epoch 76: 0.778530478477478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [00:01<00:00, 61.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "Loss at epoch 77: 0.7784910798072815\n",
      "(4, 8)\n",
      "Loss at epoch 78: 0.7784513235092163\n",
      "(4, 8)\n",
      "Loss at epoch 79: 0.778429388999939\n",
      "(4, 8)\n",
      "Loss at epoch 80: 0.778429388999939\n",
      "(4, 8)\n",
      "Loss at epoch 81: 0.7784420251846313\n",
      "(4, 8)\n",
      "Loss at epoch 82: 0.778451681137085\n",
      "(4, 8)\n",
      "Loss at epoch 83: 0.7784483432769775\n",
      "(4, 8)\n",
      "Loss at epoch 84: 0.7784326076507568\n",
      "(4, 8)\n",
      "Loss at epoch 85: 0.7784135937690735\n",
      "(4, 8)\n",
      "Loss at epoch 86: 0.7784010767936707\n",
      "(4, 8)\n",
      "Loss at epoch 87: 0.778398871421814\n",
      "(4, 8)\n",
      "Loss at epoch 88: 0.7784031629562378\n",
      "(4, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 61.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 89: 0.7784066200256348\n",
      "(4, 8)\n",
      "Loss at epoch 90: 0.7784044742584229\n",
      "(4, 8)\n",
      "Loss at epoch 91: 0.7783969640731812\n",
      "(4, 8)\n",
      "Loss at epoch 92: 0.7783883810043335\n",
      "(4, 8)\n",
      "Loss at epoch 93: 0.7783830165863037\n",
      "(4, 8)\n",
      "Loss at epoch 94: 0.7783821225166321\n",
      "(4, 8)\n",
      "Loss at epoch 95: 0.7783835530281067\n",
      "(4, 8)\n",
      "Loss at epoch 96: 0.7783839702606201\n",
      "(4, 8)\n",
      "Loss at epoch 97: 0.778381884098053\n",
      "(4, 8)\n",
      "Loss at epoch 98: 0.7783777713775635\n",
      "(4, 8)\n",
      "Loss at epoch 99: 0.7783738374710083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables[\"params\"],\n",
    "    tx=optimizer,\n",
    ")\n",
    "\n",
    "trained_model_state = train(model_state, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 1, 160)\n",
      "(1, 160)\n",
      "(1, 2)\n",
      "(1, 2, 160)\n",
      "(1, 160)\n",
      "(1, 3)\n",
      "(1, 3, 160)\n",
      "(1, 160)\n",
      "(1, 4)\n",
      "(1, 4, 160)\n",
      "(1, 160)\n",
      "(1, 5)\n",
      "(1, 5, 160)\n",
      "(1, 160)\n",
      "(1, 6)\n",
      "(1, 6, 160)\n",
      "(1, 160)\n",
      "(1, 7)\n",
      "(1, 7, 160)\n",
      "(1, 160)\n",
      "(1, 8)\n",
      "(1, 8, 160)\n",
      "(1, 160)\n",
      "(1, 9)\n",
      "(1, 9, 160)\n",
      "(1, 160)\n",
      "(1, 10)\n",
      "(1, 10, 160)\n",
      "(1, 160)\n",
      "(1, 11)\n",
      "(1, 11, 160)\n",
      "(1, 160)\n",
      "(1, 12)\n",
      "(1, 12, 160)\n",
      "(1, 160)\n",
      "(1, 13)\n",
      "(1, 13, 160)\n",
      "(1, 160)\n",
      "(1, 14)\n",
      "(1, 14, 160)\n",
      "(1, 160)\n",
      "(1, 15)\n",
      "(1, 15, 160)\n",
      "(1, 160)\n",
      "(1, 16)\n",
      "(1, 16, 160)\n",
      "(1, 160)\n",
      "(1, 17)\n",
      "(1, 17, 160)\n",
      "(1, 160)\n",
      "(1, 18)\n",
      "(1, 18, 160)\n",
      "(1, 160)\n",
      "(1, 19)\n",
      "(1, 19, 160)\n",
      "(1, 160)\n",
      "(1, 20)\n",
      "(1, 20, 160)\n",
      "(1, 160)\n",
      "(1, 21)\n",
      "(1, 21, 160)\n",
      "(1, 160)\n",
      "(1, 22)\n",
      "(1, 22, 160)\n",
      "(1, 160)\n",
      "(1, 23)\n",
      "(1, 23, 160)\n",
      "(1, 160)\n",
      "(1, 24)\n",
      "(1, 24, 160)\n",
      "(1, 160)\n",
      "(1, 25)\n",
      "(1, 25, 160)\n",
      "(1, 160)\n",
      "(1, 26)\n",
      "(1, 26, 160)\n",
      "(1, 160)\n",
      "(1, 27)\n",
      "(1, 27, 160)\n",
      "(1, 160)\n",
      "(1, 28)\n",
      "(1, 28, 160)\n",
      "(1, 160)\n",
      "(1, 29)\n",
      "(1, 29, 160)\n",
      "(1, 160)\n",
      "(1, 30)\n",
      "(1, 30, 160)\n",
      "(1, 160)\n",
      "(1, 31)\n",
      "(1, 31, 160)\n",
      "(1, 160)\n",
      "(1, 32)\n",
      "(1, 32, 160)\n",
      "(1, 160)\n",
      "(1, 33)\n",
      "(1, 33, 160)\n",
      "(1, 160)\n",
      "(1, 34)\n",
      "(1, 34, 160)\n",
      "(1, 160)\n",
      "(1, 35)\n",
      "(1, 35, 160)\n",
      "(1, 160)\n",
      "(1, 36)\n",
      "(1, 36, 160)\n",
      "(1, 160)\n",
      "(1, 37)\n",
      "(1, 37, 160)\n",
      "(1, 160)\n",
      "(1, 38)\n",
      "(1, 38, 160)\n",
      "(1, 160)\n",
      "(1, 39)\n",
      "(1, 39, 160)\n",
      "(1, 160)\n",
      "(1, 40)\n",
      "(1, 40, 160)\n",
      "(1, 160)\n",
      "(1, 41)\n",
      "(1, 41, 160)\n",
      "(1, 160)\n",
      "(1, 42)\n",
      "(1, 42, 160)\n",
      "(1, 160)\n",
      "(1, 43)\n",
      "(1, 43, 160)\n",
      "(1, 160)\n",
      "(1, 44)\n",
      "(1, 44, 160)\n",
      "(1, 160)\n",
      "(1, 45)\n",
      "(1, 45, 160)\n",
      "(1, 160)\n",
      "(1, 46)\n",
      "(1, 46, 160)\n",
      "(1, 160)\n",
      "(1, 47)\n",
      "(1, 47, 160)\n",
      "(1, 160)\n",
      "(1, 48)\n",
      "(1, 48, 160)\n",
      "(1, 160)\n",
      "(1, 49)\n",
      "(1, 49, 160)\n",
      "(1, 160)\n",
      "(1, 50)\n",
      "(1, 50, 160)\n",
      "(1, 160)\n",
      "(1, 51)\n",
      "(1, 51, 160)\n",
      "(1, 160)\n",
      "(1, 52)\n",
      "(1, 52, 160)\n",
      "(1, 160)\n",
      "(1, 53)\n",
      "(1, 53, 160)\n",
      "(1, 160)\n",
      "(1, 54)\n",
      "(1, 54, 160)\n",
      "(1, 160)\n",
      "(1, 55)\n",
      "(1, 55, 160)\n",
      "(1, 160)\n",
      "(1, 56)\n",
      "(1, 56, 160)\n",
      "(1, 160)\n",
      "(1, 57)\n",
      "(1, 57, 160)\n",
      "(1, 160)\n",
      "(1, 58)\n",
      "(1, 58, 160)\n",
      "(1, 160)\n",
      "(1, 59)\n",
      "(1, 59, 160)\n",
      "(1, 160)\n",
      "(1, 60)\n",
      "(1, 60, 160)\n",
      "(1, 160)\n",
      "(1, 61)\n",
      "(1, 61, 160)\n",
      "(1, 160)\n",
      "(1, 62)\n",
      "(1, 62, 160)\n",
      "(1, 160)\n",
      "(1, 63)\n",
      "(1, 63, 160)\n",
      "(1, 160)\n",
      "(1, 64)\n",
      "(1, 64, 160)\n",
      "(1, 160)\n",
      "(1, 65)\n",
      "(1, 65, 160)\n",
      "(1, 160)\n",
      "(1, 66)\n",
      "(1, 66, 160)\n",
      "(1, 160)\n",
      "(1, 67)\n",
      "(1, 67, 160)\n",
      "(1, 160)\n",
      "(1, 68)\n",
      "(1, 68, 160)\n",
      "(1, 160)\n",
      "(1, 69)\n",
      "(1, 69, 160)\n",
      "(1, 160)\n",
      "(1, 70)\n",
      "(1, 70, 160)\n",
      "(1, 160)\n",
      "(1, 71)\n",
      "(1, 71, 160)\n",
      "(1, 160)\n",
      "(1, 72)\n",
      "(1, 72, 160)\n",
      "(1, 160)\n",
      "(1, 73)\n",
      "(1, 73, 160)\n",
      "(1, 160)\n",
      "(1, 74)\n",
      "(1, 74, 160)\n",
      "(1, 160)\n",
      "(1, 75)\n",
      "(1, 75, 160)\n",
      "(1, 160)\n",
      "(1, 76)\n",
      "(1, 76, 160)\n",
      "(1, 160)\n",
      "(1, 77)\n",
      "(1, 77, 160)\n",
      "(1, 160)\n",
      "(1, 78)\n",
      "(1, 78, 160)\n",
      "(1, 160)\n",
      "(1, 79)\n",
      "(1, 79, 160)\n",
      "(1, 160)\n",
      "(1, 80)\n",
      "(1, 80, 160)\n",
      "(1, 160)\n",
      "(1, 81)\n",
      "(1, 81, 160)\n",
      "(1, 160)\n",
      "(1, 82)\n",
      "(1, 82, 160)\n",
      "(1, 160)\n",
      "(1, 83)\n",
      "(1, 83, 160)\n",
      "(1, 160)\n",
      "(1, 84)\n",
      "(1, 84, 160)\n",
      "(1, 160)\n",
      "(1, 85)\n",
      "(1, 85, 160)\n",
      "(1, 160)\n",
      "(1, 86)\n",
      "(1, 86, 160)\n",
      "(1, 160)\n",
      "(1, 87)\n",
      "(1, 87, 160)\n",
      "(1, 160)\n",
      "(1, 88)\n",
      "(1, 88, 160)\n",
      "(1, 160)\n",
      "(1, 89)\n",
      "(1, 89, 160)\n",
      "(1, 160)\n",
      "(1, 90)\n",
      "(1, 90, 160)\n",
      "(1, 160)\n",
      "(1, 91)\n",
      "(1, 91, 160)\n",
      "(1, 160)\n",
      "(1, 92)\n",
      "(1, 92, 160)\n",
      "(1, 160)\n",
      "(1, 93)\n",
      "(1, 93, 160)\n",
      "(1, 160)\n",
      "(1, 94)\n",
      "(1, 94, 160)\n",
      "(1, 160)\n",
      "(1, 95)\n",
      "(1, 95, 160)\n",
      "(1, 160)\n",
      "(1, 96)\n",
      "(1, 96, 160)\n",
      "(1, 160)\n",
      "(1, 97)\n",
      "(1, 97, 160)\n",
      "(1, 160)\n",
      "(1, 98)\n",
      "(1, 98, 160)\n",
      "(1, 160)\n",
      "(1, 99)\n",
      "(1, 99, 160)\n",
      "(1, 160)\n",
      "(1, 100)\n",
      "(1, 100, 160)\n",
      "(1, 160)\n",
      "(1, 101)\n",
      "(1, 101, 160)\n",
      "(1, 160)\n",
      "(1, 102)\n",
      "(1, 102, 160)\n",
      "(1, 160)\n",
      "(1, 103)\n",
      "(1, 103, 160)\n",
      "(1, 160)\n",
      "(1, 104)\n",
      "(1, 104, 160)\n",
      "(1, 160)\n",
      "(1, 105)\n",
      "(1, 105, 160)\n",
      "(1, 160)\n",
      "(1, 106)\n",
      "(1, 106, 160)\n",
      "(1, 160)\n",
      "(1, 107)\n",
      "(1, 107, 160)\n",
      "(1, 160)\n",
      "(1, 108)\n",
      "(1, 108, 160)\n",
      "(1, 160)\n",
      "(1, 109)\n",
      "(1, 109, 160)\n",
      "(1, 160)\n",
      "(1, 110)\n",
      "(1, 110, 160)\n",
      "(1, 160)\n",
      "(1, 111)\n",
      "(1, 111, 160)\n",
      "(1, 160)\n",
      "(1, 112)\n",
      "(1, 112, 160)\n",
      "(1, 160)\n",
      "(1, 113)\n",
      "(1, 113, 160)\n",
      "(1, 160)\n",
      "(1, 114)\n",
      "(1, 114, 160)\n",
      "(1, 160)\n",
      "(1, 115)\n",
      "(1, 115, 160)\n",
      "(1, 160)\n",
      "(1, 116)\n",
      "(1, 116, 160)\n",
      "(1, 160)\n",
      "(1, 117)\n",
      "(1, 117, 160)\n",
      "(1, 160)\n",
      "(1, 118)\n",
      "(1, 118, 160)\n",
      "(1, 160)\n",
      "(1, 119)\n",
      "(1, 119, 160)\n",
      "(1, 160)\n",
      "(1, 120)\n",
      "(1, 120, 160)\n",
      "(1, 160)\n",
      "(1, 121)\n",
      "(1, 121, 160)\n",
      "(1, 160)\n",
      "(1, 122)\n",
      "(1, 122, 160)\n",
      "(1, 160)\n",
      "(1, 123)\n",
      "(1, 123, 160)\n",
      "(1, 160)\n",
      "(1, 124)\n",
      "(1, 124, 160)\n",
      "(1, 160)\n",
      "(1, 125)\n",
      "(1, 125, 160)\n",
      "(1, 160)\n",
      "(1, 126)\n",
      "(1, 126, 160)\n",
      "(1, 160)\n",
      "(1, 127)\n",
      "(1, 127, 160)\n",
      "(1, 160)\n",
      "(1, 128)\n",
      "(1, 128, 160)\n",
      "(1, 160)\n",
      "(1, 129)\n",
      "(1, 129, 160)\n",
      "(1, 160)\n",
      "(1, 130)\n",
      "(1, 130, 160)\n",
      "(1, 160)\n",
      "(1, 131)\n",
      "(1, 131, 160)\n",
      "(1, 160)\n",
      "(1, 132)\n",
      "(1, 132, 160)\n",
      "(1, 160)\n",
      "(1, 133)\n",
      "(1, 133, 160)\n",
      "(1, 160)\n",
      "(1, 134)\n",
      "(1, 134, 160)\n",
      "(1, 160)\n",
      "(1, 135)\n",
      "(1, 135, 160)\n",
      "(1, 160)\n",
      "(1, 136)\n",
      "(1, 136, 160)\n",
      "(1, 160)\n",
      "(1, 137)\n",
      "(1, 137, 160)\n",
      "(1, 160)\n",
      "(1, 138)\n",
      "(1, 138, 160)\n",
      "(1, 160)\n",
      "(1, 139)\n",
      "(1, 139, 160)\n",
      "(1, 160)\n",
      "(1, 140)\n",
      "(1, 140, 160)\n",
      "(1, 160)\n",
      "(1, 141)\n",
      "(1, 141, 160)\n",
      "(1, 160)\n",
      "(1, 142)\n",
      "(1, 142, 160)\n",
      "(1, 160)\n",
      "(1, 143)\n",
      "(1, 143, 160)\n",
      "(1, 160)\n",
      "(1, 144)\n",
      "(1, 144, 160)\n",
      "(1, 160)\n",
      "(1, 145)\n",
      "(1, 145, 160)\n",
      "(1, 160)\n",
      "(1, 146)\n",
      "(1, 146, 160)\n",
      "(1, 160)\n",
      "(1, 147)\n",
      "(1, 147, 160)\n",
      "(1, 160)\n",
      "(1, 148)\n",
      "(1, 148, 160)\n",
      "(1, 160)\n",
      "(1, 149)\n",
      "(1, 149, 160)\n",
      "(1, 160)\n",
      "(1, 150)\n",
      "(1, 150, 160)\n",
      "(1, 160)\n",
      "(1, 151)\n",
      "(1, 151, 160)\n",
      "(1, 160)\n",
      "(1, 152)\n",
      "(1, 152, 160)\n",
      "(1, 160)\n",
      "(1, 153)\n",
      "(1, 153, 160)\n",
      "(1, 160)\n",
      "(1, 154)\n",
      "(1, 154, 160)\n",
      "(1, 160)\n",
      "(1, 155)\n",
      "(1, 155, 160)\n",
      "(1, 160)\n",
      "(1, 156)\n",
      "(1, 156, 160)\n",
      "(1, 160)\n",
      "(1, 157)\n",
      "(1, 157, 160)\n",
      "(1, 160)\n",
      "(1, 158)\n",
      "(1, 158, 160)\n",
      "(1, 160)\n",
      "(1, 159)\n",
      "(1, 159, 160)\n",
      "(1, 160)\n",
      "(1, 160)\n",
      "(1, 160, 160)\n",
      "(1, 160)\n",
      "(1, 161)\n",
      "(1, 161, 160)\n",
      "(1, 160)\n",
      "(1, 162)\n",
      "(1, 162, 160)\n",
      "(1, 160)\n",
      "(1, 163)\n",
      "(1, 163, 160)\n",
      "(1, 160)\n",
      "(1, 164)\n",
      "(1, 164, 160)\n",
      "(1, 160)\n",
      "(1, 165)\n",
      "(1, 165, 160)\n",
      "(1, 160)\n",
      "(1, 166)\n",
      "(1, 166, 160)\n",
      "(1, 160)\n",
      "(1, 167)\n",
      "(1, 167, 160)\n",
      "(1, 160)\n",
      "(1, 168)\n",
      "(1, 168, 160)\n",
      "(1, 160)\n",
      "(1, 169)\n",
      "(1, 169, 160)\n",
      "(1, 160)\n",
      "(1, 170)\n",
      "(1, 170, 160)\n",
      "(1, 160)\n",
      "(1, 171)\n",
      "(1, 171, 160)\n",
      "(1, 160)\n",
      "(1, 172)\n",
      "(1, 172, 160)\n",
      "(1, 160)\n",
      "(1, 173)\n",
      "(1, 173, 160)\n",
      "(1, 160)\n",
      "(1, 174)\n",
      "(1, 174, 160)\n",
      "(1, 160)\n",
      "(1, 175)\n",
      "(1, 175, 160)\n",
      "(1, 160)\n",
      "(1, 176)\n",
      "(1, 176, 160)\n",
      "(1, 160)\n",
      "(1, 177)\n",
      "(1, 177, 160)\n",
      "(1, 160)\n",
      "(1, 178)\n",
      "(1, 178, 160)\n",
      "(1, 160)\n",
      "(1, 179)\n",
      "(1, 179, 160)\n",
      "(1, 160)\n",
      "(1, 180)\n",
      "(1, 180, 160)\n",
      "(1, 160)\n",
      "(1, 181)\n",
      "(1, 181, 160)\n",
      "(1, 160)\n",
      "(1, 182)\n",
      "(1, 182, 160)\n",
      "(1, 160)\n",
      "(1, 183)\n",
      "(1, 183, 160)\n",
      "(1, 160)\n",
      "(1, 184)\n",
      "(1, 184, 160)\n",
      "(1, 160)\n",
      "(1, 185)\n",
      "(1, 185, 160)\n",
      "(1, 160)\n",
      "(1, 186)\n",
      "(1, 186, 160)\n",
      "(1, 160)\n",
      "(1, 187)\n",
      "(1, 187, 160)\n",
      "(1, 160)\n",
      "(1, 188)\n",
      "(1, 188, 160)\n",
      "(1, 160)\n",
      "(1, 189)\n",
      "(1, 189, 160)\n",
      "(1, 160)\n",
      "(1, 190)\n",
      "(1, 190, 160)\n",
      "(1, 160)\n",
      "(1, 191)\n",
      "(1, 191, 160)\n",
      "(1, 160)\n",
      "(1, 192)\n",
      "(1, 192, 160)\n",
      "(1, 160)\n",
      "(1, 193)\n",
      "(1, 193, 160)\n",
      "(1, 160)\n",
      "(1, 194)\n",
      "(1, 194, 160)\n",
      "(1, 160)\n",
      "(1, 195)\n",
      "(1, 195, 160)\n",
      "(1, 160)\n",
      "(1, 196)\n",
      "(1, 196, 160)\n",
      "(1, 160)\n",
      "(1, 197)\n",
      "(1, 197, 160)\n",
      "(1, 160)\n",
      "(1, 198)\n",
      "(1, 198, 160)\n",
      "(1, 160)\n",
      "(1, 199)\n",
      "(1, 199, 160)\n",
      "(1, 160)\n",
      "(1, 200)\n",
      "(1, 200, 160)\n",
      "(1, 160)\n",
      "(1, 201)\n",
      "(1, 201, 160)\n",
      "(1, 160)\n",
      "(1, 202)\n",
      "(1, 202, 160)\n",
      "(1, 160)\n",
      "(1, 203)\n",
      "(1, 203, 160)\n",
      "(1, 160)\n",
      "(1, 204)\n",
      "(1, 204, 160)\n",
      "(1, 160)\n",
      "(1, 205)\n",
      "(1, 205, 160)\n",
      "(1, 160)\n",
      "(1, 206)\n",
      "(1, 206, 160)\n",
      "(1, 160)\n",
      "(1, 207)\n",
      "(1, 207, 160)\n",
      "(1, 160)\n",
      "(1, 208)\n",
      "(1, 208, 160)\n",
      "(1, 160)\n",
      "(1, 209)\n",
      "(1, 209, 160)\n",
      "(1, 160)\n",
      "(1, 210)\n",
      "(1, 210, 160)\n",
      "(1, 160)\n",
      "(1, 211)\n",
      "(1, 211, 160)\n",
      "(1, 160)\n",
      "(1, 212)\n",
      "(1, 212, 160)\n",
      "(1, 160)\n",
      "(1, 213)\n",
      "(1, 213, 160)\n",
      "(1, 160)\n",
      "(1, 214)\n",
      "(1, 214, 160)\n",
      "(1, 160)\n",
      "(1, 215)\n",
      "(1, 215, 160)\n",
      "(1, 160)\n",
      "(1, 216)\n",
      "(1, 216, 160)\n",
      "(1, 160)\n",
      "(1, 217)\n",
      "(1, 217, 160)\n",
      "(1, 160)\n",
      "(1, 218)\n",
      "(1, 218, 160)\n",
      "(1, 160)\n",
      "(1, 219)\n",
      "(1, 219, 160)\n",
      "(1, 160)\n",
      "(1, 220)\n",
      "(1, 220, 160)\n",
      "(1, 160)\n",
      "(1, 221)\n",
      "(1, 221, 160)\n",
      "(1, 160)\n",
      "(1, 222)\n",
      "(1, 222, 160)\n",
      "(1, 160)\n",
      "(1, 223)\n",
      "(1, 223, 160)\n",
      "(1, 160)\n",
      "(1, 224)\n",
      "(1, 224, 160)\n",
      "(1, 160)\n",
      "(1, 225)\n",
      "(1, 225, 160)\n",
      "(1, 160)\n",
      "(1, 226)\n",
      "(1, 226, 160)\n",
      "(1, 160)\n",
      "(1, 227)\n",
      "(1, 227, 160)\n",
      "(1, 160)\n",
      "(1, 228)\n",
      "(1, 228, 160)\n",
      "(1, 160)\n",
      "(1, 229)\n",
      "(1, 229, 160)\n",
      "(1, 160)\n",
      "(1, 230)\n",
      "(1, 230, 160)\n",
      "(1, 160)\n",
      "(1, 231)\n",
      "(1, 231, 160)\n",
      "(1, 160)\n",
      "(1, 232)\n",
      "(1, 232, 160)\n",
      "(1, 160)\n",
      "(1, 233)\n",
      "(1, 233, 160)\n",
      "(1, 160)\n",
      "(1, 234)\n",
      "(1, 234, 160)\n",
      "(1, 160)\n",
      "(1, 235)\n",
      "(1, 235, 160)\n",
      "(1, 160)\n",
      "(1, 236)\n",
      "(1, 236, 160)\n",
      "(1, 160)\n",
      "(1, 237)\n",
      "(1, 237, 160)\n",
      "(1, 160)\n",
      "(1, 238)\n",
      "(1, 238, 160)\n",
      "(1, 160)\n",
      "(1, 239)\n",
      "(1, 239, 160)\n",
      "(1, 160)\n",
      "(1, 240)\n",
      "(1, 240, 160)\n",
      "(1, 160)\n",
      "(1, 241)\n",
      "(1, 241, 160)\n",
      "(1, 160)\n",
      "(1, 242)\n",
      "(1, 242, 160)\n",
      "(1, 160)\n",
      "(1, 243)\n",
      "(1, 243, 160)\n",
      "(1, 160)\n",
      "(1, 244)\n",
      "(1, 244, 160)\n",
      "(1, 160)\n",
      "(1, 245)\n",
      "(1, 245, 160)\n",
      "(1, 160)\n",
      "(1, 246)\n",
      "(1, 246, 160)\n",
      "(1, 160)\n",
      "(1, 247)\n",
      "(1, 247, 160)\n",
      "(1, 160)\n",
      "(1, 248)\n",
      "(1, 248, 160)\n",
      "(1, 160)\n",
      "(1, 249)\n",
      "(1, 249, 160)\n",
      "(1, 160)\n",
      "(1, 250)\n",
      "(1, 250, 160)\n",
      "(1, 160)\n",
      "(1, 251)\n",
      "(1, 251, 160)\n",
      "(1, 160)\n",
      "(1, 252)\n",
      "(1, 252, 160)\n",
      "(1, 160)\n",
      "(1, 253)\n",
      "(1, 253, 160)\n",
      "(1, 160)\n",
      "(1, 254)\n",
      "(1, 254, 160)\n",
      "(1, 160)\n",
      "(1, 255)\n",
      "(1, 255, 160)\n",
      "(1, 160)\n",
      "(1, 256)\n",
      "(1, 256, 160)\n",
      "(1, 160)\n",
      "(1, 257)\n",
      "(1, 257, 160)\n",
      "(1, 160)\n",
      "(1, 258)\n",
      "(1, 258, 160)\n",
      "(1, 160)\n",
      "(1, 259)\n",
      "(1, 259, 160)\n",
      "(1, 160)\n",
      "(1, 260)\n",
      "(1, 260, 160)\n",
      "(1, 160)\n",
      "(1, 261)\n",
      "(1, 261, 160)\n",
      "(1, 160)\n",
      "(1, 262)\n",
      "(1, 262, 160)\n",
      "(1, 160)\n",
      "(1, 263)\n",
      "(1, 263, 160)\n",
      "(1, 160)\n",
      "(1, 264)\n",
      "(1, 264, 160)\n",
      "(1, 160)\n",
      "(1, 265)\n",
      "(1, 265, 160)\n",
      "(1, 160)\n",
      "(1, 266)\n",
      "(1, 266, 160)\n",
      "(1, 160)\n",
      "(1, 267)\n",
      "(1, 267, 160)\n",
      "(1, 160)\n",
      "(1, 268)\n",
      "(1, 268, 160)\n",
      "(1, 160)\n",
      "(1, 269)\n",
      "(1, 269, 160)\n",
      "(1, 160)\n",
      "(1, 270)\n",
      "(1, 270, 160)\n",
      "(1, 160)\n",
      "(1, 271)\n",
      "(1, 271, 160)\n",
      "(1, 160)\n",
      "(1, 272)\n",
      "(1, 272, 160)\n",
      "(1, 160)\n",
      "(1, 273)\n",
      "(1, 273, 160)\n",
      "(1, 160)\n",
      "(1, 274)\n",
      "(1, 274, 160)\n",
      "(1, 160)\n",
      "(1, 275)\n",
      "(1, 275, 160)\n",
      "(1, 160)\n",
      "(1, 276)\n",
      "(1, 276, 160)\n",
      "(1, 160)\n",
      "(1, 277)\n",
      "(1, 277, 160)\n",
      "(1, 160)\n",
      "(1, 278)\n",
      "(1, 278, 160)\n",
      "(1, 160)\n",
      "(1, 279)\n",
      "(1, 279, 160)\n",
      "(1, 160)\n",
      "(1, 280)\n",
      "(1, 280, 160)\n",
      "(1, 160)\n",
      "(1, 281)\n",
      "(1, 281, 160)\n",
      "(1, 160)\n",
      "(1, 282)\n",
      "(1, 282, 160)\n",
      "(1, 160)\n",
      "(1, 283)\n",
      "(1, 283, 160)\n",
      "(1, 160)\n",
      "(1, 284)\n",
      "(1, 284, 160)\n",
      "(1, 160)\n",
      "(1, 285)\n",
      "(1, 285, 160)\n",
      "(1, 160)\n",
      "(1, 286)\n",
      "(1, 286, 160)\n",
      "(1, 160)\n",
      "(1, 287)\n",
      "(1, 287, 160)\n",
      "(1, 160)\n",
      "(1, 288)\n",
      "(1, 288, 160)\n",
      "(1, 160)\n",
      "(1, 289)\n",
      "(1, 289, 160)\n",
      "(1, 160)\n",
      "(1, 290)\n",
      "(1, 290, 160)\n",
      "(1, 160)\n",
      "(1, 291)\n",
      "(1, 291, 160)\n",
      "(1, 160)\n",
      "(1, 292)\n",
      "(1, 292, 160)\n",
      "(1, 160)\n",
      "(1, 293)\n",
      "(1, 293, 160)\n",
      "(1, 160)\n",
      "(1, 294)\n",
      "(1, 294, 160)\n",
      "(1, 160)\n",
      "(1, 295)\n",
      "(1, 295, 160)\n",
      "(1, 160)\n",
      "(1, 296)\n",
      "(1, 296, 160)\n",
      "(1, 160)\n",
      "(1, 297)\n",
      "(1, 297, 160)\n",
      "(1, 160)\n",
      "(1, 298)\n",
      "(1, 298, 160)\n",
      "(1, 160)\n",
      "(1, 299)\n",
      "(1, 299, 160)\n",
      "(1, 160)\n",
      "(1, 300)\n",
      "(1, 300, 160)\n",
      "(1, 160)\n",
      "\n",
      "ύἐèæὸῡxJen all a sides Jewiden Jewisym§ὸί as s siden a siden is sidewisidesympgm;“᾽den iden then al ides sidewiden al Jen sidewiden asymἰŒ)DdesisidewidewisymDζἄαöôden thesymρλ=]öîχ\n",
      "ής7Aχγάγ§Œ)œthesidesymζsymcal a s a sidewidewis Jewisis isymùὰ–, then al thesidewis Jesymq“᾽λïîἰymνcas isidewidewiden s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.zeros((1, 1), dtype=jax.numpy.int32),\n",
    "    length=300,\n",
    ")\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead():\n",
    "    '''\n",
    "    One attention head\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention():\n",
    "    '''\n",
    "    Multiple attention heads combined together\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward():\n",
    "    '''A feed forward network'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
