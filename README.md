# Philosophy-GPT
Resources to watch before next meeting:

### Andrej Karpathy full transformer tutorial.
https://www.youtube.com/watch?v=kCc8FmEb1nY&t=583s

### 3blue1brown visual explanation of Transformers
https://youtu.be/wjZofJX0v4M?si=7N4QQLC91dXZyxX0

https://youtu.be/eMlx5fFNoYc?si=eymA7IqSqssoPaUI

### Good math intuition for Transformers
https://www.youtube.com/watch?v=kWLed8o5M2Y&t=561s


## For JAX

Brightspace tutorials?
https://brightspace.rug.nl/d2l/common/dialogs/quickLink/quickLink.d2l?ou=243041&type=lti&rcode=8AE7AF6F-0534-48B3-97C6-4FDC1563223B-63518&srcou=6606&launchFramed=1&framedName=Kaltura+Videos+%26+Classroom

Notion page of the TA
https://stevenabreu7.notion.site/NN-Programming-Tutorial-abe5dc24c9344f4aacaa37fa216262b1#9df8c1681c804be5addb5d69475eb46e

### Notes on JAX:

[Flax](https://flax.readthedocs.io/en/latest/index.html) for prebuilt blocks that we dont want to implement ourselves.

[Optuna](https://optuna.org/) for hyperparameter tuning.

[Optax](https://optax.readthedocs.io/en/latest/index.html) for model optimization

[Trax](https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.transformer.Transformer) for already premade Transformer implementation.


[Uva Introduction tutorial for JAX](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html)

[Uva DL Transformer implementation in JAX. ](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html)
