{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shell for the functions needed for the gpt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "context_length = 64\n",
    "train_test_split_size = 0.9\n",
    "embed_dim = 32\n",
    "head_num = 4\n",
    "dim_mul = 4\n",
    "block_layers = 4\n",
    "learning_rate = 3e-4\n",
    "max_iters = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path: str = \"/Users/mihkelmariuszjezierski/Desktop/NN Project/Philosophy-GPT/new_nietzsche.txt\"):\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read()\n",
    "    return txt\n",
    "\n",
    "text = open_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class that takes care of encoding and decoding the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text:str, tokenizer_type:str=\"base\") -> None:\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.vocab_size, self.all_characters = self.sort_characters(text)\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return jnp.copy(self.vocab_size)\n",
    "\n",
    "    def sort_characters(self, data):\n",
    "        all_characters = sorted(list(set(data)))\n",
    "        vocab_size = len(all_characters)\n",
    "        \n",
    "        return vocab_size, all_characters\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for c in text:\n",
    "                num = self.all_characters.index(c)\n",
    "                encoded_text.append(num)\n",
    "        return jnp.array(encoded_text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        text = []\n",
    "        if self.tokenizer_type == \"base\":\n",
    "            for n in encoded_text:\n",
    "                char = self.all_characters[n]\n",
    "                text.append(char)\n",
    "            text = \"\".join([str(item) for item in text])\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text=text, tokenizer_type=\"base\")\n",
    "all_data = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I am now going to relate is the history of the next two centuries.\n",
      "I shall describe what will happen, what must necessarily happen:\n",
      "the triumph of Nihilism. This history can be written already; for\n",
      "necessity itself is at work in bringing it about. This future is\n",
      "already proclaimed by a hundred different omens; as a destiny it\n",
      "announces its advent everywhere, for this music of to-morrow all ears\n",
      "are already pricked. The whole of our culture in Europe has long\n",
      "been writhing in an agony of su\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "print(tokenizer.decode(all_data[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            pos = jax.random.randint(\n",
    "                key=subkey, shape=(), minval=0, maxval=(len(b_data) - context_length)\n",
    "            )\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader:\n",
    "    def __init__(self, data, train_test_split_size) -> None:\n",
    "        self.training_data, self.validation_data = self.splitting_data(\n",
    "            data, train_test_split_size\n",
    "        )\n",
    "\n",
    "    def splitting_data(self, data, split_size):\n",
    "        n = int(split_size * len(data))\n",
    "        training_data = data[:n]\n",
    "        validation_data = data[n:]\n",
    "        return training_data, validation_data\n",
    "\n",
    "    def get_batch(self, key, batch_size, context_length, training: bool = True):\n",
    "        train_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        if training:\n",
    "            b_data = self.training_data\n",
    "        else:\n",
    "            b_data = self.validation_data\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            pos = np.random.randint(0, high=(len(b_data) - context_length))\n",
    "            batch_data = b_data[pos : pos + context_length]\n",
    "            train_batches.append(batch_data)\n",
    "            batch_data = b_data[pos + 1 : pos + context_length + 1]\n",
    "            target_batches.append(batch_data)\n",
    "\n",
    "        train_batches = jnp.stack(train_batches)\n",
    "        target_batches = jnp.stack(target_batches)\n",
    "\n",
    "        return train_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72  75  61  75   9   1  58  77]\n",
      " [  1  65  76  75   1  74  71  71]\n",
      " [  1  70  71  76   1  58  61   1]\n",
      " [ 75  76   0  66  77  60  63  69]\n",
      " [ 75  59  61  70  65  59   1  57]\n",
      " [ 75   0  69  57  70  70  61  74]\n",
      " [ 70   1  64  65  75   1  75  76]\n",
      " [ 75  61   1  71  68  60   1  63]\n",
      " [ 65  68  60  65  70  63   1  59]\n",
      " [ 65  76  77  60  61   9   1  59]\n",
      " [ 67  65  70  63   1  71  70   1]\n",
      " [ 72  71  75  65  76  65  71  70]\n",
      " [156  47  77  72  61  74  69  57]\n",
      " [  1  76  71   1  75  61  72  57]\n",
      " [ 62   1  57   1  60  65  78  65]\n",
      " [ 71  74  76  10  75  65  63  64]\n",
      " [ 65  70  61  74  76  65  57   9]\n",
      " [ 70  59  61   0  71  62   1  76]\n",
      " [ 74  61  11   1  48  64  57  76]\n",
      " [ 63  65  71  77  75   1  75  76]\n",
      " [  1  62  74  71  69   1  76  64]\n",
      " [ 65  70  63   1  77  75   1  75]\n",
      " [ 71  77  63  64   1  76  64  61]\n",
      " [ 71   1  64  57  78  61   1  74]\n",
      " [ 65  75   1  57  58  68  61   1]\n",
      " [ 65  70  63   0  63  68  71  58]\n",
      " [ 68  57  74  68  81   1  65  62]\n",
      " [ 65  63  63  61  74  75   1  79]\n",
      " [  0  71  62   1  59  71  77  74]\n",
      " [ 68  60   0  72  74  61  62  61]\n",
      " [ 81   2   1  48  64  61  74  61]\n",
      " [ 81   1  76  77  74  70  61  60]\n",
      " [ 57  63  61  11 153  43  70  61]\n",
      " [ 61  68  68  57  76  65  71  70]\n",
      " [ 11  10  10   0   0  48  64  77]\n",
      " [ 75  57  70  59  61   1  79  65]\n",
      " [ 64  61   1  64  57  72  72  65]\n",
      " [  1  61  78  61  70   1  65  70]\n",
      " [ 71  71  67   1  77  72   0  79]\n",
      " [ 57  70  81   1  71  70  61   1]\n",
      " [  1  72  71  68  65  76  65  59]\n",
      " [ 64   1  75  79  57  68  68  71]\n",
      " [ 11   0   0   1   1   1   1   1]\n",
      " [  1  76  71   1  58  61   1  62]\n",
      " [ 64  65  75   1  62  57  58  77]\n",
      " [ 71  69  61  76  64   9   1  57]\n",
      " [  6  76  64  61   1  31  64  65]\n",
      " [ 71  62   1  78  65  71  68  61]\n",
      " [ 76  81   1  71  62   1  76  64]\n",
      " [ 75  76   1  65  70   1  76  64]\n",
      " [  1  64  71  68  60   1  70  71]\n",
      " [ 10  46  71  77  75  75  61  57]\n",
      " [  1  76  79  71   1  71  59  59]\n",
      " [  0  65  70   1  79  64  65  59]\n",
      " [ 70  63   1  31  64  74  65  75]\n",
      " [ 64  57  76  64   1  76  65  69]\n",
      " [ 67  70  71  79  68  61  60  63]\n",
      " [ 61  72  68  81  11   0   0   1]\n",
      " [  1  76  64  61   1  75  77  69]\n",
      " [ 58  68  65  69  61   1  75  81]\n",
      " [ 64  57  76   1  37   1  69  57]\n",
      " [ 57  70  81   1  71  62   1  81]\n",
      " [  1  68  57  75  76   1  77  75]\n",
      " [ 81   1  75  71  74  76   1  71]]\n",
      "[[ 75  61  75   9   1  58  77  76]\n",
      " [ 65  76  75   1  74  71  71  76]\n",
      " [ 70  71  76   1  58  61   1  76]\n",
      " [ 76   0  66  77  60  63  69  61]\n",
      " [ 59  61  70  65  59   1  57  74]\n",
      " [  0  69  57  70  70  61  74   2]\n",
      " [  1  64  65  75   1  75  76  71]\n",
      " [ 61   1  71  68  60   1  63  74]\n",
      " [ 68  60  65  70  63   1  59  65]\n",
      " [ 76  77  60  61   9   1  59  71]\n",
      " [ 65  70  63   1  71  70   1  76]\n",
      " [ 71  75  65  76  65  71  70   1]\n",
      " [ 47  77  72  61  74  69  57  70]\n",
      " [ 76  71   1  75  61  72  57  74]\n",
      " [  1  57   1  60  65  78  65  70]\n",
      " [ 74  76  10  75  65  63  64  76]\n",
      " [ 70  61  74  76  65  57   9   1]\n",
      " [ 59  61   0  71  62   1  76  64]\n",
      " [ 61  11   1  48  64  57  76   1]\n",
      " [ 65  71  77  75   1  75  76  57]\n",
      " [ 62  74  71  69   1  76  64  61]\n",
      " [ 70  63   1  77  75   1  75  61]\n",
      " [ 77  63  64   1  76  64  61  69]\n",
      " [  1  64  57  78  61   1  74  61]\n",
      " [ 75   1  57  58  68  61   1  76]\n",
      " [ 70  63   0  63  68  71  58  61]\n",
      " [ 57  74  68  81   1  65  62   1]\n",
      " [ 63  63  61  74  75   1  79  57]\n",
      " [ 71  62   1  59  71  77  74  75]\n",
      " [ 60   0  72  74  61  62  61  74]\n",
      " [  2   1  48  64  61  74  61   0]\n",
      " [  1  76  77  74  70  61  60   1]\n",
      " [ 63  61  11 153  43  70  61   1]\n",
      " [ 68  68  57  76  65  71  70   1]\n",
      " [ 10  10   0   0  48  64  77  75]\n",
      " [ 57  70  59  61   1  79  65  76]\n",
      " [ 61   1  64  57  72  72  65  70]\n",
      " [ 61  78  61  70   1  65  70   1]\n",
      " [ 71  67   1  77  72   0  79  65]\n",
      " [ 70  81   1  71  70  61   1  79]\n",
      " [ 72  71  68  65  76  65  59  75]\n",
      " [  1  75  79  57  68  68  71  79]\n",
      " [  0   0   1   1   1   1   1   1]\n",
      " [ 76  71   1  58  61   1  62  71]\n",
      " [ 65  75   1  62  57  58  77  68]\n",
      " [ 69  61  76  64   9   1  57  70]\n",
      " [ 76  64  61   1  31  64  65  70]\n",
      " [ 62   1  78  65  71  68  61  70]\n",
      " [ 81   1  71  62   1  76  64  61]\n",
      " [ 76   1  65  70   1  76  64  61]\n",
      " [ 64  71  68  60   1  70  71  76]\n",
      " [ 46  71  77  75  75  61  57  77]\n",
      " [ 76  79  71   1  71  59  59  57]\n",
      " [ 65  70   1  79  64  65  59  64]\n",
      " [ 63   1  31  64  74  65  75  76]\n",
      " [ 57  76  64   1  76  65  69  61]\n",
      " [ 70  71  79  68  61  60  63  61]\n",
      " [ 72  68  81  11   0   0   1   1]\n",
      " [ 76  64  61   1  75  77  69  69]\n",
      " [ 68  65  69  61   1  75  81  69]\n",
      " [ 57  76   1  37   1  69  57  60]\n",
      " [ 70  81   1  71  62   1  81  71]\n",
      " [ 68  57  75  76   1  77  75  61]\n",
      " [  1  75  71  74  76   1  71  62]]\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(data=all_data, train_test_split_size=train_test_split_size)\n",
    "train, targets = batch_loader.get_batch(key, batch_size, context_length, training=True)\n",
    "print(train)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttentionHead(nn.Module):\n",
    "    embed_dim: int\n",
    "    head_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.key = nn.Dense(self.head_size, use_bias=False) \n",
    "        self.query = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.value = nn.Dense(self.head_size, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        \n",
    "        k = self.key(data)  # from embed_dim to head_size (B,T,C)\n",
    "        q = self.query(data) # from embed_size to head_size (B,T,C)\n",
    "        v = self.value(data) # from embed_size to head_size (B,T,C)\n",
    "\n",
    "        weights = jnp.matmul(q,jnp.swapaxes(k, -2,-1)) / math.sqrt(self.head_size) # (B,T,T)\n",
    "        \n",
    "        #Lower triangular mask matrix of the size B, T, C (same btw as attention)\n",
    "        mask = jnp.tril(weights)\n",
    "        \n",
    "        # for every zero, make it to -inf \n",
    "        weights = nn.softmax(jnp.where(mask == 0, -9e16, weights), axis=-1) # axis=-1 since we only want to softmax for each row of T not for the whole data as a whole\n",
    "        \n",
    "        weights = self.dropout(weights, deterministic = not training)\n",
    "\n",
    "        attention = jnp.matmul(weights, v) # (B,T,C)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple attention heads combined together\n",
    "    \"\"\"\n",
    "\n",
    "    head_num: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.heads = [\n",
    "            SingleAttentionHead(\n",
    "                embed_dim=self.embed_dim, head_size=self.embed_dim // self.head_num\n",
    "            )\n",
    "            for _ in range(self.head_num)\n",
    "        ]\n",
    "        self.think = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training):\n",
    "        multiple_attentions = jnp.concatenate(\n",
    "            [head(data, training) for head in self.heads], axis=-1\n",
    "        )\n",
    "        thoughts = self.think(multiple_attentions)\n",
    "        out = self.dropout(thoughts, deterministic = not training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Simple Feed Forward NN that goes from embed_dim to a higher dimension and then back to embed_dim'''\n",
    "    \n",
    "    embed_dim: int\n",
    "    dim_mul: int\n",
    "\n",
    "    def setup(self):\n",
    "        #this is the heavy thinking part of the model, where it tries to make sense of what was learned\n",
    "        # in the attention cycle lol\n",
    "        self.layer1 = nn.Dense(features=(dim_mul*embed_dim), use_bias=False)\n",
    "        self.layer2 = nn.Dense(features=embed_dim, use_bias=False)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = self.layer1(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x, deterministic = not training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''One run through a block, which consists of MultiheadAttention + Feedforward + Layer Normalisation'''\n",
    "    dim_mul: int\n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "        self.multihead = MultiHeadAttention(head_num = self.head_num, embed_dim=self.embed_dim)\n",
    "        self.feedforward = FeedForward(embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "    \n",
    "    def __call__(self, data, training: bool):\n",
    "        x = data\n",
    "        x = x + self.multihead(self.norm1(x), training)\n",
    "        x = x + self.feedforward(self.norm2(x), training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequential(nn.Module):\n",
    "    layers: list\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, *args, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    vocab_size: int\n",
    "    context_length: int \n",
    "    embed_dim: int\n",
    "    head_num: int\n",
    "    dim_mul: int\n",
    "    block_layers: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.token_embedding_table = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.position_embedding_table = nn.Embed(\n",
    "            self.context_length, self.embed_dim\n",
    "        ) \n",
    "        #########################\n",
    "        self.blocks = CustomSequential([\n",
    "            Block(head_num=self.head_num, embed_dim=self.embed_dim, dim_mul=self.dim_mul)\n",
    "            for _ in range(self.block_layers)\n",
    "        ])\n",
    "        \n",
    "        #########################\n",
    "        self.norm = nn.LayerNorm()\n",
    "        self.linear = nn.Dense(self.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, data, training: bool = True):\n",
    "        \n",
    "        \n",
    "        _, context_length = data.shape\n",
    "        \n",
    "        token = self.token_embedding_table(data)\n",
    "        position = self.position_embedding_table(jnp.arange(context_length))\n",
    "        \n",
    "        embedded_data = token + position\n",
    "\n",
    "        iteration_data = self.blocks(embedded_data, training) # data after one iteration MH,FF (4,8,32)\n",
    "        data_normalized = self.norm(iteration_data)\n",
    "        final_data = self.linear(data_normalized)\n",
    "\n",
    "        return final_data\n",
    "    \n",
    "    def generate(self, key, params, data, length):\n",
    "        \n",
    "        batch_size, _ = data.shape\n",
    "        \n",
    "        # Prepare jax.random.choice to operate on batches of data\n",
    "        # points, without the need for explicit loops\n",
    "        batched_random_choice = jax.vmap(jax.random.choice)\n",
    "            \n",
    "        for _ in range(length):\n",
    "            \n",
    "            # One new random key for every new character\n",
    "            key, subkey = jax.random.split(\n",
    "                key\n",
    "            )\n",
    "            \n",
    "            # Prepare a (batch_size, 1) column of subkeys, one for every batch\n",
    "            batched_key = subkey.reshape(1, -1)\n",
    "            batched_key = jnp.repeat(batched_key, batch_size, axis=0)\n",
    "            \n",
    "            # Only use the last context_window characters to make predictions\n",
    "            data_to_use = data[:, -self.context_length:]\n",
    "            \n",
    "            # Forward pass through the network to get the predictions\n",
    "            logits = self.apply({\"params\": params}, \n",
    "                                data_to_use, \n",
    "                                training=False)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = jax.nn.softmax(logits)\n",
    "            \n",
    "            # Preare a (batch_size, vocab_size) matrix storing token indexes\n",
    "            token_indexes = jnp.arange(self.vocab_size).reshape(1, -1)\n",
    "            token_indexes = jnp.repeat(token_indexes, batch_size, axis=0)\n",
    "            \n",
    "            # Selext new tokens for all batches based on probabilities\n",
    "            next_indexes = batched_random_choice(batched_key, token_indexes, p=probabilities)\n",
    "            next_indexes = next_indexes.reshape(batch_size, -1)\n",
    "            \n",
    "            # Append the new tokens to the sequence\n",
    "            data = jnp.concatenate([data, next_indexes], axis=1)\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.01, peak_value=1, warmup_steps=100, decay_steps=2000\n",
    ")\n",
    "#optimizer = optax.adamw(scheduler)\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _train_step(state, batch, dropout_key):\n",
    "    dropout_key, dropout_train_key = jax.random.split(dropout_key)\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        \n",
    "        data, labels = batch\n",
    "                \n",
    "        # Same as model.apply\n",
    "        logits = state.apply_fn( \n",
    "            {\"params\": params},\n",
    "            data,\n",
    "            training = True,\n",
    "            rngs={'dropout': dropout_train_key}\n",
    "        )\n",
    "\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "        mean_loss = jnp.mean(loss)\n",
    "        \n",
    "        return mean_loss, logits\n",
    "\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        loss_fn,  # Function to calculate the loss\n",
    "        has_aux=True,  # Function has additional outputs, here accuracy\n",
    "    )\n",
    "    # Determine gradients for current model, parameters and batch\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    # Perform parameter update with gradients and optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Return state and any other value we might want\n",
    "    return state, loss\n",
    "\n",
    "\n",
    "# @jax.jit  # Jit the function for efficiency\n",
    "def _eval_step(state, batch, training: bool):\n",
    "    data, labels = batch\n",
    "    logits = state.apply_fn({\"params\": state.params}, \n",
    "                            data, \n",
    "                            training)\n",
    "    \n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    mean_loss = jnp.mean(loss)\n",
    "    #accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    \n",
    "    return mean_loss    \n",
    "\n",
    "\n",
    "def train(state, num_epochs, dropout_key):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train, train_labels = batch_loader.get_batch(\n",
    "            key, batch_size, context_length, training=True\n",
    "        )\n",
    "        \n",
    "        train_batch = (train, train_labels)\n",
    "        state, train_loss = _train_step(state, train_batch, dropout_key)\n",
    "         \n",
    "        # We could use the loss and accuracy for logging here, e.g. in TensorBoard\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            eval, eval_labels = batch_loader.get_batch(\n",
    "                key, batch_size, context_length, training=False\n",
    "            )\n",
    "            eval_batch = (eval, eval_labels)\n",
    "            eval_loss = _eval_step(state, eval_batch, training=False)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train loss {train_loss}, Eval loss {eval_loss}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "data = jnp.ones(\n",
    "    (batch_size, context_length), dtype=jnp.int32\n",
    ")  # Example shape (batch_size, sequence_length)\n",
    "labels = jnp.ones((batch_size, context_length), dtype=jnp.int32)\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    head_num=head_num,\n",
    "    dim_mul=dim_mul,\n",
    "    block_layers=block_layers\n",
    ")\n",
    "\n",
    "## specify what the key is used \n",
    "key, param_key, dropout_key = jax.random.split(key, num=3)\n",
    "variables = model.init(param_key, data=data, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3000 [00:02<2:00:41,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss 5.771120071411133, Eval loss 5.777881622314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 101/3000 [00:40<20:02,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Train loss 3.9448418617248535, Eval loss 3.798156261444092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 201/3000 [01:17<18:37,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Train loss 3.47705078125, Eval loss 3.3682920932769775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 301/3000 [01:55<17:51,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Train loss 3.2307708263397217, Eval loss 3.1397347450256348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 401/3000 [02:32<17:18,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Train loss 3.0880560874938965, Eval loss 2.9814682006835938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 501/3000 [03:10<16:23,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train loss 2.9477016925811768, Eval loss 2.848099946975708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 601/3000 [03:48<16:21,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600: Train loss 2.8708744049072266, Eval loss 2.7828361988067627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 701/3000 [04:26<15:21,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700: Train loss 2.865926504135132, Eval loss 2.7303664684295654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 801/3000 [05:05<15:32,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800: Train loss 2.8725662231445312, Eval loss 2.6926679611206055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 901/3000 [05:42<13:56,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 900: Train loss 2.721914291381836, Eval loss 2.6276214122772217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1001/3000 [06:20<13:15,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: Train loss 2.734056234359741, Eval loss 2.644724130630493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1101/3000 [06:57<12:35,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100: Train loss 2.727660894393921, Eval loss 2.5570545196533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1201/3000 [07:35<11:56,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1200: Train loss 2.746577024459839, Eval loss 2.6471874713897705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1301/3000 [08:12<11:13,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1300: Train loss 2.6960935592651367, Eval loss 2.598940849304199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1401/3000 [08:50<10:41,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400: Train loss 2.626797676086426, Eval loss 2.5940568447113037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1501/3000 [09:27<09:56,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1500: Train loss 2.6213326454162598, Eval loss 2.540086030960083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1601/3000 [10:05<09:08,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1600: Train loss 2.6115777492523193, Eval loss 2.536756753921509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1701/3000 [10:43<08:36,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1700: Train loss 2.61089825630188, Eval loss 2.55234956741333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1801/3000 [11:21<08:10,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800: Train loss 2.599921464920044, Eval loss 2.579639196395874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1901/3000 [11:58<07:17,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1900: Train loss 2.6460163593292236, Eval loss 2.5395030975341797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2001/3000 [12:36<06:37,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: Train loss 2.5736565589904785, Eval loss 2.482215404510498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2101/3000 [13:14<06:06,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2100: Train loss 2.529961109161377, Eval loss 2.482984781265259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2201/3000 [13:52<05:23,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2200: Train loss 2.5736091136932373, Eval loss 2.5296435356140137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2301/3000 [14:29<04:33,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2300: Train loss 2.524461269378662, Eval loss 2.5047318935394287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2401/3000 [15:07<03:57,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2400: Train loss 2.5382566452026367, Eval loss 2.4496898651123047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2501/3000 [15:45<03:17,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2500: Train loss 2.577253818511963, Eval loss 2.503922462463379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 2601/3000 [16:23<02:41,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2600: Train loss 2.543030261993408, Eval loss 2.526136875152588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 2701/3000 [17:00<02:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2700: Train loss 2.5099592208862305, Eval loss 2.4686970710754395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2801/3000 [17:38<01:21,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2800: Train loss 2.544265031814575, Eval loss 2.4546725749969482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2901/3000 [18:16<00:39,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2900: Train loss 2.5255260467529297, Eval loss 2.462092638015747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [18:53<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = variables['params']\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    key: jax.Array\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    key=dropout_key,\n",
    "    tx=optimizer\n",
    ")\n",
    "\n",
    "trained_model_state = train(state=state, num_epochs=3000, dropout_key=dropout_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 76 61 78 57 68 68 75 61  1 35 71 77 57 78 61  1 63 71 62  1 72 64 61\n",
      "   1 67 71 62  1 71 75  1 68  9  1 71 69 71 70  1 60  1 68 81  1 69 62 61\n",
      "   1 72 61]]\n",
      " tevallse Gouave gof phe kof os l, omon d ly mfe pe\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "\n",
    "key, subkey, dropout_key = jax.random.split(key, num=3)\n",
    "\n",
    "generated_seq = model.generate(\n",
    "    key=subkey,\n",
    "    params=trained_model_state.params,\n",
    "    data=jax.numpy.ones((1, 1), dtype=jax.numpy.int32),\n",
    "    length=50\n",
    ")\n",
    "print(generated_seq)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_seq[0])\n",
    "\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
